<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>鹧鸪天.西都作</title>
      <link href="/2024/04/09/%E9%B9%A7%E9%B8%AA%E5%A4%A9.%E8%A5%BF%E9%83%BD%E4%BD%9C/"/>
      <url>/2024/04/09/%E9%B9%A7%E9%B8%AA%E5%A4%A9.%E8%A5%BF%E9%83%BD%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<h2 id="鹧鸪天-西都作-朱敦儒"><a href="#鹧鸪天-西都作-朱敦儒" class="headerlink" title="鹧鸪天.西都作~朱敦儒~"></a>鹧鸪天.西都作~朱敦儒~</h2><p>我是清都山水郎，天教分付与疏狂。曾批给雨支风敕，累上留云借月章。</p><p>诗万首，酒千觞。几曾着眼看君王？玉楼金阙慵归去，切插梅花醉洛阳。</p>]]></content>
      
      
      <categories>
          
          <category> 诗情画意 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>《自然语言处理入门》学习day3</title>
      <link href="/2024/04/09/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E5%AD%A6%E4%B9%A0day3/"/>
      <url>/2024/04/09/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E5%AD%A6%E4%B9%A0day3/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p><a href="词典分词">2. 词典分词</a></p><ul><li><a href="什么是词">2.1 什么是词</a></li><li><a href="词典">2.2 词典</a></li><li><a href="切分算法">2.3 切分算法</a></li><li><a href="字典树">2.4 字典树</a></li><li><a href="基于字典树的其它算法">2.5 基于字典树的其它算法</a></li><li><a href="HanLP的词典分词实现">2.6 HanLP的词典分词实现</a></li></ul><h2 id="2-词典分词"><a href="#2-词典分词" class="headerlink" title="2.词典分词"></a>2.词典分词</h2><ul><li><strong>中文分词</strong>：指的是讲一段文本拆分成一系列单词的过程，这些单词按顺序拼接是原文本。</li><li>中文分词算法主要<strong>基于词典规则</strong>和<strong>基于机器学习</strong>这两大类</li></ul><h3 id="2-1什么是词"><a href="#2-1什么是词" class="headerlink" title="2.1什么是词"></a>2.1什么是词</h3><ul><li>在基于词典的中文分词中，词的定义现实得多：<strong>在词典中的字符串就是词</strong>。</li><li>词的性质：齐夫定律：一个单词的词频与它的词频排名成反比。</li></ul><h3 id="2-2词典"><a href="#2-2词典" class="headerlink" title="2.2词典"></a>2.2词典</h3><p>互联网词库(SogouW，15万个词条)、清华大学开放中文词库(THUOCL)、HanLP词库(千万级词条)</p><p>这里以HanLP附带的迷你核心词典为例:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">上升  V 98 vn 18</span><br><span class="line">上升期 n 1</span><br><span class="line">上升股 n 1</span><br><span class="line">上午   t 147</span><br><span class="line">上半叶 t 3</span><br><span class="line">上半场 n 2</span><br><span class="line">上半夜 t 1</span><br></pre></td></tr></table></figure><p>HanLP中的词典形式是一种以空格分隔的表格形式，第一列是单词本身，后几列依次是词性，词频，命名实体及相应频率。</p><h3 id="2-3切分算法"><a href="#2-3切分算法" class="headerlink" title="2.3切分算法"></a>2.3切分算法</h3><p>首先，加载词典：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_dictionary</span>():</span><br><span class="line">    dic=<span class="built_in">set</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#按行读取字典文件，每行第一个空格之前的字符串提取出来。</span></span><br><span class="line">    <span class="keyword">for</span> line inopen(<span class="string">&quot;文件地址&quot;</span>,<span class="string">&quot;r&quot;</span>):</span><br><span class="line">        dic.add(line[<span class="number">0</span>:line.find(<span class="string">&#x27; &#x27;</span>)])</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> dic</span><br></pre></td></tr></table></figure><p>1.<strong>完全切分</strong></p><p>指的是，找出一段文本中的所有单词。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fully_segment</span>(<span class="params">text,dic</span>):</span><br><span class="line">    word_list = []</span><br><span class="line">    <span class="comment">#i从0到text的最后一个字的下标遍历</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(text)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>,<span class="built_in">len</span>(text)+<span class="number">1</span>):</span><br><span class="line">            word=text[i:j]</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> dic:</span><br><span class="line">                word_list.append(word)</span><br><span class="line">    <span class="keyword">return</span> word_list</span><br><span class="line">dic=load_dictionary()</span><br><span class="line"><span class="built_in">print</span>(fully_segment(<span class="string">&#x27;就读北京大学&#x27;</span>,dic))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[&#x27;就&#x27;,&#x27;就读&#x27;,&#x27;读&#x27;,&#x27;北&#x27;,&#x27;北京大学&#x27;,&#x27;京&#x27;,&#x27;大&#x27;,&#x27;大学&#x27;,&#x27;学&#x27;]</span><br></pre></td></tr></table></figure><p>输出了所有可能的单词，因为词库中含有单字，所以也输出了一些单字。</p><p>2.<strong>正向最长匹配</strong></p><p>上面的输出并不是中文分词，我们更需要那种有意义的词语序列，而不是所有出现在词典中的单词所构成的链表。比如，我们希望“北京大学”成为一整个词，而不是“北京+大学”之类的碎片。具体来说，就是在某个下标为起点递增查词的过程中，优先输出更长的单词，这种规则被称为<strong>最长匹配算法</strong>。从前往后匹配则称为<strong>正向最长匹配</strong>，反之则称为<strong>逆向最长匹配</strong>。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_segment</span>(<span class="params">text,dic</span>):</span><br><span class="line">    word_list=[]</span><br><span class="line">    i=<span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i&lt;<span class="built_in">len</span>(text):</span><br><span class="line">        longest_word=text[i]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>,<span class="built_in">len</span>(text)+<span class="number">1</span>):</span><br><span class="line">            word=text[i:j]</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> dic:</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(word)&gt;<span class="built_in">len</span>(longest_word):</span><br><span class="line">                    longest_word=word</span><br><span class="line">                    </span><br><span class="line">          word_list.append(longest_word)</span><br><span class="line">          i+=<span class="built_in">len</span>(longest_word)</span><br><span class="line">   <span class="keyword">return</span> word_list</span><br><span class="line"></span><br><span class="line">dic=load_dictionary()</span><br><span class="line"><span class="built_in">print</span>(forward_segment(<span class="string">&#x27;就读北京大学&#x27;</span>,dic))</span><br><span class="line"><span class="built_in">print</span>(forward_segment(<span class="string">&#x27;研究生命起源&#x27;</span>,duc))</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[&#x27;就读&#x27;,&#x27;北京大学&#x27;]</span><br><span class="line">[&#x27;研究生&#x27;,&#x27;命&#x27;,&#x27;起源&#x27;]</span><br></pre></td></tr></table></figure><p>第二句话产生了误差，我们是要把“研究”提取出来，结果按照正向最长匹配算法就提取出了“研究生”，所以人们就想出了逆向最长匹配。</p><p>3.<strong>逆向最长匹配</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backward_segment</span>(<span class="params">text,dic</span>):</span><br><span class="line">    word_list=[]</span><br><span class="line">    i=<span class="built_in">len</span>(text)-<span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> i&gt;=<span class="number">0</span>:</span><br><span class="line">        longest_word=text[i]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,i):</span><br><span class="line">            word=text[j:i+<span class="number">1</span>]</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> dic:</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(word)&gt;<span class="built_in">len</span>(longest_word):</span><br><span class="line">                    longest_word=word</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">         word_list.insert(<span class="number">0</span>,longest_word)</span><br><span class="line">         i-=<span class="built_in">len</span>(longest_word)</span><br><span class="line">   <span class="keyword">return</span> word_list</span><br><span class="line">dic=load_dictionary()</span><br><span class="line"><span class="built_in">print</span>(backward_segment(<span class="string">&#x27;研究生命起源&#x27;</span>,dic))</span><br><span class="line"><span class="built_in">print</span>(backward_segment(<span class="string">&#x27;项目的研究&#x27;</span>,dic))</span><br><span class="line">            </span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[&#x27;研究&#x27;,&#x27;生命&#x27;,&#x27;起源&#x27;]</span><br><span class="line">[&#x27;项&#x27;,&#x27;目的&#x27;,&#x27;研究&#x27;]</span><br></pre></td></tr></table></figure><p>第一句正确了，但下一句又出错了，可谓拆东墙补西墙。另一些人提出综合两种规则，期待它们取长补短，称为双向最长匹配。</p><p>4.<strong>双向最长匹配</strong></p><p>这是一种融合两种匹配算法的复杂规则集，流程如下：</p><ul><li>同时执行正向和逆向最长匹配，若两者的词数不同，则返回词数更少的那一个。</li><li>否则，返回两者中单字更少的那一个。当单字数也相同时，优先返回逆向匹配的结果。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">count_single_char</span>(<span class="params">word_list:<span class="built_in">list</span></span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(<span class="number">1</span> <span class="keyword">for</span> word <span class="keyword">in</span> word_list <span class="keyword">if</span> <span class="built_in">len</span>(word)==<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bidirectional_segment</span>(<span class="params">text,dic</span>):</span><br><span class="line">    f=forward_segment(text,dic)</span><br><span class="line">    b=backward_segment(text,dic)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(f)&lt;<span class="built_in">len</span>(b):</span><br><span class="line">        <span class="keyword">return</span> f</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">len</span>(f)&gt;<span class="built_in">len</span>(b):</span><br><span class="line">        <span class="keyword">return</span> b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> count_single_char(f)&lt;count_singlr_char(b):</span><br><span class="line">            <span class="keyword">return</span> f</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> b</span><br><span class="line"><span class="built_in">print</span>(bidirectional_segment(<span class="string">&#x27;研究生命起源&#x27;</span>,dic))</span><br><span class="line"><span class="built_in">print</span>(bidirectional_segment(<span class="string">&#x27;项目的研究&#x27;</span>,dic))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[&#x27;研究&#x27;,&#x27;生命&#x27;,&#x27;起源&#x27;]</span><br><span class="line">[&#x27;项&#x27;,&#x27;目的&#x27;,&#x27;研究&#x27;]</span><br></pre></td></tr></table></figure><p>通过以上几种切分算法，我们可以做一个对比：</p><p>上图所示，双向最长匹配的确在2,3,5这三种情况下选择出了最好的结果，但在4号句子上选择错误的结果，使得最终正确率3/6反而小于逆向最长匹配的4/6.由此，规则系统的脆弱可见一斑。规则集的维护有时是拆东墙补西墙，有时是帮倒忙。</p><h3 id="2-4字典树"><a href="#2-4字典树" class="headerlink" title="2.4字典树"></a>2.4字典树</h3><p>匹配算法的瓶颈之一在于如何判断集合(词典)中是否含有字符串。如果用有序集合(TreeMap)的话，复杂度是O(logn)(n是词典大小);如果用散列表(Java的HashMap.Python的dict)的话，账面上的时间复杂度虽然下降了，但内存复杂度却上去了。有没有速度又快、内存又省得数据结构呢？这就是<strong>字典树</strong>。</p><ol><li><p><strong>什么是字典树</strong></p><p>​    字符串集合常用字典树(trie树、前缀树)存储，这是一种字符串上的树形结构。字典树中每条边都对应一个字符，从根节点往下的路径构成一个个字符串。字典树并不直接在节点上存储字符串，而是将词语视作从跟节点到某节点的一条路径，并在终点节点(蓝色)上做个标记”该节点对应词语的结尾”。字符串就是一条路径，只需从根节点顺着这条路径往下走就能到达特殊标记的节点，则说明该字符串在集合中，否则说明不在。一个典型的字符串如下图所示</p><p><img src="https://github.com/NLP-LOVE/Introduction-NLP/raw/master/img/2020-2-4_14-46-52.png" alt=""></p></li></ol><p>​    其中，每个蓝色节点代表一个词语结尾，数字是人为的编号。按照路径我们可以得到如下表所示：</p><div class="table-container"><table><thead><tr><th>词语</th><th>路径</th></tr></thead><tbody><tr><td>入门</td><td>0-1-2</td></tr><tr><td>自然</td><td>0-3-4</td></tr><tr><td>自语</td><td>0-3-8</td></tr><tr><td>自然人</td><td>0-3-4-5</td></tr><tr><td>自然语言</td><td>0-3-4-6-7</td></tr></tbody></table></div><p>​    当词典大小为n时，算法时间复杂度依然为O(logn)(假设子节点用对数复杂度的数据结构存储，所有词语都是单字)，但它的实际速度比二分查找快，这是因为随着路径深入，前缀匹配是递进的过程，不必理会字符串的前缀。</p><ol><li><p><strong>字典树的实现</strong></p><p>由上图可知，每个节点都应该知道自己的子节点与对应的边，以及自己是否对应一个词。如果要实现映射而不是集合的话，还应该知道自己对应的值。我们约定用值为None表示节点不对应词语，虽然这样就不能插入值为None的键了，但实现起来更简洁。那么字典树的实现参照下图所示项目路径</p><p>通过<strong>debug运行 trie.py 代码</strong>，可以观察到 trie 类的字典树结构：</p><p><img src="https://github.com/NLP-LOVE/Introduction-NLP/raw/master/img/2020-2-4_16-49-15.png" alt=""></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>《自然语言处理入门》学习Day1</title>
      <link href="/2024/04/07/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E5%AD%A6%E4%B9%A0Day1/"/>
      <url>/2024/04/07/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E5%AD%A6%E4%B9%A0Day1/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p><a href="#1-新手上路">1.新手上路</a></p><ul><li><a href="">1.1 自然语言与编程语言的比较</a></li><li><a href="">1.2 自然语言处理的层次</a></li><li><a href="">1.3 自然语言处理的流派</a></li><li><a href="">1.4 机器学习</a></li><li><a href="">1.5 语料库</a></li><li><a href="">1.6 开源工具</a></li><li><a href="">1.7 总结</a></li></ul><h2 id="1-新手上路"><a href="#1-新手上路" class="headerlink" title="1.新手上路"></a>1.新手上路</h2><p>自然语言处理（Natural Language Processing, NLP）是一门融合了计算机科学、人工智能及语言学的交叉学科，它们的关系如下图所示。这门学科研究的是如何通过机器学习等技术，让计算机学会处理人类语言，乃至实现终极目标—理解人类语言或人工智能。</p><p><img src="https://github.com/NLP-LOVE/Introduction-NLP/raw/master/img/2020-2-3_10-50-30.png" alt=""></p><h3 id="1-1自然语言与编程语言的比较"><a href="#1-1自然语言与编程语言的比较" class="headerlink" title="1.1自然语言与编程语言的比较"></a>1.1自然语言与编程语言的比较</h3><div class="table-container"><table><thead><tr><th>比较</th><th>不同</th><th>例子</th></tr></thead><tbody><tr><td>词汇量</td><td>自然语言中的词汇比编程语言中的关键词丰富，我们还可以随时创造各种类型的新词</td><td>蓝瘦、香菇</td></tr><tr><td>机构化</td><td>自然语言是非结构化的，而编程语言是结构化的</td><td></td></tr><tr><td>歧义性</td><td>自然语言中含有大量歧义，而编程语言是结构化的</td><td>这人真有意思：没意思</td></tr><tr><td>容错性</td><td>自然语言错误随处可见，而编程语言错误会导致编译不通过</td><td>的、地的用法错误</td></tr><tr><td>易变性</td><td>自然语言变化相对迅速嘈杂一些，而编程语言的变化要缓慢的多</td><td>新时代词汇</td></tr><tr><td>简略性</td><td>自然语言往往简洁、干练，而编程语言就要明确定义</td><td>“老地方”不必明确指出</td></tr></tbody></table></div><h3 id="1-2自然语言处理的层次"><a href="#1-2自然语言处理的层次" class="headerlink" title="1.2自然语言处理的层次"></a>1.2自然语言处理的层次</h3><p><img src="https://github.com/NLP-LOVE/Introduction-NLP/raw/master/img/2020-2-3_11-17-38.png" alt=""></p><ol><li><p><strong>语音、图像和文本</strong></p><p>自然语言处理系统的输入源一共有三个，即语音、图像与文本。语音和图像这两种形式一般经过识别后转化为文字，转化后就可以进行后续的NLP任务了。</p></li><li><p><strong>中文分词、词性标注和命名实体识别</strong></p><p>这三个任务都是围绕词语进行的分析，所以统称<strong>词法分析</strong>。词法分析的主要任务是将文本分隔为有意义的词语(<strong>中文分词</strong>)，确定每个词语的类别和浅层的歧义消除(词性标注)，并且识别出一些较长的专有名词(<strong>命名实体识别</strong>)。对中文而言，词性分析常常是后续高级任务的基础。</p></li><li><p><strong>信息抽取</strong></p><p>词法分析之后，文本已经呈现出部分结构化的趋势，根据分析出来的每个单词和附有自己词性及其他标签的数据，抽取出一部分有用的信息、关键词、专业术语等，也可以根据统计学信息抽取出更大颗粒度的文本。</p></li><li><p><strong>文本分类与文本聚类</strong></p><p>将文本拆分为一系列词语之后，就可以对文本进行分类和聚类操作，找出相似的文本。</p></li><li><p><strong>句法分析</strong></p><p>词法分析只能得到零散的词汇信息，计算机不知道词语之间的关系，在一些问答系统中，需要得到句子的主谓宾结构，这就是句法分析得到的结果，如下图所示：</p><p><img src="https://github.com/NLP-LOVE/Introduction-NLP/raw/master/img/2020-2-3_12-8-55.png" alt=""></p><p>不仅是问答系统或搜索引擎，句法分析还经常应用有基于短语的机器翻译，给译文的词语重新排序。</p></li><li><p><strong>语义分析与篇章分析</strong></p><p>相较于语法分析，语义分析侧重语义而非语法。它包括<strong>词义消歧</strong>(确定一个词在语境中的含义，而不是简单的词性)、<strong>语义角色标注</strong>(标注句子中的谓语与其他成分的关系)乃至<strong>语义依存分析</strong>(分析句子中词之间的语义关系)。</p></li><li><p><strong>其他高级任务</strong></p><p>自动问答、自动摘要、机器翻译</p><p>注意，一般认为信息检索(Information Retrieve,IR)是区别于自然语言处理的独立学科，IR的目标是查询信息，而NLP的目标是理解语言。</p></li></ol><h3 id="1-3自然语言处理的流派"><a href="#1-3自然语言处理的流派" class="headerlink" title="1.3自然语言处理的流派"></a>1.3自然语言处理的流派</h3><ol><li><p><strong>基于规则的专家系统</strong></p><p>规则，指的是由专家手工制定的确定性流程。专家系统要求设计者对所处理的问题具备深入的理解，并且尽量以人力全面考虑所有所有可能的情况。它最大的弱点是难以拓展。当规则数量增加或者多个专家维护同一个系统时，就容易出现冲突。</p></li><li><p><strong>基于统计的学习方法</strong></p><p>人们使用统计方法让计算机自动学习语言。所谓”统计”，指的是在语料库上进行的统计。所谓”<strong>语料库</strong>“，指的是人工标注的结构化文本。</p></li><li><p><strong>历史</strong></p><p><img src="https://github.com/NLP-LOVE/Introduction-NLP/raw/master/img/2020-2-3_12-26-11.png" alt=""></p></li></ol><h3 id="1-4机器学习"><a href="#1-4机器学习" class="headerlink" title="1.4机器学习"></a>1.4机器学习</h3><ol><li><p><strong>什么是机器学习</strong></p><p><strong>机器学习</strong>指的是计算机通过某项任务的经验数据提高了在该项任务上的能力。</p></li><li><p><strong>模型</strong></p><p>模型是对现实问题的数学抽象，由一个假设函数以及一系列参数构成。以下就是最简单的模型公式：</p><p><img src="https://github.com/NLP-LOVE/Introduction-NLP/raw/master/img/2020-2-3_19-41-15.png" alt=""></p></li><li><p><strong>特征</strong></p><ul><li><strong>特征</strong>指的是事物的特点转化的数值。</li><li>如何挑选特征，如何设计特征模板，这称作<strong>特征工程</strong>。特征越多，参数就越多；参数越多，模型就越复杂。</li></ul></li><li><p><strong>数据集</strong></p><p>样本的集合在机器学习领域被称作<strong>数据集</strong>，在自然语言处理领域称作<strong>语料库</strong>。</p></li><li><p><strong>监督学习</strong></p><p>如果数据集附带标准答案y，则此时的学习算法称作<strong>监督学习</strong>。学习一遍误差还不够小，需要反复学习、反复调整。此时的算法是一种迭代式的算法，每一遍学习称作<strong>一次迭代</strong>。这种在有标签的数据集上迭代学习的过程称作<strong>训练</strong>。</p></li><li><p><strong>无监督学习</strong></p><p>如果我们只给机器做题，却不告诉它参考答案，机器学习仍然可以学到知识吗？可以！此时的学习称作<strong>无监督学习</strong>，而不含标准答案的数据集被称作<strong>无标注的数据集</strong>。无监督学习一般用于聚类和降维，<strong>降维</strong>指的是将样本点从高维空间变换成低维空间的过程。</p></li><li><p><strong>其他类型的机器学习算法</strong></p><ul><li><strong>半监督学习</strong>：如果我们训练多个模型，然后对同一个实例进行预测，会得到多个结果。如果这些结果多数一致，则可以将结果和实例放到一起作为一个训练样本，并扩充训练集。这样的算法称作半监督学习。</li><li><strong>强化学习</strong>：现实世界中的事物往往具有很强的因果链：我们要正确地执行一系列彼此关联的决策，才能得到最终的成果。这样的问题往往需要一边预测，一边根据环境的反馈规划下次的决策。这类算法称作强化学习。</li></ul></li></ol><h3 id="1-5语料库"><a href="#1-5语料库" class="headerlink" title="1.5语料库"></a>1.5语料库</h3><ol><li><p><strong>中文分词语料库</strong></p><p>中文分词语料库指的是，由人工正确切分的句子集合。以著名的1998年《人民日报》语料库为例：</p><blockquote><p>先 有 通货膨胀 干扰，后 有 通货 紧缩 叫板</p></blockquote></li><li><p><strong>词性标注语料库</strong></p><p>它指的是切分并为每个词语制定一个词性的语料。依然以《人民日报》语料库为例：</p><blockquote><p>迈向/v 充满/v 希望/n 的/u 新/a 世纪/n — 一九九八年/t 新年/t 讲话/n</p></blockquote></li><li><p><strong>命名实体识别语料库</strong></p><p>这种语料库人工标注了坐着关心的实体名词或者实体类别。比如《人民日报》中包含了人名、地名和机构名三种命名实体：</p><blockquote><p><strong>萨哈夫/n</strong> 说/v,/w <strong>伊拉克/ns</strong> 将/d 同/p <strong>[联合国/nt 销毁/v 伊拉克/ns  大规模/b 杀伤性/n 武器/n 特别/a 委员会/n]/nt</strong> 继续/v 保持/v 合作/v。/w</p></blockquote><p>这个句子加粗的部分分别是人名，地名，机构名，中括号里的是由机构名和地名构成的复合句，从这可看出机构名和地名会构成更长的机构名，这种构词法的嵌套现象增加了命名实体识别的难度</p></li><li><p><strong>句法分析语料库</strong></p><p><img src="https://github.com/NLP-LOVE/Introduction-NLP/raw/master/img/2020-2-3_13-40-56.png" alt=""></p></li><li><p><strong>文本分类语料库</strong></p><p>它指的是人工标注了所属分类的文章构成的语料库。</p></li><li><p><strong>语料库的建设</strong></p><p>语料库建设指的是构建一份语料库的过程，分为规范制定，人员培训与人工标注三个阶段。针对不同类型的任务，人们开发出许多标注的软件，其中比较成熟的一款是brat，它支持词性标注、命名实体识别和句法分析等任务。</p></li></ol><h3 id="1-6开源工具"><a href="#1-6开源工具" class="headerlink" title="1.6开源工具"></a>1.6开源工具</h3><ol><li><p><strong>主流NLP工具比较</strong></p><p><img src="https://github.com/NLP-LOVE/Introduction-NLP/raw/master/img/2020-2-3_13-52-12.png" alt=""></p><p>本书采用<strong>HanLP作为本书的实现</strong></p></li><li><p><strong>Python接口</strong></p><p>HanLP的Python接口由pyhanlp包提供，其安装只需一句命令：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$ pip install pyhanlp</span><br></pre></td></tr></table></figure></li></ol><h3 id="1-7总结"><a href="#1-7总结" class="headerlink" title="1.7总结"></a>1.7总结</h3><p>本章给出了人工智能、机器学习和自然语言处理的宏观缩略图与发展时间线。机器学习是人工智能的子集，而自然语言处理是人工智能、语言学和计算机科学的交集。这个交集虽然小，它的难度却很大。为了实现理解自然语言这个宏伟目标，人们尝试了规则系统，最终发展到基于大规模语料库的统计学习系统。</p><p>在接下来的章节中要学习第一个NLP问题—中文分词，从规则系统入手，介绍一些快而不准的算法，然后逐步进化到更加准确的统计模型。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>南乡子</title>
      <link href="/2024/04/06/%E5%8D%97%E4%B9%A1%E5%AD%90.%E5%92%8C%E6%9D%A8%E5%85%83%E7%B4%A0%E6%97%B6%E7%A7%BB%E7%A7%BB%E5%AE%88%E5%AF%86%E5%B7%9E/"/>
      <url>/2024/04/06/%E5%8D%97%E4%B9%A1%E5%AD%90.%E5%92%8C%E6%9D%A8%E5%85%83%E7%B4%A0%E6%97%B6%E7%A7%BB%E7%A7%BB%E5%AE%88%E5%AF%86%E5%B7%9E/</url>
      
        <content type="html"><![CDATA[<h2 id="南乡子-和杨元素时移守密州"><a href="#南乡子-和杨元素时移守密州" class="headerlink" title="南乡子.和杨元素时移守密州"></a>南乡子.和杨元素时移守密州</h2><p>东武望余杭，云海天涯两杳茫。何日功成名遂了，还乡，醉笑陪公三万场。</p><p>不用诉离殇，痛饮从来别有肠。今夜送归灯火冷，河塘，堕泪羊公却姓杨。</p>]]></content>
      
      
      <categories>
          
          <category> 诗情画意 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>NLP学习day5：词向量汇总：word2vec、fastText、Glove,Bert</title>
      <link href="/2024/04/06/NLP%E5%AD%A6%E4%B9%A0day5/"/>
      <url>/2024/04/06/NLP%E5%AD%A6%E4%B9%A0day5/</url>
      
        <content type="html"><![CDATA[<p>-</p><h2 id="一、Onehot"><a href="#一、Onehot" class="headerlink" title="一、Onehot"></a>一、Onehot</h2><h3 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h3><ol><li>解决了分类器处理离散数据困难的问题</li><li>一定程度上起到了扩展特征的作用</li></ol><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol><li>one-hot是一个词袋模型，不考虑词与词之间的顺序问题，而在文本中次序是一个很重要的问题</li><li>one-hot是基于词与词之间相互独立的情况下的，然而在多数情况中，词与词之间应该是相互影响的</li><li>one-hot得到的特征是离散的、稀疏的</li></ol><h2 id="二、TF-IDF与TextRank"><a href="#二、TF-IDF与TextRank" class="headerlink" title="二、TF-IDF与TextRank"></a>二、TF-IDF与TextRank</h2><p>NLP中最为经典的关键词提取算法，虽然简单，但应用广泛</p><h3 id="1-TF-IDF-参见上文"><a href="#1-TF-IDF-参见上文" class="headerlink" title="1. TF-IDF:参见上文"></a>1. TF-IDF:参见上文</h3><p><strong>总结</strong>:</p><ul><li>当一个词在文档频率越高并且新鲜度高（即普遍度低），其TF-IDF值越高。</li><li>TF-IDF兼顾词频与新鲜度，过滤一些常见词，保留能提供更多信息的重要词。</li></ul><h3 id="2-TextRank简介"><a href="#2-TextRank简介" class="headerlink" title="2. TextRank简介"></a>2. TextRank简介</h3><p>通过词之间的相邻关系构建网络，然后用<strong>PageRank</strong>迭代计算每个节点的rank值，排序rank值即可得到关键词。</p><p>PageRank本来是用来解决网页排名的问题，网页之间的链接关系即为图的边，迭代计算公式如下：</p><script type="math/tex; mode=display">PR(Vi)=(1-d)+d*\sum_{j}</script>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Docker学习Day1</title>
      <link href="/2024/04/04/Docker%E5%AD%A6%E4%B9%A0day1/"/>
      <url>/2024/04/04/Docker%E5%AD%A6%E4%B9%A0day1/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>随着AI,云原生等技术的向前推进，容器技术逐渐成为每位算法同学的必备技能之一，开始关于容器技术的学习很有必要。我想从零基础实现将代码打包docker镜像-调试-提交仓库-提交云服务训练模型/天池大赛提交/学校服务器训练等流程。希望初次接触docker能够做到以上步骤。</p><h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><p>docker作为虚拟机领域成熟的轻量化容器产品，可以轻松的将代码和所依赖的整个环境（包括整个操作系统）都打包在一起，不依赖于软件环境，方便把自己的代码从windows电脑分享到mac电脑运行、或者服务器上运行等。</p><p>docker三要素：<strong>镜像(image)，容器(contarin)，registry(包含多个仓库)</strong></p><p>以下是对三个要素的分别解释：</p><h3 id="镜像"><a href="#镜像" class="headerlink" title="镜像"></a>镜像</h3><p>顾名思义就是将要把代码以及环境打包在一起的这个产物就叫做镜像</p><h3 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h3><p>运行起来的镜像称之为容器，可以理解为运行环境或者实例。其实质是进程，随着代码运行结束，进程结束容器也就消失了。</p><h3 id="registry"><a href="#registry" class="headerlink" title="registry"></a>registry</h3><p>镜像存储的地方在registry,这是各云厂商提供的镜像存取服务，类似于网盘，将镜像存储在云端仓库，方便我们随时随地在不同介质运行自己的代码或分享代码。如果你要把本地开发好的代码放在服务器上做耗时的训练动作，那么只需要在服务器上直接拉取自己云端的镜像运行即可。类似于git还有代码管理功能。</p>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>NLP学习day4：结合tfidf与RidgeClassifier(岭回归)计算f1_score得分</title>
      <link href="/2024/04/04/NLP%E5%AD%A6%E4%B9%A0day4/"/>
      <url>/2024/04/04/NLP%E5%AD%A6%E4%B9%A0day4/</url>
      
        <content type="html"><![CDATA[<h1 id="代码展示"><a href="#代码展示" class="headerlink" title="代码展示"></a>代码展示</h1><h2 id="导入相关库与工具"><a href="#导入相关库与工具" class="headerlink" title="导入相关库与工具"></a>导入相关库与工具</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br></pre></td></tr></table></figure><h2 id="提取数据表"><a href="#提取数据表" class="headerlink" title="提取数据表"></a>提取数据表</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df=pd.read_csv(<span class="string">&quot;C:\Users\hjg\OneDrive\桌面\train_set.csv&quot;</span>,sep=<span class="string">&#x27;\t&#x27;</span>,nrows=<span class="number">20000</span>)</span><br></pre></td></tr></table></figure><h2 id="提取tfidf值"><a href="#提取tfidf值" class="headerlink" title="提取tfidf值"></a>提取tfidf值</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tfidf=Vectorizer(ngram_range=(<span class="number">1</span>,<span class="number">3</span>),max_feature=<span class="number">3000</span>)</span><br><span class="line">train_test=tfidf.fit_transfrom(train_df[<span class="string">&#x27;text&#x27;</span>])</span><br></pre></td></tr></table></figure><h2 id="构建岭回归，拟合训练模型"><a href="#构建岭回归，拟合训练模型" class="headerlink" title="构建岭回归，拟合训练模型"></a>构建岭回归，拟合训练模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf=RidgeClassifier()</span><br><span class="line">clf.fit(train_test[:<span class="number">1000</span>],train_df[<span class="string">&#x27;label&#x27;</span>].values[:<span class="number">1000</span>])</span><br><span class="line">vlt_per=clf.predict(tran_test[<span class="number">1000</span>:])</span><br></pre></td></tr></table></figure><h2 id="计算f1-score分数"><a href="#计算f1-score分数" class="headerlink" title="计算f1_score分数"></a>计算f1_score分数</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(f1_score(train_df[<span class="string">&#x27;label&#x27;</span>].values[<span class="number">1000</span>:],vlt_pre,averages=<span class="string">&#x27;macro&#x27;</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>NLP学习Day3：基于TF—IDF机器学习模型完成新闻文本分类</title>
      <link href="/2024/04/02/NLP%E5%AD%A6%E4%B9%A0day3/"/>
      <url>/2024/04/02/NLP%E5%AD%A6%E4%B9%A0day3/</url>
      
        <content type="html"><![CDATA[<h2 id="Task3-基于机器学习的文本分类"><a href="#Task3-基于机器学习的文本分类" class="headerlink" title="Task3 基于机器学习的文本分类"></a>Task3 基于机器学习的文本分类</h2><h3 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>学会TF-IDF的原理</li><li>使用sklearn的机器学习模型完成文本分类</li></ul><h2 id="机器学习模型"><a href="#机器学习模型" class="headerlink" title="机器学习模型"></a>机器学习模型</h2><p>机器学习是对能通过经验自动改进的计算机算法的研究机器学习通过历史数据<strong>训练</strong>出<strong>模型</strong>对应于人类经验进行<strong>归纳</strong>的过程，机器学习利用<strong>模型</strong>对新数据进行<strong>预测</strong>对应于人类利用总结的<strong>规律</strong>对新问题进行预测的过程。</p><p>机器学习有很多分支，对于学习者来说应该优先掌握机器学习算法的分类，然后在其中一种机器学习算法进行学习。</p><p>机器学习初学者，应该知道如下的事情：</p><ol><li>机器学习能解决一定的问题，但不能奢求机器学习是万能的</li><li>机器学习算法有很多种，看具体问题进行选择算法</li><li>每种机器学习算法有一定的偏好，具体问题具体分析</li></ol><h2 id="文本表示方法-Part1"><a href="#文本表示方法-Part1" class="headerlink" title="文本表示方法 Part1"></a>文本表示方法 Part1</h2><p>首先要知道在机器学习算法的训练过程中，假设给定N个样本，每个样本有M个特征，这样组成了N<em>M的样本举证，然后完成算法的训练和预测。但在NLP中这样的方法是不行的：文本是不定长度的。文本表示成计算机能够运算的数字或向量的方法一般称为<em>*词嵌入方法</em></em>。词嵌入将不定长的文本转换到定长的空间内，是文本分类的第一步。</p><h2 id="One-hot"><a href="#One-hot" class="headerlink" title="One-hot"></a>One-hot</h2><p>这里的One-hot与数据挖掘任务中的操作是一致的，即将每一个单词使用一个离散的向量表示。具体讲灭各自/词编码一个索引，然后根据索引进行赋值。</p><p>One-hot表示方法的例子如下：</p><blockquote><p>句子1：我 爱 北 京 天 安 门</p><p>句子2：我 喜 欢 上 海</p></blockquote><p>首先对所有句子的字进行索引，即将每个字分配一个编号：</p><blockquote><p>{</p><p>‘我’：1，‘爱’：2…………….’海’：11</p><p>}</p></blockquote><p>在这里共包括11个字，每个字可以转换为一个11维度稀疏向量：</p><p>我：[1,0,0,0,0,0,0,0,0,0,0]</p><p>爱：[0,1,0,0,0,0,0,0,0,0,0]</p><p>…….</p><p>海：[0,0,0,0,0,0,0,0,0,0,1]</p><h2 id="Bag-of-Words"><a href="#Bag-of-Words" class="headerlink" title="Bag of Words"></a>Bag of Words</h2><p>词袋表示，每个文档的字/词可以使用其出现次数来进行表示。词袋模型之所以被称为“词袋”模型，是因为这种模型在实现文本数值化过程中将所有词汇扔进一个袋子中，而忽略了它们在原文中的语法结构和顺序，只关心每个词汇出现的次数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction</span><br><span class="line">corpus=[</span><br><span class="line">    <span class="string">&#x27;This is the frist document.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;This document is the second document.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;And this is the third one.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Is this the first document&#x27;</span>,</span><br><span class="line">]</span><br><span class="line">vectorizer=CountVectorizer()</span><br><span class="line">vectorizer.fit_transform(corpus).toarray()</span><br></pre></td></tr></table></figure><h3 id="文本数值化分析过程"><a href="#文本数值化分析过程" class="headerlink" title="文本数值化分析过程"></a>文本数值化分析过程</h3><ul><li>对文本分词</li><li>去除停用词</li><li>构建词汇表</li><li>文本向量化</li></ul><h2 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h2><p>TF-IDF分数由两部分组成：第一部分是<strong>词语频率</strong>（Term Frequency），第二部分<strong>逆文档频率</strong>（Lnverse Document）。其中计算语料库中文档总数除以含有该词语的文档数量，然后再取对数就是逆文档频率。</p><h3 id="TF-IDF作用："><a href="#TF-IDF作用：" class="headerlink" title="TF-IDF作用："></a>TF-IDF作用：</h3><ul><li>评估每个词在文档中的重要性，以实现对关键词的抽取</li><li>将输入文档表示为向量（关键词重要向量），可用于文档检索</li></ul><h3 id="1-算法公式"><a href="#1-算法公式" class="headerlink" title="1.算法公式"></a>1.算法公式</h3><p>TF-IDF=TF*IDF，评估一个词的重要性要综合考虑该词的TF值和IDF值。</p><ol><li>TF:表示一个词在文档中的出现次数</li><li>IDF:表示包含某个词的文档数量，如果包含该词的文档数量越少则IDF值越大</li></ol><p>例如，词A和词B是在某个文档中出现的词，出现数量一样多，当：</p><ol><li>词A在其他文档中大量出现</li><li>词B在其它文档中少量出现</li></ol><p>则认为：词A的重要程度不如词B</p><p><strong><em>TF-IDF认为词A在其它文档中大量出现，说明该词很普遍，会降低该词的重要性。反之，则认为该词更为重要</em></strong></p><h4 id="TF（词频）"><a href="#TF（词频）" class="headerlink" title="TF（词频）"></a>TF（词频）</h4><p>TF（词频）=某个词在文档中出现的次数</p><p>TF~month~=1+log(TF)</p><h4 id="IDF（逆文档词频）"><a href="#IDF（逆文档词频）" class="headerlink" title="IDF（逆文档词频）"></a>IDF（逆文档词频）</h4><p>IDF（逆文档词频）=<script type="math/tex">\log{\left(\frac{文档总数}{包含某个词的文档数量}\right)}</script></p><p>IDF~month~=<script type="math/tex">\log{\left(\frac{文档总数+1}{包含某个词的文档数量+1}\right)}</script></p><h4 id="公式反映出的信息是什么？"><a href="#公式反映出的信息是什么？" class="headerlink" title="公式反映出的信息是什么？"></a>公式反映出的信息是什么？</h4><p>文档集合中，包含某个词的文档数量越多，该词的IDF值就越小，反之则越大。</p><h4 id="为什么进行log运算？"><a href="#为什么进行log运算？" class="headerlink" title="为什么进行log运算？"></a>为什么进行<script type="math/tex">log</script>运算？</h4><p>对数函数能够缩小数据范围，将大范围的值映射到相对较小的范围。通过取对数，可以有效地缩小高频词的影响，使得低频词在计算中更具影响力</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>NLP学习Day2：数据读取与数据分析</title>
      <link href="/2024/04/01/NLP%E5%AD%A6%E4%B9%A0day2/"/>
      <url>/2024/04/01/NLP%E5%AD%A6%E4%B9%A0day2/</url>
      
        <content type="html"><![CDATA[<h1 id="NLP学习Day2"><a href="#NLP学习Day2" class="headerlink" title="NLP学习Day2"></a>NLP学习Day2</h1><h2 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h2><ul><li>学习使用pandas读取赛题数据</li><li>分析赛题数据的分布规律</li></ul><h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><p>首先文本数据是使用csv格式进行存储。因此可以直接使用pandas完成数据读取的操作</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">train_df=pd.read_csv(<span class="string">&#x27;文本地址&#x27;</span>,sep=<span class="string">&#x27;\t&#x27;</span>,nrows=<span class="number">100</span>)</span><br></pre></td></tr></table></figure><h3 id="read-csv解析"><a href="#read-csv解析" class="headerlink" title="read_csv解析"></a>read_csv解析</h3><ul><li>读取的文件路径要改成本地的路径（相对路径或绝对路径）</li><li>分隔符<strong>sep</strong>,为每列分割的字符，设置为<strong>\t</strong>即可；</li><li>读取行数nrows,为每次读取文件的函数，是数值类型（数据集比较大）</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df.head()</span><br></pre></td></tr></table></figure><h2 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h2><p>在数据读取操作完成之后，要对数据集进行数据分析的操作，我们希望在读取训练集数据之后得到以下结论：</p><ul><li>赛题数据中，新闻文本的长度是多少？</li><li>赛题数据的类别分布是怎么样的，哪些类别比较多？</li><li>赛题数据中，字符分布是怎么样的？</li></ul><h3 id="句子长度分析"><a href="#句子长度分析" class="headerlink" title="句子长度分析"></a>句子长度分析</h3><p>在赛题数据中每行句子的字符使用空格进行隔开，所以可以直接统计单词的个数来得到每个句子的长度</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%pylab inline</span><br><span class="line">train_df[<span class="string">&#x27;text_len&#x27;</span>]=train_df[<span class="string">&#x27;text&#x27;</span>].apply(<span class="keyword">lambda</span> x:<span class="built_in">len</span>(x,split(<span class="string">&#x27; &#x27;</span>)))</span><br><span class="line">train_df[<span class="string">&#x27;text_len&#x27;</span>].describe()</span><br></pre></td></tr></table></figure><p>下面将句子长度绘制直方图</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.hist(train_df[<span class="string">&#x27;text_len&#x27;</span>],bins=<span class="number">200</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Text char count&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Histogram of char count&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="新闻类别分布"><a href="#新闻类别分布" class="headerlink" title="新闻类别分布"></a>新闻类别分布</h2><p>对数据集的类别进行分布统计，具体统计没类新闻的样本个数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df[<span class="string">&#x27;label&#x27;</span>].value_counts().plot(kind=<span class="string">&#x27;bar&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;News class count&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;category&quot;</span>)</span><br></pre></td></tr></table></figure><p>在数据集中标签的对应关系如下：</p><blockquote><p>在数据集中标签的对应的关系如下：{‘科技’: 0, ‘股票’: 1, ‘体育’: 2, ‘娱乐’: 3, ‘时政’: 4, ‘社会’: 5, ‘教育’: 6, ‘财经’: 7, ‘家居’: 8, ‘游戏’: 9, ‘房产’: 10, ‘时尚’: 11, ‘彩票’: 12, ‘星座’: 13}</p></blockquote><p><strong>从统计结果看出，赛题的数据集类别分布存在较为不均匀的情况，科技类新闻最多</strong></p><h2 id="字符分布统计"><a href="#字符分布统计" class="headerlink" title="字符分布统计"></a>字符分布统计</h2><p>接下来可以统计每个字符出现的次数，首先将训练集中所有的句子进行拼接进而划分为字符，并统计每个字符的个数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">all_lines=<span class="string">&#x27; &#x27;</span>.join(<span class="built_in">list</span>(train_df[<span class="string">&#x27;text&#x27;</span>]))</span><br><span class="line">word_count=Counter(all_lines.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">word_count=<span class="built_in">sorted</span>(word_count.items(),key=<span class="keyword">lambda</span> d:d[<span class="number">1</span>],reserve=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(word_count))</span><br><span class="line"><span class="built_in">print</span>(word_count[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(word_count[-<span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>还可根据字在每个句子的出现情况，反推出标点符号。下面代码统计了不同字符在句子中出现的次数</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">train_df[<span class="string">&#x27;text_unique&#x27;</span>]=train_df[<span class="string">&#x27;text&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="string">&#x27; &#x27;</span>.join(<span class="built_in">list</span>(<span class="built_in">set</span>(x.split(<span class="string">&#x27; &#x27;</span>)))))</span><br><span class="line">all_lines=<span class="string">&#x27; &#x27;</span>.join(<span class="built_in">list</span>(train_df[<span class="string">&#x27;text_unique&#x27;</span>]))</span><br><span class="line">word_count=Counter(all_lines.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">word_count=<span class="built_in">sorted</span>(word_count.items(),key=<span class="keyword">lambda</span> d:<span class="built_in">int</span>(d[<span class="number">1</span>]),reserve=<span class="literal">True</span>)</span><br><span class="line">word_count[<span class="number">0</span>]</span><br><span class="line">word_count[<span class="number">1</span>]</span><br><span class="line">word_count[<span class="number">2</span>]</span><br></pre></td></tr></table></figure><h2 id="数据分析的结论"><a href="#数据分析的结论" class="headerlink" title="数据分析的结论"></a>数据分析的结论</h2><p>通过上述分析可得出以下结论：</p><ol><li>赛题中每个新闻包含的字符个数平均为1000个，还有一些新闻字符较长</li><li>赛题中新闻类别分布不均匀，科技类新闻样本量接近4w,星座类不到1k</li><li>赛题总共包括7000——8000个字符</li></ol><p>通过数据分析，我们还可以得出以下结论</p><ol><li>每个新闻平均字符个数较多，需要截断</li><li>由于类别不均衡，会严重影响模型的精度</li></ol><h2 id="本章小结"><a href="#本章小结" class="headerlink" title="本章小结"></a>本章小结</h2><p>对赛题数据进行读取，并新闻句子长度、类别和字符进行了可视化分析</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>NLP学习day1：NPL实战流程</title>
      <link href="/2024/03/31/NLP%E5%AD%A6%E4%B9%A0day1/"/>
      <url>/2024/03/31/NLP%E5%AD%A6%E4%B9%A0day1/</url>
      
        <content type="html"><![CDATA[<h2 id="赛事理解"><a href="#赛事理解" class="headerlink" title="赛事理解"></a>赛事理解</h2><ul><li>通过这道赛题应了解NLP的预处理、模型构建和模型训练等知识点</li><li>本赛题以自然语言处理为背景，对新闻文本进行分类，是一个典型的字符识别问题<h2 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h2></li><li>理解赛题背景与赛题数据</li><li>完成赛题报名和数据下载，理解赛题的解题思路<h2 id="赛题数据"><a href="#赛题数据" class="headerlink" title="赛题数据"></a>赛题数据</h2></li></ul><ul><li>赛题数据为新闻文本，并按照字符匿名处理。整合划分出14个类别<h2 id="数据标签"><a href="#数据标签" class="headerlink" title="数据标签"></a>数据标签</h2></li></ul><hr><p>在数据集中标签的对应的关系如下：{‘科技’: 0, ‘股票’: 1, ‘体育’: 2, ‘娱乐’: 3, ‘时政’: 4, ‘社会’: 5, ‘教育’: 6, ‘财经’: 7, ‘家居’: 8, ‘游戏’: 9, ‘房产’: 10, ‘时尚’: 11, ‘彩票’: 12, ‘星座’: 13}</p><hr><h2 id="评测指标"><a href="#评测指标" class="headerlink" title="评测指标"></a>评测指标</h2><p><strong><em>为类别f1_score的均值</em></strong>，提交结果与实际测试集的类别进行对比，结果越大越好。</p><h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><p>使用<strong>pandas</strong>库完成读取操作，并对赛题数据进行分析</p><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>赛题实质是一个文本分类问题，需要对每句的字符进行分类，但赛题给出的数据是匿名的，无法直接使用中文分词等操作，这个是赛题的难点。<br>赛题难点：需要对匿名字符进行建模，进而完成文本分类的过程，这涉及到<strong>特征提取</strong>和<strong>分类模型</strong>两部分。</p><ul><li><p>思路一：TF-IDF+机器学习分类器</p><p>直接使用TF-IDF对文本提取特征，并使用分类器进行分类。在分类器的选择上，可以使用SVM、LR、或者XGBoost。<del>看不懂qwq</del></p></li><li><p>思路二：FastText</p><p>这是一款入门款的词向量，利用Facebook提供的FastText工具，可以快速构建出分类器。<del>还是看不懂（逃）</del></p></li><li><p>思路三：WordVec + 深度学习分类器</p><p>这是进阶款的词向量，并通过构建深度学习分类完成分类。深度学习分类的网络结构可选择TextCNN、TextRNN或者BiLSTM。<del>这是啥啊</del></p></li><li><p>思路四：Bert词向量</p><p>Bert是高配款的词向量，具有强大的建模学习能力。<del>呜呜呜，不像给NLP0基础新人做的，不会只有我零基础吧</del></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>要学习的技术</title>
      <link href="/2024/03/31/%E4%B8%80%E6%AE%B5%E6%97%B6%E9%97%B4%E7%9A%84%E5%B0%8F%E7%9B%AE%E6%A0%87/"/>
      <url>/2024/03/31/%E4%B8%80%E6%AE%B5%E6%97%B6%E9%97%B4%E7%9A%84%E5%B0%8F%E7%9B%AE%E6%A0%87/</url>
      
        <content type="html"><![CDATA[<ol><li>个人博客主题优化，维护，并设计小功能</li><li>docker容器环境搭建并运行</li><li>NLP新闻文本分类入门+西瓜书机器学习入门</li><li>微信机器人小程序学习部署</li><li>算竞训练（一天一道1800，写题解）<br>此外<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">务须坚持学习，写博客</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
