<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>4.2隐马尔可夫模型</title>
      <link href="/2024/05/04/4.2%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/"/>
      <url>/2024/05/04/4.2%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h3 id="4-2隐马尔可夫模型"><a href="#4-2隐马尔可夫模型" class="headerlink" title="4.2隐马尔可夫模型"></a>4.2隐马尔可夫模型</h3><p>隐马尔可夫模型是指两个时序序列联合分布$P(x,y)$的概率模型，$x$是观测者能够观察到的显状态称为<strong>观测序列</strong>，$y$是观测者观察不到的隐状态，称为<strong>状态序列</strong>。例如对一个单词而言，单词本身是显状态，而单词词性是隐状态，我们需要通过单词序列去猜测它们的词性。隐马尔可夫模型之所以被称为“隐”，是因为从外界来看，状态序列隐藏不可见，是带求的因变量。之所以被称为“马尔科夫模型”是因为符合马尔科夫假设。</p><h4 id="4-2-1-从马尔可夫假设到隐马尔可夫模型"><a href="#4-2-1-从马尔可夫假设到隐马尔可夫模型" class="headerlink" title="4.2.1 从马尔可夫假设到隐马尔可夫模型"></a>4.2.1 从马尔可夫假设到隐马尔可夫模型</h4><p>马尔可夫假设是指一个事件的发生概率只取决于前一个事件的发生，将满足该假设的多个事件串联在一起就构成了马尔可夫链。在<strong>NLP</strong>中可将事件抽象成单词，于是马尔可夫模型就具象为二元语法模型。</p><p>在此基础上，隐马尔可夫模型就是马尔科夫假设作用于状态序列。</p><ul><li>当前状态$y_t$只依赖于$y_{t-1}$，连续多个状态构成隐马尔可夫模型。</li><li>$x_t$只依赖于当前状态的$y_t$，与其他时刻的状态或独立无关。</li></ul><p><img src="https://12345picture.oss-cn-hangzhou.aliyuncs.com/202405031427566.png" alt="2020-2-6_11-43-39"></p><p>按通常理解应该是$x$决定$y$，而不是反过来，这是由于在联合分布中$p(x,y)=p(y,x)$，两个随机变量并没有明确的因果关系。</p><p>从贝叶斯定理的角度来说，联合分布可以做两种等价交换：</p><script type="math/tex; mode=display">p(x,y)=p(x)\times p(y|x)=p(y)\times p(x|y)</script><p>隐马尔可夫模型的原理就是后一种：先有状态后有观测，取决于两个序列的可见与否。在生活中也能找到例子，比如写文章时，先构思好主体脉络、词性，剩下就是往里填充单词。</p><p>状态与观测的依赖关系确定之后，隐马尔可夫模型就用三个要素来模拟时序序列的发生过程——即初始状态概率向量、状态转移概率矩阵、发射概率矩阵。</p><h4 id="4-2-2初始状态概率向量"><a href="#4-2-2初始状态概率向量" class="headerlink" title="4.2.2初始状态概率向量"></a>4.2.2初始状态概率向量</h4><p>系统启动时第一个状态$y_1$被称为<strong>初始状态</strong>，假设$y$有$N$种可能，即$y\in\{s_1…s_N\}$，那么$y_1$就是一个独立的离散型随机变量，由$p(y_1|\pi)$描述。其中$\pi=(\pi_{1},…,\pi_{N})^T,0\le\pi_i\le1$,$\sum_{t=1}^{N}\pi_i=1$，是概率的参数向量，称为<strong>初始状态概率向量</strong>。如图所示：</p><p><img src="https://12345picture.oss-cn-hangzhou.aliyuncs.com/202405031446061.png" alt="2020-2-6_11-57-24"></p><p>给定$\pi$，初始状态的$y_1$的取值分布就确定了。比中文分词问题采用$\{B,M,E,S\}$的标注集时，各个元素的状态$y_1$的概率是</p><script type="math/tex; mode=display">p(y_1=B)=0.7</script><script type="math/tex; mode=display">p(y_2=M)=0</script><script type="math/tex; mode=display">p(y_3=E)=0</script><script type="math/tex; mode=display">p(y_4=S)=0.3</script><p>那么此时的隐马尔可夫模型的初始状态概率向量就是$\pi=[0.7,0,0,0.3]$。注意标签$M$和$E$的概率为0，说明第一个字符不可能成为单词的中部或尾部。另外，句子第一个词是单字的概率要小一些。</p><h4 id="4-2-3状态转移概率矩阵"><a href="#4-2-3状态转移概率矩阵" class="headerlink" title="4.2.3状态转移概率矩阵"></a>4.2.3状态转移概率矩阵</h4><p>$y_t$确定之后如何转移到$y_{t+1}$呢？根据马尔可夫假设，$t+1$的状态只取决于$t$时的状态，既然一共有$N$种状态，那么从状态$s_i$到状态$s_j$的概率就构成了一个$N\times N$的方阵，称为<strong>状态转移概率矩阵A</strong>：</p><script type="math/tex; mode=display">A=[p(y_{t+1}=s_j|y_t=s_i)]_{N\times N}</script><p>其中下标$i,j$分别表示状态的第$i,j$种取值。将转态转移的示意图添加到上图，即：</p><p><del>假装有图</del></p><p>状态转移概率的存在有实际意义，在中文分词中标签$B$的后面不可能是$S$，于是只需赋予$p(y_{t+1}=B|y_t=S)=0$，就可以模拟这种禁止转移的需求。此外，汉语中的长词相对较少，于是隐马尔可夫模型就可以通过$p(y_{t+1}=M|y_t=M)$来模拟该语言现象。同时，词性标注中的“形容词$\to$名词”，“副词$\to$动词”也可以通过状态转移来模拟。这些概率分布的参数都是通过语料库上的统计自动学习。</p><h4 id="4-2-4-发射概率矩阵"><a href="#4-2-4-发射概率矩阵" class="headerlink" title="4.2.4 发射概率矩阵"></a>4.2.4 发射概率矩阵</h4><p>有了状态$y_t$之后，如何确定观测$x_t$的概率分布呢？根据隐马尔可夫模型2，当前观测$x_t$仅仅取决于当前状态$y_t$。给定每种$y$，$x$都是一个独立的离散型随机变量，其参数对应一个向量。假设观测$x$一共有$M$种可能的取值，即$x\in \{o_1,…,o_M\}$，则$x$的概率分布参数向量维度为$M$。由于$y$有$N$种，所以这些参数向量构成了$N\times M$的矩阵，称为<strong>发射概率矩阵B</strong>。</p><script type="math/tex; mode=display">B=[p(x_t=o_j|y_t=s_i)]</script><p>其中，第$i$行$j$列的元素下表$i$和$j$分别代表状态和观测的第$i$种和第$j$种取值，例如约定，$j=1$代表字符集中的“阿”。此时$p(x_1=阿|y_1=B)$就代表矩阵中左上角第一个元素，如果字符级是1000的话，那么$B$=1000$\times$4的矩阵。</p><p>发射概率矩阵是一个非常形象的属于：将$y_t$想象成不同的枪，$x_t$为不同颜色的子弹，根据$y_t$确定$x_t$的过程就像把枪发射子弹一样。不同的枪中弹的比例不同。</p><p>发射概率在中文分词中也具备实际意义，有些字符构词时的位置比较固定。比如作为词首的话（一把名为词首枪）不容易观测到“忑”，因为它一般作为词尾出现。通过赋予$p(x_1=忑|y_1=B)$较低的概率模型就可有效防止忐忑被错误切开。</p><p>初始状态概率向量，状态转移概率矩阵，发射概率矩阵被称为隐马尔可夫模型的<strong>三元组</strong>$\gamma=(\pi,A,B)$，只要三元组确定了，隐马尔可夫模型就确定了。</p><h4 id="4-2-5隐马尔可夫模型的三个基本用法："><a href="#4-2-5隐马尔可夫模型的三个基本用法：" class="headerlink" title="4.2.5隐马尔可夫模型的三个基本用法："></a>4.2.5隐马尔可夫模型的三个基本用法：</h4><p>隐马尔可夫模型并不局限于预测序列标注，还有以下用法：</p><ul><li>样本生成问题：给定模型$\gamma=(\pi,A,B)$，生成满足模型约束的样本，即一系列观测序列及其对应的状态序列$\{(x^{(i)},y^{(i)})\}$。</li><li>模型训练问题：给定训练集$\{(x^{(i)},y^{(i)})\}$，估计模型参数$\gamma=(\pi,A,B)$。</li><li>序列预测问题：已知模型参数$\gamma=(\pi,A,B)$，给定观测序列$x$，求最可能的状态序列$y$。</li></ul><p>这三个问题都很重要，熟练掌握样本生成问题，可以巩固对隐马尔可夫模型的基本流程的理解，而模型训练更是直接关系到最后的预测问题。接下来将介绍这些问题的解决原理与具体实现。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>4.1序列标注问题</title>
      <link href="/2024/04/30/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/"/>
      <url>/2024/04/30/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="第四章-4-1隐马尔可夫模型与序列标注"><a href="#第四章-4-1隐马尔可夫模型与序列标注" class="headerlink" title="第四章 4.1隐马尔可夫模型与序列标注"></a>第四章 4.1隐马尔可夫模型与序列标注</h2><p>第三章的n元语法模型从词语接续的流畅度出发，为全切分词网中的二元接续打分，进而利用维特比算法求解<strong>似然概率</strong>最大的路径。这种词语级别的模型无法应对$OOV$问题：$OOV$在最初的全切分阶段都不可能进入词网更何谈召回。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">头上戴着束发嵌宝紫金冠，齐眉勒着二龙抢珠金抹额</span><br></pre></td></tr></table></figure><p>当读者读到“戴着”时，已经期待一个描述帽饰的词。另外，既然存在“披肩”这种构词法，说明也存在“抹额”。人类拥有动态组词的能力。</p><p>这说明词语级别的模型天然缺乏$OOV$召回能力，我们需要更细颗粒度的模型。比词语更细的就是字符，如果字符级模型能够掌握汉字组词的规律，那么它就能够由字构词、动态地识别新词汇，而不局限于词典了。</p><p>具体来说，只要将每个汉字所处的位置（首尾等）作为标签则中文分词就转化为给定汉字序列找出标签序列的问题。一般而言，由字构词是“序列标注”模型的一种应用。在所有“序列标注”模型中，隐马尔可夫模型是最基础的一种。本章先介绍序列标注问题的定义及应用，然后讲述实现隐马尔可夫模型，最终将其应用到中文分词上去。</p><h3 id="4-1-序列标注问题"><a href="#4-1-序列标注问题" class="headerlink" title="4.1 序列标注问题"></a>4.1 序列标注问题</h3><p> <strong>序列标注</strong>问题指的是有原始数据序列$x=\{x1,x2,x3…\}$，找出每个元素对应的标签序列为$y=\{y1,y2,y3…\}$的问题。其中，$y$所有可能取值叫做<strong>标注集</strong>。比如，输入一个自然数，输出它们的奇偶性，其标注集为$\{奇，偶\}$。</p><p><img src="https://12345picture.oss-cn-hangzhou.aliyuncs.com/202404302046281.png" alt="2020-2-6_10-36-13"></p><p>数字奇偶性的判断只取决于当前的元素，这是最简单的情况。然而，大多数情况需要考虑元素前后其他元素以及之前的标签才能再做决定。比如在“小猫钓鱼”这个扑克牌游戏中，双方轮流出牌，当出现与之前牌相同的牌时，出牌人收走两张牌之间的所有牌、如果将出牌顺序记作序列$x$，是否应当收牌记作$y$，这就转化为一个序列标注的问题了。如下图所示：</p><p><del>假装有图</del></p><p>求解序列标注问题的模型一般称为“<strong>序列标注器</strong>”，通常由模型从一个标注数据集$\{X,Y\}={(x^{(i)}，y^{(i)})},i=1,…,$K中学习相关知识后再进行预测。在<strong>NLP</strong>问题中，$x$一般是词语或者字符，$y$一般是待预测的组词角色或者词性等标签。无论是第三章的中文分词，第七章的词性标注，第八章的命名实体识别都可转化为序列标注问题。</p><h4 id="4-1-1序列标注与中文分词"><a href="#4-1-1序列标注与中文分词" class="headerlink" title="4.1.1序列标注与中文分词"></a>4.1.1序列标注与中文分词</h4><p>考虑一个字符串$x$，想象分词器真的是一把刀在切$x$，那么字符串中的字符在分词时无非充当两种角色，要么在i之后切开，要么跳过不切。那么中文分词就转化为标注集为$\{切，过\}$的序列标注问题，如下图所示：</p><p><del>这次真有图</del></p><p><img src="https://12345picture.oss-cn-hangzhou.aliyuncs.com/202404302113974.png" alt="2020-2-6_10-55-28"></p><p>只要标注器正确标注出每个字符切与过的结果，那么分词器就能够按照标注器的指示切割出正确的结果。可以将序列标注看作中文分词的中间结果，往后则是纯粹的字符串分割逻辑。</p><p>分词标注集并非只有一种，为了捕捉汉字分别作为词语首尾，词中，单字成词的概率，人们提出了$\{B,M,E,S\}$这种最流行的标注集，如下图所示：</p><p><img src="https://12345picture.oss-cn-hangzhou.aliyuncs.com/202404302117622.png" alt="2020-2-6_11-4-22"></p><p>标注后，分词器将最近的两个<strong>BE</strong>标签区间内的所有字符合并作为一个词语，S标签表示字符作为一个单字词语，按顺序输出即完成分词过程。</p><h4 id="4-1-2序列标注与词性标注"><a href="#4-1-2序列标注与词性标注" class="headerlink" title="4.1.2序列标注与词性标注"></a>4.1.2序列标注与词性标注</h4><p>词性标注是一个天然的序列标注问题：$x$是单词序列，$y$是相应词性序列，如下图所示：</p><p><img src="https://12345picture.oss-cn-hangzhou.aliyuncs.com/202404302121666.png" alt="2020-2-6_11-11-0"></p><p>词性标注集同样不是唯一的，人们根据需要制定了不同的数据集，比较著名的是863标注集以及北大标注集，前者词性数量少一些，颗粒度更大，这将在未来章节详细介绍。</p><p>词性标注与”小猫钓鱼”类似，需要综合考虑前后字符与它们的词性，根据这些再来决定当前字符的词性。比如副词之后跟动词，“的”之后容易出现名词。这里的“容易”其实意味着较大的概率，需要用概率模型去模拟。</p><h4 id="4-1-3-序列标注与命名实体识别"><a href="#4-1-3-序列标注与命名实体识别" class="headerlink" title="4.1.3 序列标注与命名实体识别"></a>4.1.3 序列标注与命名实体识别</h4><p>所谓命名实体，指的是现实存在的实体，比如人名，地名和机构名。命名实体是$OOV$的主要组成部分，往往也是句子中最令人关注的部分。命名实体的数量是无穷的，因为世界上每种事物都有一个名字代表自身。比如每颗星星，每种蛋白质都有自己的名字，这些显然都不可数。</p><p>简短的人名和地名可以通过中文分词进行切分，然后通过词性标注确定所属类别。但地名和机构名往往由多个单词组成(称为<strong>复合词</strong>)，较难识别。由于<strong>复合词</strong>的丰度较小，导致分词器和词性标注很难一步到位将其识别出来，一般需要在中文分词和词性标注的中间结果上进行召回。</p><p>考虑到字符级别的中文分词和词语级别的命名实体识别有着类似的特点，都是组合短单位形成长单位的问题。所以命名实体识别可以复用$BMES$标注集，并沿用中文分词的逻辑，只不过标注的对象由字符变成单词而已。但命名实体识别还需确定单词所属的类别。这个要求依然是个标注问题，可以通过将命名实体识别附着于$BMES$标签的方法解决。比如，构成地名的单词标注为“$B/M/E/S-地名$”以此类推。对于不构成命名实体识别的单词，统一用$O$标注，即复合词之外，如图所示：</p><p><img src="https://12345picture.oss-cn-hangzhou.aliyuncs.com/202404302143203.png" alt="2020-2-6_11-23-10"></p><p>命名实体识别模块，根据标注结果，将“北京”和“天安门”作为首尾组合成词，并标注为地名。</p><p>总之，序列标注问题是NLP常见问题之一。许多应用任务都可以变换思路，转化为序列标注来解决。所以一个准确的序列标注模型非常重要，直接关系到NLP系统的准确率。机器学习领域为NLP提供了许多模型，本着循序渐进的原则，讲述最基础的一个—<strong><em>隐马尔可夫模型</em></strong>。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>3.5 评测</title>
      <link href="/2024/04/28/3.5%20%E8%AF%84%E6%B5%8B/"/>
      <url>/2024/04/28/3.5%20%E8%AF%84%E6%B5%8B/</url>
      
        <content type="html"><![CDATA[<h3 id="3-5-评测"><a href="#3-5-评测" class="headerlink" title="3.5 评测"></a>3.5 评测</h3><p>按照NLP任务的一般流程，我们已经完成了语料标注与模型训练，本节进行最后一道工序：标准化评测。</p><h4 id="3-5-1-标准化评测"><a href="#3-5-1-标准化评测" class="headerlink" title="3.5.1 标准化评测"></a>3.5.1 标准化评测</h4><p>利用MSR语料库进行标准化评测，评测分为训练、预测、计算准确率3步。</p><p>python评测程序：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_bigram(msr_train,msr_model)<span class="comment">#训练</span></span><br><span class="line">segment=load_bigram(msr_model)<span class="comment">#加载</span></span><br><span class="line">result=CWSEvaluator.evaluate(segment,msr_test,msr_output,msr_gold,msr_dict)<span class="comment">#打分</span></span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><h4 id="3-5-2-误差分析"><a href="#3-5-2-误差分析" class="headerlink" title="3.5.2 误差分析"></a>3.5.2 误差分析</h4><p>将二元语法与词典分词的结果汇总后：</p><div class="table-container"><table><thead><tr><th>算法</th><th>p</th><th>R</th><th>F~1~</th><th>R~oov~</th><th>R~IV~</th></tr></thead><tbody><tr><td>最长匹配</td><td>89.41</td><td>94.64</td><td>91.95</td><td>2.58</td><td>97.14</td></tr><tr><td>二元语法</td><td>92.38</td><td>96.70</td><td>94.49</td><td>2.58</td><td>99.26</td></tr></tbody></table></div><p>相较于词典分词，二元语法在精确率、召回率以及$IV$召回率上全面胜出，最总$F_1$值提高了2.5%，成绩的提高主要受惠于消歧能力的提高。同时，我们也注意到$OOV$召回能力没有任何提升。因为词网中几乎所有的中文词语都来自于训练集词典，自然无法召回新词。</p><p>接下来比较预测输出与标准答案之间的差别。通过样本可知：人名，地名，机构名等oov识别能力太差，最后一个则是消歧失误。验证了上述猜测：消歧能力尚可，OOV召回能力太低。</p><h4 id="3-5-3-调整模型"><a href="#3-5-3-调整模型" class="headerlink" title="3.5.3 调整模型"></a>3.5.3 调整模型</h4><p>上述前4个样本都可以通过用户词典弥补，在词典中添加相应人名，地名，机构名。</p><p>这里每个单词的词性后会在第7章详细介绍，对分词结果影响不大。词性后面的频次越大越容易切分，建议先选取一个比较小的值，若不够再逐步增加。然后删除缓存，并启用用户分词器的词典$segment.enableCusttomDictionary(true)$，启动程序静待缓存重新生成。此外，用户还可将上述词条加入到核心词典中，因为HanLP的用户词典与核心词典完全相同。</p><p>但第5个句子无法通过一元语法词典解决，因为词语已经在核心词典中，我们可以创建易于调试的DijkstraSegment，并打开调试模式，追踪分词过程。</p><p>python:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Segment=load_bigram(model_path_msr_model,verbose=<span class="literal">False</span>,ret_viterbi=<span class="literal">False</span>)</span><br><span class="line"><span class="keyword">assert</span> CoreDictionary.contains(<span class="string">&quot;管道&quot;</span>)</span><br><span class="line">text=<span class="string">&quot;北京输气管道工程&quot;</span></span><br><span class="line">HanLP.config.enableDebug()</span><br><span class="line"><span class="built_in">print</span>(segment.seg(text))</span><br></pre></td></tr></table></figure><p><img src="https://12345picture.oss-cn-hangzhou.aliyuncs.com/202404281707644.png" alt="image-20240428170726494"></p><p>这份调试是词网中所有边的列表：第一列$to$是边的终点，第二列$from$是边的起点，第三列$weight$是边的话费，第四列$word$是二元语法。我们希望分词器选择“北京 输 气 管道 工程”这条路径，该路径包含的二元语法略。</p><p>错误结果为“北京 输 气管 道 工程”两条路径区别在于“输”是往“气”走还是往“气管”走，模型给出的话费都是11.55，但“管道 工程”比“道 工程”的话费多太多，而“气 管道”比“气管 道”花费少不了多少，那就应该让这条路径上的话费尽可能小。花费与二元语法频次负相关。为此需要编辑二元语法模型$.ngram.txt$，增加下列二元语法及频次：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输@气 1</span><br><span class="line">气@管道 1</span><br><span class="line">管道@工程 1</span><br></pre></td></tr></table></figure><p>然后删除对应的缓存文件即可。运行就能得到正确答案。</p><p>比较调试内容，发现正确路径上每条边的花费已经降至个位数了，人工调整取得了成功，最终取得了正确的结果。另外，如果二元语法模型中已经存在某个二元语法，直接将后面的频次增大点即可。</p><p>任何语料、模型都不是完美的，HanLP独特的“人工调整模型”技巧可以在不重新训练的条件下，得到想要的结果。事实上，这种人工调整与<strong>增量训练</strong>的本质是一样的：如果我们往语料库中新增一句“输 气 管道 工程”并重新训练，也能得到同样的结果。人工调整只不过是让这个过程更快、成本更低而已。随着用户调整二元语法越多，二元语法模型就越符合用户的习惯，但人工新增的二元语法，一元语法并没有得到更新，用户指定的频次也未必符合统计规律，可能产生一些副作用。</p><p>总之，尽量使用语料标注及统计方法解决问题。</p><h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>这一章讲述了NLP重要的概念之一：语言模型。为了学习自然语言模型，我们标注了一份微型语料库，并在这上面进行极大似然法估计了统计二元语法的参数，捕捉了二元接续的统计知识。统计方法中，<strong>数据稀疏</strong>是永恒的问题，为此我们尝试了平滑策略。为了搜索最大概率的分词序列，我们将中文分词转化为有向无环图的最短路问题。为了高效求解词网上的最短路，我们学习了维特比算法。</p><p>相较于词典分词，二元语法分词在MSR语料库上的$F_1$值提高了$25\%$。通过误差分析，明白词典并非万能的，并学习了HanLP独有的模型调整。</p><p>然而$OOV$召回是$n$元语法的硬伤，我们需要更加强大的统计模型。</p><p><img src="https://12345picture.oss-cn-hangzhou.aliyuncs.com/202404281755677.webp" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>3.4预测</title>
      <link href="/2024/04/27/3.4%E9%A2%84%E6%B5%8B/"/>
      <url>/2024/04/27/3.4%E9%A2%84%E6%B5%8B/</url>
      
        <content type="html"><![CDATA[<p>-</p><h3 id="3-4预测"><a href="#3-4预测" class="headerlink" title="3.4预测"></a>3.4预测</h3><p>在机器学习和自然语言处理的语境下，<strong>预测</strong>指的是利用模型对于样本进行推断的过程，在中文分词任务中也就是利用模型推测分词序列。有时候也称为解码。在中文分词任务中，我们已经训练了二元语法模型，如何解码它呢？</p><p>HanLP中二元语法的解码由ViterbiSegment分词器提供，本节将介绍算法原理以及工程实现。</p><h4 id="3-4-1加载模型"><a href="#3-4-1加载模型" class="headerlink" title="3.4.1加载模型"></a>3.4.1加载模型</h4><p>第一步是将模型加载到内存中。对于一元语法模型，HanLP提供了CoreDictionary类，对于二元语法模型则是CoreBiGramTableDictionary。由于早期设计中系统仅需一套模型，所以均为静态工具类，对应全局唯一的模型。使用时不需创建实例，只需修改配置文件中的模型即可。</p><p>Python:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_bigram</span>(<span class="params">model_path</span>):</span><br><span class="line">    HanLP.Config.CoreDictionaryPath=model_path+<span class="string">&quot;.txt&quot;</span></span><br><span class="line">    HanLP.Config.CoreBiGramTableDictionary=model_path+<span class="string">&quot;.ngram.txt&quot;</span></span><br><span class="line">    CoreDictionary=SafeJClass(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">    CoreBiGramTableDictionary=SafeJClass(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(CoreDictionary.getTermFrequency(<span class="string">&quot;商品&quot;</span>))</span><br><span class="line">    <span class="built_in">print</span>(CoreBiGramTableDictionary.getBiFrequency(<span class="string">&quot;商品&quot;</span>，<span class="string">&quot;和&quot;</span>))</span><br></pre></td></tr></table></figure><p>此处我们直接是用来$CoreDictionary.getTermFrequency$获取了”商品”的词频，调用$CoreBiGramTableDictionary.getBiFrequency$获取了”商品 和“的二元语法词频。</p><p>$CoreDictionary$内部使用DoubleArrayTrie来存储词典，性能极高。$CoreBiGrameTableDictionary$没有存储字符串形式的词语，而是依赖$CoreDictionary$查询词语的id,然后根据两个词语的id查询二元语法的频次。还有一种压缩算法，仅仅使用了两个整数数组。</p><h4 id="3-4-2构建词网"><a href="#3-4-2构建词网" class="headerlink" title="3.4.2构建词网"></a>3.4.2构建词网</h4><p>词网指的是句子中所有一元语法构成的网状结构，是<strong>HanLP</strong>工程上的概念。比如“商品和服务”这个句子，给定一元语法词典，我们将句子中所有单词都找出来。起始位置(offset)相同的单词写在一起，得到如下词网：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0:[]</span><br><span class="line">1:[商品]</span><br><span class="line">2:[ ]</span><br><span class="line">3:[和，和服]</span><br><span class="line">4:[服务]</span><br><span class="line">5:[务]</span><br><span class="line">6:[ ]</span><br></pre></td></tr></table></figure><p>其中首尾分别对应<strong>始##始</strong>，<strong>末##末</strong>，此处写作空格。</p><p>词网的创建就是利用DoubleArrayTrie中的Searcher扫描出句子中所有的一元语法及其位置而已，与词典分词的全切分概念类似。python:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate_wordnet</span>=nerate_wordnet(sent,trie):</span><br><span class="line">    searcher=trie.getSearcher(JCstring(sent),<span class="number">0</span>)</span><br><span class="line">    wordnet=WordNet(sent)</span><br><span class="line">    <span class="keyword">while</span> searcher.<span class="built_in">next</span>():                  wordnet.add(searcher.begin+<span class="number">1</span>,Vertex(sent[searcher.begin:searcher.begin+searcher.length],searcher.value,searcher.index))</span><br><span class="line">    <span class="comment">#原子分词，保证图联通</span></span><br><span class="line">    vertexes=wordnet.getVertexes()</span><br><span class="line">    i=<span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i&lt;<span class="built_in">len</span>(vertexes):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(vertexes[i])==<span class="number">0</span>:</span><br><span class="line">            j=i+<span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>,<span class="built_in">len</span>(vertexes)-<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span>(<span class="built_in">len</span>(vertexes[j])):</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            wordnet.add(i,Vertex.newPunctuationInstance(sent[i-<span class="number">1</span>:j-<span class="number">1</span>]))</span><br><span class="line">            i=j</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            i+=<span class="built_in">len</span>(vertexes[i][-<span class="number">1</span>].realWord)</span><br><span class="line">    <span class="keyword">return</span> wordnet</span><br></pre></td></tr></table></figure><p>词网必须保证从起点出发的所有路径都能通往终点。行5的务是一元语法中并不存在的词语，它的存在就是为了保证词网联通而添加的单字词语。</p><p>以上是词网的创建逻辑，下面来看看这些词语如何进行彼此连接。按行索引的节点具有一个极佳的性质，就是第i行的单词w能与$i+len(w)$行的所有词语相连构成二元语法。例如：第0行的始##始，它的长度是1，能与商品构成二元语法，例如商品，能与3行的[和 和服]构成二元语法，以此类推，词网中第i行的单词w与i+len(w)行的所有单词相互连接，构成一个“词图”。</p><p><img src="https://12345picture.oss-cn-hangzhou.aliyuncs.com/202404271552774.png" alt=""></p><p>从起点到终点的每条路径代表一种分词方式，二元语法模型的解码任务就是找出其中最合理的那条路径（图3-1加粗的“商品 和 服务”路径）。</p><h4 id="3-4-3节点间的距离计算"><a href="#3-4-3节点间的距离计算" class="headerlink" title="3.4.3节点间的距离计算"></a>3.4.3节点间的距离计算</h4><p>如果我们将上述词图每条边以二元语法的概率作为距离，那么中文分词任务就转化成了求一个有向无环图的最长路径问题。</p><p>二元语法概率可以用$MLE$辅以平滑策略得到，在中文分词里经常使用如下经验公式：</p><script type="math/tex; mode=display">\hat{p}(w_t|w_{t-1}=\gamma \left[\mu \frac{c(w_{t-1}w_t)}{c(w_{t-1})}+1-\mu \right])</script><p>其中$\gamma,\mu \in(0,1)$为两个不同的平滑因子，也即上式额外做了一次平滑。频次加一也是一种平滑策略，称之为<strong>加一平滑</strong>，或者<strong>拉普拉斯平滑</strong>。</p><p>考虑到对浮点数连续相乘之后会出现下溢出（等于0），因此工程上经常对概率取负对数，将浮点数乘法转化为负对数之间的加法：</p><script type="math/tex; mode=display">\prod_{t=1}^{k+1}\hat{p}(w_t|w_{t-1})\to\sum_{t=1}^{k+1}log(\hat{p}(w_t|w_{t-1}))</script><p>相应，词图上的最长路径转化为负对数的最短路径。</p><p>为上图的每条路径计算距离后得：</p><h4 id="3-4-4-词图上的维特比算法"><a href="#3-4-4-词图上的维特比算法" class="headerlink" title="3.4.4 词图上的维特比算法"></a>3.4.4 词图上的维特比算法</h4><p>如何解决词图上的最短路径问题？假设文本长度为n，则共有$2^{n-1}$种切分方式，因为每2个字符间都有两种选择：切或不切。因此暴力枚举的复杂度是$O(2^{n-1})$，不可行。</p><p>如果用动态规划的思想做，维护记录源点到每个点的最短路径，则可以节省很多运算。图上的最短路径还有很多种，例如Bellman-Ford以及Dijkstra算法。在自然语言领域应用维特比算法处理由马尔科夫链构成的网状图。</p><p>维特比算法分为前向和后向两个步骤。</p><p>(1)<strong>前向</strong>：由起点出发从前往后遍历节点，更新从起点到该节点的最小花费以及前驱指针。</p><p>(2)<strong>后向</strong>：由终点出发从后往前回溯前驱指针，取得最短路径。</p><p>python:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">viterbi</span>(<span class="params">wordnet</span>):</span><br><span class="line">    nodes=wordnet.getVertexes()</span><br><span class="line">    <span class="comment">#前向遍历</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="built_in">len</span>(nodes)-<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> nodes[i]:</span><br><span class="line">            <span class="keyword">for</span> to <span class="keyword">in</span> nodes[i+<span class="built_in">len</span>(node.realWord)]:</span><br><span class="line">                to.updateFrom(node)<span class="comment">#根据距离公式计算节点距离，并维护最短路径上的前驱指针from</span></span><br><span class="line">    <span class="comment">#向后回溯</span></span><br><span class="line">    path=[]<span class="comment">#最短路径</span></span><br><span class="line">    f=node[<span class="built_in">len</span>[nodes]-<span class="number">1</span>].getFirst()<span class="comment">#从终点回溯</span></span><br><span class="line">    <span class="keyword">while</span> f:</span><br><span class="line">        path.insert(<span class="number">0</span>,f)</span><br><span class="line">        f=f.getFrom()</span><br><span class="line">    <span class="keyword">return</span> [v.realWord <span class="keyword">for</span> v <span class="keyword">in</span> path]</span><br></pre></td></tr></table></figure><p>每个node都维护着从起点到自己的最小花费，以及相应的前驱指针from。updateFrom负责维护这两个成员。</p><p>测试，现为商品和服务创建词图，最后运行维特比算法：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_bigram</span>(<span class="params">model_path</span>):</span><br><span class="line">    HanLP.Config.CoreDictionaryPath=model_path+<span class="string">&quot;.txt&quot;</span></span><br><span class="line">    HanLP.Config.CoreBiGramTableDictionary=model_path+<span class="string">&quot;.ngram.txt&quot;</span></span><br><span class="line">    sent=<span class="string">&quot;商品和服务&quot;</span></span><br><span class="line">wordnet=generate_wordnet(sent,CoreDictionary.trie)</span><br><span class="line">    <span class="built_in">print</span>(viterbi(wordnet))</span><br><span class="line">    <span class="keyword">return</span> ViterbiSegment().enableAllNamedEntityRecognize(<span class="literal">False</span>).enableCustomDictionarry(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;_main_&#x27;</span>:</span><br><span class="line">    corpus_path=my_cws_corpus()</span><br><span class="line">    model_path=os.path.join(test_data_path(),<span class="string">&#x27;my_cws_model&#x27;</span>)</span><br><span class="line">    train_bigram(corpus_path,model_path)</span><br><span class="line">    load_bigram(model_path)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[&#x27; &#x27;,&#x27;商品&#x27;,&#x27;和&#x27;，&#x27;服务&#x27;,&#x27; &#x27;]</span><br></pre></td></tr></table></figure><p>如果将sent换成“货币和服务”结果依然正确。至此，我们走通了语料标注，模型训练，预测分词结果的完整流程，初步体验了统计自然语言处理的魅力。</p><h4 id="3-4-5-与用户词典的集成"><a href="#3-4-5-与用户词典的集成" class="headerlink" title="3.4.5 与用户词典的集成"></a>3.4.5 与用户词典的集成</h4><p>语料库需要人工标注，规模有限，训练出来的一元语法模型也有限，导致词网中不可能会出现oov,这导致几乎所有的专业术语、网络新词都会成为oov，给二元语法分词带来了巨大的挑战。</p><p>而词典往往廉价易得，资源丰富。利用统计模型的消歧能力，再辅以用户词典处理新词，是提高分词器准确率的有效方式。在使用词典之前必须明白词典不是万能的。</p><p>那么HanLP中的词典有什么作用呢？HanLP中的所有分词器都支持用户词典，还支持2档用户词典优先级。</p><ol><li><strong>低优先级</strong>下，分词器首先在不考虑用户词典的情况下由统计模型预测分词结果，最后将该结果按照用户词典合并。比如统计分词结果$商品\,和\,服务\,员$，用户词典中有$和服\,服务员$两个词，那么合并结果为$商品\,和\,服务员$，即便用户词典中有和服也不影响。</li><li><strong>高优先级</strong>下，分词器首先考虑用户词典，但具体实现由分词器子类决定。在二元语法分词器$ViterbiSegment和DijkstraSegment$中，采用干预词网生成的方式实现：也就是先扫描一遍用户词典，将句子中出现过的所有用户词语加入词网。于是，高优先级下词网中可能含有用户词典中的单词。用户词语的一元语法词频由用户提供，但二元语法频次依然缺失。它们的二元语法由程序伪造为与一元语法频次相同。这样用户就可以通过词频来干预每个用户词语的优先级。</li></ol><p>中文分词不等价于收集词典，词典无法解决中文分词。也许读者以为往词典中加入“川普”，并且用户词典优先级高的话，就可以解决眼前的”普京与川普通话”这个句子。但在没注意的地方是，有更多类似于”四川普通话”,”银川普通话考试”的句子会发生错误。HanLP坚持的是统计为主，词典为辅的思路，力争假设用户加入”川普“这样的词条仍然能区分上述句子的效果。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">segment=ViterbiSegmrnt()</span><br><span class="line">sentence=<span class="string">&quot;社会摇摆简称社会摇&quot;</span></span><br><span class="line">segment.enableCustomDictionary(<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(segment.seg(sentense))</span><br><span class="line">segment.enableCustomDictionary(<span class="literal">True</span>)<span class="comment">#低优先级</span></span><br><span class="line">segment.enableCustomDictionaryForcing(<span class="literal">True</span>)<span class="comment">#高优先级</span></span><br></pre></td></tr></table></figure><p><img src="https://12345picture.oss-cn-hangzhou.aliyuncs.com/202404271735731.png" alt="image-20240427173503663"></p><p>可见，用户词典的高优先级未必是件好事，HanLP中的用户词典默认低优先级，可根据实际需求自行开启高优先级。当用户词典中的词语一定需要分出来时可以开启高优先级。当词语长度较长，不容易产生歧义时，也可开启高优先级。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>3.3训练</title>
      <link href="/2024/04/26/3.3%E8%AE%AD%E7%BB%83/"/>
      <url>/2024/04/26/3.3%E8%AE%AD%E7%BB%83/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="3-3训练"><a href="#3-3训练" class="headerlink" title="3.3训练"></a>3.3训练</h3><p>语料就绪之后我们就可开始训练了，<strong>训练</strong>的概念就是给定样本集估计模型参数的过程。训练也称作编码，因为学习算法将知识以参数的形式编码至模型中。对于本章的二元语法模型，训练指的就是统计二元语法频次以及一元语法频次，有了频次通过极大似然估计以及平滑策略。我们就可以任意估计句子的概率分布，亦即得到了语言模型。</p><p>我们先以小数据集进行测试，然后再切换到大数据集。最后在MSR上进行大规模模型训练以及标准化评测，与词典分词的结果进行比较。</p><h4 id="3-3-1加载语料库"><a href="#3-3-1加载语料库" class="headerlink" title="3.3.1加载语料库"></a>3.3.1加载语料库</h4><p>考虑到语料库的处理是NLP工程师的日常工作之一，HanLP提供了许多封装好的工具。对于空格分隔的分词语料库来讲，可利用HanLP提供的CorpusLoader.convert2SentenceList加载。</p><p>Python版如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sents=CorpusLoader.convert2SentenceList(corpus_path)</span><br><span class="line"><span class="keyword">for</span> sent <span class="keyword">in</span> sents:</span><br><span class="line">    <span class="built_in">print</span>(sent)</span><br></pre></td></tr></table></figure><p>结果都是：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[商品， 和， 服务]</span><br><span class="line">[商品， 和服， 物美价廉]</span><br><span class="line">[服务， 和， 货币]</span><br></pre></td></tr></table></figure><h4 id="3-3-2统计一元语法"><a href="#3-3-2统计一元语法" class="headerlink" title="3.3.2统计一元语法"></a>3.3.2统计一元语法</h4><p>一元语法其实就是单词，如果把单词与词频写成纯文本格式，就得到了一部词频词典。有些语料库含有人工标注的词性，因此词典格式最好还支持词性，这也就是HanLP词典格式的设计初衷。</p><p>在HanLP中，一元语法的统计功能由DictionaryMarker提供，二元语法的统计功能由NGramDictionaryMaker提供。语言模型由一元语法和二元语法构成，因此HanLP提供两者的包装类：NatureDictionaryMaker。</p><p>Python:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_bigram</span>(<span class="params">corpus_path,model_path</span>):</span><br><span class="line">    sents=CorpusLoader.convert2SentenceList(corpus_path)</span><br><span class="line">    <span class="keyword">for</span> sent <span class="keyword">in</span> sents:</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sent:</span><br><span class="line">            word.setLabel(<span class="string">&quot;n&quot;</span>)</span><br><span class="line">    maker=NatureDictionaryMaker()</span><br><span class="line">    maker.compute(sents)</span><br><span class="line">    maker.saveTxtTo(model_path)</span><br></pre></td></tr></table></figure><p>加载语料后，赋予了每个单词一个虚拟的n词性，仅用作占位符。然后创建了NatureDictionaryMaker()对象，将所有句子传入compute方法完成统计。统计结果以.txt格式保存到model_path指定的路径中。</p><p>运行后得到以下文件：</p><ol><li>my_cws_model.txt:一元语法模型</li><li><p>my_cws_model.ngram.txt:二元语法模型</p></li><li><p>my_cws_model.tr.txt:与词性标注有关，暂时忽略</p></li></ol><p>代开1：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">和 n 2</span><br><span class="line">和服 n 1</span><br><span class="line">商品</span><br><span class="line">始##始 begin 3</span><br><span class="line">服务 n 2</span><br><span class="line">末##末 end 3</span><br><span class="line">物美价廉 n 1</span><br><span class="line">货币 n 1</span><br></pre></td></tr></table></figure><p>这就是标准的HanLP词典的格式，分别为单词，词性，词频。其中始、末代表句子开头、结尾。</p><p>在compute接口中，NatureDictionaryMaker灵活调用了DictionaryMaker的add方法。该方法将单词添加到一棵泛型BinTrie<Item>中，而<Item>就是存储词性和词频的结构，HanLP的泛型设计解耦了数据结构与业务逻辑，使底层算法灵活驱动了上层应用。</p><h4 id="3-3-3-统计二元语法"><a href="#3-3-3-统计二元语法" class="headerlink" title="3.3.3 统计二元语法"></a>3.3.3 统计二元语法</h4><p>NatureDictionaryMaker已经完成了二元语法统计，直接查看二元语法训练效果：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">和 服务 1</span><br><span class="line">和 货币 1</span><br><span class="line">和服 物美价廉 1</span><br><span class="line">商品 和 1</span><br><span class="line">商品 和服 1</span><br><span class="line">始##始 商品 2</span><br><span class="line">始##始 服务 1</span><br><span class="line">服务 和 1</span><br><span class="line">服务 末##末 1</span><br><span class="line">物美价廉 末##末 1</span><br><span class="line">货币 末##末 1</span><br></pre></td></tr></table></figure><p>前面是二元语法中的两个单词，后面是二元语法的频次。</p><p>至此我们就完成了二元语法模型的训练，这是训练的第一个模型，原理无非是词频统计，代码也只有几行。模型的存储是熟悉的.txt，看上去和词典没什么两样。的确，统计方法和机器学习就是这么简单。</p><p>模型和词典也就是形式上的不同，HanLP将二元以及一元语法的存储称作词典形式，是为了兼容词典分块模块。本书叙述抽象原理时尽量使用模型来称呼它们。</p><p>接下来介绍二元语法模型如何分词。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>3.2中文分词语料库</title>
      <link href="/2024/04/24/3.2%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E8%AF%AD%E6%96%99%E5%BA%93/"/>
      <url>/2024/04/24/3.2%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E8%AF%AD%E6%96%99%E5%BA%93/</url>
      
        <content type="html"><![CDATA[<h3 id="3-2中文分词语料库"><a href="#3-2中文分词语料库" class="headerlink" title="3.2中文分词语料库"></a>3.2中文分词语料库</h3><p>语言模型只是函数的一个骨架，函数的参数需要在语料库上统计才能得到，所以选择一个合适好用的语料库是一件至关重要的事情。本节将介绍一些常用的中文分词语料库，为后面的参数统计提供样本。</p><p>统计中文分词字20世纪90年代积发展至今，积累了一些标注语料库，由于分词标准难以制定，各大公司都制定了自己的标准发展。另一方面，中文分词是门槛最低的NLP任务，研究者众多。这些原因使得中文分词语料库呈现出百家争鸣的现象。大大小小的语料库加起来有不下十种，总字数在千万级别。每个语料库都根据规范由语言学专家标注，包含大量人力物力成本，可谓黄金数据。</p><h4 id="3-2-1-1998年《人民日报》语料库PKU"><a href="#3-2-1-1998年《人民日报》语料库PKU" class="headerlink" title="3.2.1 1998年《人民日报》语料库PKU"></a>3.2.1 1998年《人民日报》语料库PKU</h4><p>《人民日报》语料库由北大和富士通联合标注，是最著名的中文分词语料库。《人民日报》语料库目前一共公开了6个月。</p><h4 id="3-2-2微软亚洲研究院语料库MSR"><a href="#3-2-2微软亚洲研究院语料库MSR" class="headerlink" title="3.2.2微软亚洲研究院语料库MSR"></a>3.2.2微软亚洲研究院语料库MSR</h4><p>优点：</p><ol><li>标注一致性上更优</li><li>切分颗粒度上更大，比如对机构名不予切分</li><li>姓名作为一个整体</li><li>量级几乎是PKU两倍</li></ol><h4 id="3-2-3繁体中文分词语料库"><a href="#3-2-3繁体中文分词语料库" class="headerlink" title="3.2.3繁体中文分词语料库"></a>3.2.3繁体中文分词语料库</h4><p>一般而言，繁体用户不需要专门使用繁体语料库，HanLP中有“字符规正化”功能，可将繁体转化成简体使用。如果追求短平快的话，不需要专门标注繁体中文语料库，追求精致的话则需要。</p><h4 id="3-2-4语料库统计"><a href="#3-2-4语料库统计" class="headerlink" title="3.2.4语料库统计"></a>3.2.4语料库统计</h4><p>作为数据科学从事者，我们需要熟悉数据，对于语料库而言就是统计语料库次数，词语种数，总词频等。<strong>词语种数</strong>指的是语料库中有多少个不重复的词语；而<strong>总词频</strong>指的是所有词语的词频之和。分别用来衡量语料库的丰富程度和规模大小。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">count_corpus</span>(<span class="params">train_path: <span class="built_in">str</span>, test_path: <span class="built_in">str</span></span>):</span><br><span class="line">    <span class="comment">#统计训练集和测试集的字符数、词语种数、总词频</span></span><br><span class="line">    train_counter, train_freq, train_chars = count_word_freq(train_path)</span><br><span class="line">    test_counter, test_freq, test_chars = count_word_freq(test_path)</span><br><span class="line">    <span class="comment">#统计测试集中不在训练集词语总数</span></span><br><span class="line">    test_oov = <span class="built_in">sum</span>(test_counter[w] <span class="keyword">for</span> w <span class="keyword">in</span> (test_counter.keys() - train_counter.keys()))</span><br><span class="line">    <span class="keyword">return</span> train_chars / <span class="number">10000</span>, <span class="built_in">len</span>(</span><br><span class="line">        train_counter) / <span class="number">10000</span>, train_freq / <span class="number">10000</span>, train_chars / train_freq, test_chars / <span class="number">10000</span>, <span class="built_in">len</span>(</span><br><span class="line">        test_counter) / <span class="number">10000</span>, test_freq / <span class="number">10000</span>, test_chars / test_freq, test_oov / test_freq * <span class="number">100</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_word_freq</span>(<span class="params">train_path</span>):</span><br><span class="line">    f = Counter()</span><br><span class="line">    <span class="comment">#打开训练集，逐行读取并统计词频</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(train_path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> src:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> src:</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> re.<span class="built_in">compile</span>(<span class="string">&quot;\\s+&quot;</span>).split(line.strip()):</span><br><span class="line">                f[word] += <span class="number">1</span></span><br><span class="line">    <span class="comment">#返回词频字典，总词频和平均长度</span></span><br><span class="line">    <span class="keyword">return</span> f, <span class="built_in">sum</span>(f.values()), <span class="built_in">sum</span>(<span class="built_in">len</span>(w) * f[w] <span class="keyword">for</span> w <span class="keyword">in</span> f.keys())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    sighan05 = ensure_data(<span class="string">&#x27;icwb2-data&#x27;</span>, <span class="string">&#x27;http://sighan.cs.uchicago.edu/bakeoff2005/data/icwb2-data.zip&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;|语料库|字符数|词语种数|总词频|平均词长|字符数|词语种数|总词频|平均词长|OOV|&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> <span class="string">&#x27;pku&#x27;</span>, <span class="string">&#x27;msr&#x27;</span>, <span class="string">&#x27;as&#x27;</span>, <span class="string">&#x27;cityu&#x27;</span>:</span><br><span class="line">        train_path = os.path.join(sighan05, <span class="string">&#x27;training&#x27;</span>, <span class="string">&#x27;&#123;&#125;_training.utf8&#x27;</span>.<span class="built_in">format</span>(data))</span><br><span class="line">        test_path = os.path.join(sighan05, <span class="string">&#x27;gold&#x27;</span>,</span><br><span class="line">                                 (<span class="string">&#x27;&#123;&#125;_testing_gold.utf8&#x27;</span> <span class="keyword">if</span> data == <span class="string">&#x27;as&#x27;</span> <span class="keyword">else</span> <span class="string">&#x27;&#123;&#125;_test_gold.utf8&#x27;</span>).<span class="built_in">format</span>(data))</span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">&#x27;|%s|%.0f万|%.0f万|%.0f万|%.1f|%.0f万|%.0f万|%.0f万|%.1f|%.2f%%|&#x27;</span> % (</span><br><span class="line">                (data.upper(),) + count_corpus(train_path, test_path)))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>1处使用正则表达式\\s+(\s表示空格，+表示至少一个)分隔每一行得到单词列表，接着统计了字符数，词语种数、总词频。字符数除以总词频得到平均词语长度，测试集中的OOV频次除以测试集总词频得到OOV比例。</p><p>分析两个表格中的数据，可得到以下结论：</p><ol><li>语料库的规模</li><li>语料库的难度</li><li>汉语的平均词语长度约为1.7</li><li>汉语的常用词汇量约在10万这个量级</li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>3.1语言模型</title>
      <link href="/2024/04/22/3.1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
      <url>/2024/04/22/3.1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h3 id="3-1语言模型"><a href="#3-1语言模型" class="headerlink" title="3.1语言模型"></a>3.1语言模型</h3><h4 id="3-1-1什么是语言模型"><a href="#3-1-1什么是语言模型" class="headerlink" title="3.1.1什么是语言模型"></a>3.1.1什么是语言模型</h4><p><strong>模型</strong>指的是对事物的数学抽象，那么<strong>语言模型</strong>就是对语言现象的数学抽象。举个例子就是，给定一个句子$w$,计算$w$在一篇文档中出现的次数$p(w)$的模型。我们只能在一个较小的样本空间采样，称为语料库。这个概率一般来自人工标注而成的语料库。</p><p>语料库的建设并非高不可攀的过程，一个微型预料库如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">商品 和 服务</span><br><span class="line">商品 和服 物美价廉</span><br><span class="line">服务 和 货币</span><br></pre></td></tr></table></figure><p>这就是一个语料库，就这个语料库我们可以计算出$p(商品 和 服务)=\frac{1}{3}$,$p(商品 和服 物美价廉)=\frac{1}{3}$，$p(服务 和 货币)=\frac{1}{3}$，其他句子概率都为0。因为样本空间大小为3，所以平均分给了这三个句子，其他句子的概率为0。如果希望更准确地估计句子分布，则需要增大样本数量。估算句子分布是NLP任务中最基本的问题。如果模型能分辨出$p(商品\,和\,服务)&gt;p(商品\,和服\,务)$，分词器就能据此选择正确结果，机器翻译系统就能翻译出更流畅的译文。</p><p>然而$p(w)$的计算非常困难，即使是大型语料库也只有几百万句子，每个句子几乎不会重复，这导致每个句子的$p(w)=1$，而在语料库之外的句子$p(w)=0$，日常遇到的句子几乎都在语料库之外，所以它们的概率几乎都为0，这样的现象被称为<strong>数据稀疏</strong>。</p><p>更合理的方法是考虑单词。考虑到句子有单词构成，句子无限，但构成句子的单词有限。我们从单词构成句子的角度出发去建模句子，把句子表示为单词列表$w=w1w2w~k~…$，每个$wt，t\in[1,k]$都是一个单词，定义语言模型：</p><script type="math/tex; mode=display">p(w)=p(w_1w_2...w_k)</script><script type="math/tex; mode=display">=p(w_1|w_0)\times p(w_2|w_0w_1)\times ....\times p(w_k|w_0w_1w_2...w_{k-1})=\prod_{t=1}^{k+1}p(w_t|w_0w_1...w_{t-1})</script><p>其中，$w0=BOS$，定义为一个句子的开头，$wk=EOS$定义为一个句子的结尾，这在NLP领域的论文和代码中经常出现。</p><p>语言模型模仿人的说话顺序，给定已经说出口的词语序列，预测下一个词语的后验概率，然后一个单词一个单词地乘上后验概率，就能估计出任意一句话的概率。比如“商品”和“服务”的概率估计如下：</p><script type="math/tex; mode=display">p(商品\,和\,服务)=p(商品|BOS)p(和|BOS商品)p(服务|BOS商品\,和)p(EOS|BOS\,商品\,和\,服务)</script><p>用上面的语料库来计算$p(商品 和 服务)$的概率，用极大似然估计来计算每个后验概率，也即：</p><script type="math/tex; mode=display">p(w_t|w_0...w_{t-1})=P_{ML}(w_t|w_0...w_t-_1)=\frac{c(w_0...w_t)}{c(w_0...w_{t-1})}</script><p>比如商品$p(商品|BOS)$的估计只需计算“商品”作为第一个单词出现的次数2，和所有单词作为第一个单词出现的次数3，然后相除得$p(商品|BOS)=\frac{2}{3}$，$p(和|BOS\,商品)$则需要统计”BOS 商品 和”的出现次数1，除以“BOS 商品”的频次2即可，即$p(和|BOS\,商品)=\frac{1}{2}$，同理$p(服务|BOS\,商品\,和)=\frac{1}{1}$,$p(EOS|BOS\,商品\,和\,服务)=\frac{1}{1}$。整个句子的概率是四者的乘积。</p><script type="math/tex; mode=display">p(商品\,和\,服务)=\frac{2}{3}\times\frac{1}{2}\times\frac{1}{1}\times\frac{1}{1}</script><p>与枚举的概率一致，但随着句子长度的增大，枚举会遇到以下两个问题：</p><ol><li><strong>数据稀疏</strong>指的是长度越大的句子越难出现，语料库中极有可能统计不到长句子的频次，导致$p(w_k|w_0w_1…w_{k-1})=0$。比如$p(商品\,和\,服务)=0$。同枚举一样，数据稀疏的问题依然存在。</li><li><strong>计算代价大</strong>，k越大,$p(w_k|w_0w_1…w_{k-1})$存储的数据就越多，即使是用字典树索引，依然需要很大代价。</li></ol><h4 id="3-1-2马尔可夫链与二元语法"><a href="#3-1-2马尔可夫链与二元语法" class="headerlink" title="3.1.2马尔可夫链与二元语法"></a>3.1.2马尔可夫链与二元语法</h4><p>为了解决这个问题，NLP领域的前辈们使用<strong>马尔可夫链</strong>来解决这个问题，<strong>马尔可夫链</strong>是因果链，表示每一件事的发生概率取决于前一件事。在语言模型中，第k个事件表示$w_k$作为第k个单词出现，也就是说，<strong>马尔科夫链</strong>假设每个单词出现的概率取决于前面一个单词：</p><script type="math/tex; mode=display">p(w_k|w_0w_1...w_{k-1})=p(w_k|w_{k-1})</script><p>基于此假设，式子一下子短了不少，由于每次计算只涉及到两个单词的二元接续，所以此时的语言模型又被称为<strong>二元语法</strong>模型：</p><script type="math/tex; mode=display">p(w)=p(w_0w_1...w_k)=p(w_1|w_0)\times p(w_2|w_1)\times...\times p(w_{k+1}|w_k)=\prod_{t=1}^{k+1}{p(w_{t+1}|w_{t})}</script><p>由于语料库中二元接续的重复程度要高于整个句子的重复程度，所以<strong>数据稀疏</strong>的问题得到了解决，同时，二元接续的数量远小于句子的数量，长度也更短，所以存储和查询的问题也得到了解决。</p><p>利用<strong>二元语法</strong>模型来计算$p(商品\,和\,服务)$：</p><script type="math/tex; mode=display">p(商品\,和\,服务)=p(商品|BOS)p(和|商品)p(服务|和)p(EOS|服务)=\frac{2}{3}\times\frac{1}{2}\times\frac{1}{2}\times\frac{1}{2}</script><p>如何计算的：以$p(和|商品)$为例：（商品 和）的频次除以（商品）的频次得$\frac{1}{2}$。</p><p>这次的概率比上次的少了点，剩下的概率到哪去了？计算一下语料库之外的句子$p(商品\,和\,货币)$。</p><p>得$\frac{1}{6}$。原来概率分给了语料库之外的句子，这就缓解了<strong>数据稀疏</strong>的问题。</p><h4 id="3-1-3-n-元语法"><a href="#3-1-3-n-元语法" class="headerlink" title="3.1.3 $n$元语法"></a>3.1.3 $n$元语法</h4><p>利用类似的思路，也可以得到<strong>n元语法</strong>的定义：每个单词的概率仅取决于该单词之前的$n-1$个单词。也即：</p><script type="math/tex; mode=display">p(w)==\prod_{k=1}^{k+n-1}{p(w_{k}|w_{k-n+1}...w_{k-1})}</script><p>当$n=1$时，<strong>n元语法</strong>即一元语法，n=3时，<strong>n元语法</strong>即三元语法；$n\geq4$时，数据稀疏和计算代价有明显起来，实际工程中几乎用不到。另外，深度学习带来了一种递归神经网络语言模型(<strong>RNN Language Model</strong>)，理论上可以记忆无数个单词，可以看做无穷语法。</p><h4 id="3-1-4数据稀疏与平滑策略"><a href="#3-1-4数据稀疏与平滑策略" class="headerlink" title="3.1.4数据稀疏与平滑策略"></a>3.1.4数据稀疏与平滑策略</h4><p>对于$n$元语法模型，$n$越大，数据稀疏问题越严峻。即便是二元语法，也存在语料库中没有二元接续的情况。比如上述语料库中“商品 货币”的频次就为零。我们可以采用低元n语法平滑至高元n语法的策略。所谓平滑就是指将n元语法频次的折线平滑成曲线。我们不希望二元语法“商品 货币”的频次突然降为0，所以用一元语法“商品”和“货币”的频次去平滑它。</p><p>平滑策略是语言模型的研究课题之一，最简单的一种是线性插值法，它定义新的二元语法概率为：</p><script type="math/tex; mode=display">p(w_k|w_{k-1})=\gamma p_{ML}(w_k|w_{k-1})p(w_k)+(1-\gamma)p(w_k)</script><p>其中，$\gamma\in(0,1)$为常数平滑因子，通俗理解，这是一种劫富济贫的制度。其中的$\gamma$就是个人所得税的税率，$p_{w_k|w_{k-1}}$就是税前所得，$p(w_t)$就是社会福利。通过缴税，实现高概率的二元语法的概率被移动到社会福利中，这保证了在语料库中没有的二元语法能够有一点概率，不至于为0。低保金的额度与二元语法的挣钱潜力成正比：二元语法中的第二个词词频越高，它未来被统计到的概率也应该越高，因此它多拿一点。</p><p>类似地，一元语法也可以用过线性插值来平滑：</p><script type="math/tex; mode=display">p(w_k)=\gamma p_{ML}(w_k)+(1-\gamma)(\frac{1}{N})</script><p>其中,$N$是语料库总词频。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>2.10字典树的其他应用</title>
      <link href="/2024/04/22/2.10%E5%AD%97%E5%85%B8%E6%A0%91%E7%9A%84%E5%85%B6%E4%BB%96%E5%BA%94%E7%94%A8/"/>
      <url>/2024/04/22/2.10%E5%AD%97%E5%85%B8%E6%A0%91%E7%9A%84%E5%85%B6%E4%BB%96%E5%BA%94%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="2-10字典树的其他应用"><a href="#2-10字典树的其他应用" class="headerlink" title="2.10字典树的其他应用"></a>2.10字典树的其他应用</h3><p>字典树除了应用于中文分词之外，还可应用于任何词典与最长匹配的任务。在HanLP中，字典树被广泛应用于停用词过滤、简繁转换、拼音转换等任务。</p><h4 id="2-10-1-停用词过滤"><a href="#2-10-1-停用词过滤" class="headerlink" title="2.10.1 停用词过滤"></a>2.10.1 停用词过滤</h4><p>汉语中有一类称为停用词的词，它们没有什么特殊意义，如助词“的”，连词“以及”，副词“甚至”、语气词“吧”，称为<strong>停用词</strong>。一个句子去掉了停用词并不影响理解。停用词视具体任务不同而不同，在网站系统中，一些非法的敏感词也被视为停用词。因此，停用词的过滤是一个常见的预处理过程。</p><p>HanLP也提供了停用词词典，可以用BinTrie、DAT、ADAT中的任意一个存储词典。考虑到词典中含有单子词语，用双数组字典树更划算。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_from_file</span>(<span class="params">path</span>):</span><br><span class="line">    <span class="built_in">map</span>=JCalss(<span class="string">&#x27;java.util.TreeMap&#x27;</span>)()</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path) <span class="keyword">as</span> src:</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> src:</span><br><span class="line">            word=word.strip()<span class="comment">#去掉python读入的\n</span></span><br><span class="line">            <span class="built_in">map</span>[word]=word</span><br><span class="line">    <span class="keyword">return</span> JClass(<span class="string">&#x27;&#x27;</span>)(<span class="built_in">map</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_from_words</span>(<span class="params">*words</span>):</span><br><span class="line">    <span class="built_in">map</span> =JClass(<span class="string">&#x27;java.util.TreeMap&#x27;</span>)()<span class="comment">#创建TreeMap实例</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        <span class="built_in">map</span>[word]=word</span><br><span class="line">    <span class="keyword">return</span> JClass(<span class="string">&#x27;&#x27;</span>)(<span class="built_in">map</span>)</span><br><span class="line">            </span><br></pre></td></tr></table></figure><p>加载停用词后，会得到一棵双数组字典树。现在针对分词结果，遍历每个词语，若它在字典树中，则删除。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">remove_stopwords_termlist</span>(<span class="params">termlist,trie</span>):</span><br><span class="line">    <span class="keyword">return</span> [term.word <span class="keyword">for</span> term <span class="keyword">in</span> termlist <span class="keyword">if</span> <span class="keyword">not</span> trie.containsKey(term.word)]</span><br></pre></td></tr></table></figure><p>在敏感词过滤的场景下，通常需要将敏感词替换为特定字符串，一般是**。此时可以先分词再替换，也可以不分词，利用接口直接替换。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>野有蔓草</title>
      <link href="/2024/04/21/%E9%87%8E%E6%9C%89%E8%94%93%E8%8D%89/"/>
      <url>/2024/04/21/%E9%87%8E%E6%9C%89%E8%94%93%E8%8D%89/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class='poem'><div class='poem-title'>野有蔓草 诗经 国风 郑风</div><div class='poem-author'></div><p>野有蔓草，零露漙兮。有美一人，清扬婉兮。邂逅相遇，适我愿兮。</p><p>野有蔓草，零露瀼瀼。有美一人，婉如清扬。邂逅相遇，与子偕臧。</p></div>]]></content>
      
      
      <categories>
          
          <category> 诗情画意 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>2.9准确率评测</title>
      <link href="/2024/04/21/2.9%E5%87%86%E7%A1%AE%E7%8E%87%E8%AF%84%E6%B5%8B/"/>
      <url>/2024/04/21/2.9%E5%87%86%E7%A1%AE%E7%8E%87%E8%AF%84%E6%B5%8B/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="2-9准确率评测"><a href="#2-9准确率评测" class="headerlink" title="2.9准确率评测"></a>2.9准确率评测</h2><p>通过数据结构上的优化，将分词速度推向了几千万字每秒的量级。是时候关注一下分词器的精准程度了。</p><h3 id="2-9-1准确率"><a href="#2-9-1准确率" class="headerlink" title="2.9.1准确率"></a>2.9.1准确率</h3><p>准确率是用来衡量一个系统的准确程度的值，可以理解为一系列评测指标。在不同的NLP任务中，采用了不同的评价指标。但都习惯笼统地说准确率这个词。</p><p>严格地讲，当预测与答案的数量相等时，==准确率指的是系统做出正确判断的次数除以总的测试次数==。比如测试病毒携带者100人，系统测试只有一人为阳性，剩余99人均为阴性，而事实确实如此，这个系统的准确率就占到100%。但考虑到健康人占大多数，如果不良商家将所有人都测为阴性，准确率为$\frac{99}{100}$=99%。这种准确率显然是没有说服力的。</p><p>在中文分词任务中，一般使用在标准数据集上词语级别的精确率、召回率与$F1$值来衡量分词器的准确程度。这三个术语借用自信息检索与分类问题，常用来衡量搜索引擎和分类器的准确程度。但首先要学习混淆矩阵的概念。</p><h3 id="2-9-2混淆矩阵与-TP-FN-FP-TN"><a href="#2-9-2混淆矩阵与-TP-FN-FP-TN" class="headerlink" title="2.9.2混淆矩阵与$TP/FN/FP/TN$"></a>2.9.2混淆矩阵与$TP/FN/FP/TN$</h3><p>实际上，搜索引擎、分类器、中文分词场景下的准确率本质上都是4个集合的并集运算。以第一章中的姓名性别预测问题为例，根据标准答案是男是女，以及预测结果是男是女，一共有$2\times2=4$种组合。记男性为正类，女性为负类。为每种组合取个名字。</p><p>两者的四种组合解释如下：</p><ul><li>TP:真阳，预测为p，答案为p</li><li>FP:假阳，预测为p，答案为n</li><li>TN:真阴，预测为n，答案为n</li><li>FN:假阴，预测为n，答案为p</li></ul><p>这种图表在机器学习中也称为==混淆矩阵==(confusion matrix)，用来衡量分类结果的混淆程度。从集合的角度来看，定义$U$为并集运算符，则混淆矩阵有如下性质：</p><ul><li>样本全集 = $TP$U$FP$U$TN$U$FN$：</li><li>任何一个样本属于且只属于4个集合中的一个，也就是说它们没有交集</li></ul><p>只要混淆矩阵确定了，3个准确指标就都确定了。</p><h3 id="2-9-3精确率"><a href="#2-9-3精确率" class="headerlink" title="2.9.3精确率"></a>2.9.3精确率</h3><p>精确率（precision，简称P值）指的是预测结果中正类数量占全部结果的比率，即$p=\frac{TP}{TP+FP}$。在姓名个呢类问题中,$p=\frac{1}{2}$；在病毒检测案例中，真阳病人没检测出来，因此$p=0$，让伪劣产品现了形。</p><p>但如果将阴性与阳性对调，导致的结果就是$p=99$%，但我们应该优先将关注度高的指标作为正类，如果是阴性做正类，即使出错是将阴性误诊成阳性，复查一下就好了。但阳性误诊成阴性会耽误治疗，危害要大得多。所以应将病人（阳性）作为正类。</p><h3 id="2-9-4召回率"><a href="#2-9-4召回率" class="headerlink" title="2.9.4召回率"></a>2.9.4召回率</h3><p>召回率指的是预测为正类（且正确）的样本占总体为正类的比率，也就是正类样本被找出来的比率。<strong>召回率</strong>顾名思义就是能回想起的比例。$R=\frac{TP}{TP+FN}$。在上述姓名分类问题中，$R=\frac{1}{3}$；在病毒检测样例中$R=0$。在搜索引擎中，召回率指的是相关网页被搜到的比例。在区分$P,R$值时，首先明确分子均是$TP$（真阳样本数），$P$值的分母是预测阳性的数量，$R$值的分母是答案阳性的数量。</p><h3 id="2-9-5-F-1-的值"><a href="#2-9-5-F-1-的值" class="headerlink" title="2.9.5$F_1$的值"></a>2.9.5$F_1$的值</h3><p>假设还是病毒检测问题，不良商家开发新试剂这次所有测试均会显示阳性，则$P=\frac{TP}{TP+FP}=1$%，$R=\frac{TP}{TP+FN}=1$。两个指标差距极大，一般而言，精确率和召回率比较难平衡，召回率高的系统往往精确率低，反之亦然。</p><p>为了防止商家只拿$100$%召回率做宣传，忽略$1%$%的精确率，我们需要一个综合性的指标调和平均<strong>$F1$值</strong>：$F1=\frac{2<em>P</em>R}{P+R}$。这样P,R值必须同时较高，才能得到较高的$F1值$。在上述病毒检测样例中，商家试剂的$F1$值为$F1\approx2$%。</p><h3 id="2-9-6中文分词中的-P、R、F-1-计算"><a href="#2-9-6中文分词中的-P、R、F-1-计算" class="headerlink" title="2.9.6中文分词中的$P、R、F_1$计算"></a>2.9.6中文分词中的$P、R、F_1$计算</h3><p>混淆矩阵针对的是答案与预测数量相等的情况，比如5个姓名，100个受试者一一对应的答案与预测。在中文分词中，标准答案与分词结果的单词数不一定相等。而且混淆矩阵针对的是分类问题，而中文分词却是一个分块(chunking)问题。</p><p>我们需要将分块问题转换为分类问题。对于长为$n$的字符串，分词结果是一系列的单词。这些单词是在文本中的起止位置是$[i,j]$，其中$1\leq{i}\leq{j}\leq{n}$。那么标准答案的所有区间作为一个集合$A$构成一个正类。此集合之外的所有区间作为另一个集合（A的补集），构成一个负类。</p><script type="math/tex; mode=display">TP\cup{FN}=A</script><script type="math/tex; mode=display">TP\cup{FP}=B</script><script type="math/tex; mode=display">TP=A\cup{B}</script><p>相应，P、R的计算公式如下：</p><script type="math/tex; mode=display">P=\mid\frac{A\cap{B}}{B}\mid</script><script type="math/tex; mode=display">R=\mid\frac{A\cap{B}}{A}\mid</script><p>编写函数计算$P,R,F_1$值</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">to_region</span>(<span class="params">segmentation: <span class="built_in">str</span></span>) -&gt; <span class="built_in">list</span>:</span><br><span class="line">    region=[]</span><br><span class="line">    start=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> region.<span class="built_in">compile</span>(<span class="string">&quot;\\s+&quot;</span>).split(segmentation.strip()):</span><br><span class="line">        end=start+<span class="built_in">len</span>(word)</span><br><span class="line">        region.append((start,end))</span><br><span class="line">        start=end</span><br><span class="line">    <span class="keyword">return</span> region</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prf</span>(<span class="params">gold: <span class="built_in">str</span>,pred: <span class="built_in">str</span></span>) -&gt; <span class="built_in">tuple</span>:</span><br><span class="line">    A_size,B_size,A_cap_B_size=<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(gold) <span class="keyword">as</span> gd,<span class="built_in">open</span>(pred) <span class="keyword">as</span> pd:</span><br><span class="line">        <span class="keyword">for</span> g,p <span class="keyword">in</span> <span class="built_in">zip</span>(gd,pd):</span><br><span class="line">            A,B=<span class="built_in">set</span>(to_region(g)),<span class="built_in">set</span>(to_region(p))</span><br><span class="line">            A_size+=<span class="built_in">len</span>(A)</span><br><span class="line">            B_size+=<span class="built_in">len</span>(B)</span><br><span class="line">            A_cap_B_size+=<span class="built_in">len</span>(A&amp;B)</span><br><span class="line">    p,r=A_cap_B_size/B_size,A_cap_B_size/A_size</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(p)</span><br><span class="line">    <span class="keyword">return</span> p,r,<span class="number">2</span>*p*r/(p+r)</span><br></pre></td></tr></table></figure><h3 id="2-9-8第二届国际中文分词评测"><a href="#2-9-8第二届国际中文分词评测" class="headerlink" title="2.9.8第二届国际中文分词评测"></a>2.9.8第二届国际中文分词评测</h3><p>本节来评估一下词典分词的准确率，首先要明确词典，HanLP中的CoreNatureDictionary.txt颗粒度较大，mini词典主要收录了人民日报上的语料库，颗粒度小。使用颗粒度大的词典在颗粒度小的语料上做评测，会导致分值低，无法反映算法的是准确率。</p><p>本书将用第二届国际中文分词评测。评测流程是先加载词典构造分词器，然后对测试语料进行分词，最后将结果与标准语料作比较。</p><p>用$python$实现:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sighan05 = ensure_data(<span class="string">&#x27;icwb2-data&#x27;</span>, <span class="string">&#x27;http://sighan.cs.uchicago.edu/bakeoff2005/data/icwb2-data.zip&#x27;</span>)<span class="comment">#1</span></span><br><span class="line">   msr_dict = os.path.join(sighan05, <span class="string">&#x27;gold&#x27;</span>, <span class="string">&#x27;msr_training_words.utf8&#x27;</span>)</span><br><span class="line">   msr_test = os.path.join(sighan05, <span class="string">&#x27;testing&#x27;</span>, <span class="string">&#x27;msr_test.utf8&#x27;</span>)</span><br><span class="line">   msr_output = os.path.join(sighan05, <span class="string">&#x27;testing&#x27;</span>, <span class="string">&#x27;msr_output.txt&#x27;</span>)</span><br><span class="line">   msr_gold = os.path.join(sighan05, <span class="string">&#x27;gold&#x27;</span>, <span class="string">&#x27;msr_test_gold.utf8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">   DoubleArrayTrieSegment = JClass(<span class="string">&#x27;com.hankcs.hanlp.seg.Other.DoubleArrayTrieSegment&#x27;</span>)<span class="comment">#2</span></span><br><span class="line">   segment = DoubleArrayTrieSegment([msr_dict]).enablePartOfSpeechTagging(<span class="literal">True</span>)</span><br><span class="line">   <span class="keyword">with</span> <span class="built_in">open</span>(msr_gold, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> test, <span class="built_in">open</span>(msr_output, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> output:</span><br><span class="line">       <span class="keyword">for</span> line <span class="keyword">in</span> test:</span><br><span class="line">           output.write(<span class="string">&quot;  &quot;</span>.join(term.word <span class="keyword">for</span> term <span class="keyword">in</span> segment.seg(re.sub(<span class="string">&quot;\\s+&quot;</span>, <span class="string">&quot;&quot;</span>, line))))</span><br><span class="line">           output.write(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">   <span class="built_in">print</span>(<span class="string">&quot;P:%.2f R:%.2f F1:%.2f OOV-R:%.2f IV-R:%.2f&quot;</span> % prf(msr_gold, msr_output, segment.trie))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>1处保证语料库目录存在，之后获取了<strong>MSR词典、测试文本、分词结果与标准答案的路径</strong>。接着在2处创建DoubleArrayTrieSegment分词器，打开了“词性标注”开关，这样就可以识别数字和英文。然后按行切分测试文本输出到分词结果中。最后调用prf函数计算$P、R、F$~1~并保留两位小数输出。</p><h3 id="2-9-9OOV-Recall-Rate-与-IV-Recall-Rate"><a href="#2-9-9OOV-Recall-Rate-与-IV-Recall-Rate" class="headerlink" title="2.9.9OOV Recall Rate 与 IV Recall Rate"></a>2.9.9OOV Recall Rate 与 IV Recall Rate</h3><p>$OOV$指的是“<strong>未登录词</strong>”(Out Of Vocabulary)，或者说是词典未收录的新词。如何准确切分$OOV$，乃至识别其语义，是NLP的核心难题之一。词典分词对新词的召回能力几乎为0，主要还是靠单字成词和英文数字的合并规则。</p><p>$IV$指的是<strong>登录词</strong>(In Vocabulary)，词典对已存在的词汇的正确召回的概率。如果连在词典中的词都无法百分之百召回，说明词典分词的消歧能力不好。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>2.8HanLP的词典分词实现</title>
      <link href="/2024/04/20/2.8HanLP%E7%9A%84%E8%AF%8D%E5%85%B8%E5%88%86%E8%AF%8D%E5%AE%9E%E7%8E%B0/"/>
      <url>/2024/04/20/2.8HanLP%E7%9A%84%E8%AF%8D%E5%85%B8%E5%88%86%E8%AF%8D%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="2-8HanLP的词典分词实现"><a href="#2-8HanLP的词典分词实现" class="headerlink" title="2.8HanLP的词典分词实现"></a>2.8HanLP的词典分词实现</h2><p>随着本书的深入，我们会接触到许多算法。为了提供统一的接口，HanLP中所有的分词器都继承自Segment这个基类。词典分词家族是其中一个分支。</p><h3 id="2-8-1DoubleArrayTrieSegment"><a href="#2-8-1DoubleArrayTrieSegment" class="headerlink" title="2.8.1DoubleArrayTrieSegment"></a>2.8.1DoubleArrayTrieSegment</h3><p>DoubleArrayTrieSegment分词器是对DAT最长匹配的封装，默认加载hanlp，properties中CoreDictionaryPath指定的词典，对应的python代码：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyhanlp <span class="keyword">import</span>*</span><br><span class="line"></span><br><span class="line">HanLP.Config.ShowTermNature = <span class="literal">False</span></span><br><span class="line">segment=DoubleArrayTrieSegment()</span><br><span class="line"><span class="built_in">print</span>(segment.seg(<span class="string">&#x27;江西鄱阳湖干枯，中国最大淡水湖变成大草原&#x27;</span>))</span><br></pre></td></tr></table></figure><p><img src="https://12345picture.oss-cn-hangzhou.aliyuncs.com/202404262145697.png" alt="image-20240420205937835"></p><p>我们也可以上传自己的词典路径。</p><p>一个实用的分词器还得考虑这些算法之外的脏活（英文与数字不是拆开呈现）。在DictionaryBasedSegment家族中，数字英文的合并与词性标注作为整个功能，可以通过enablePartOfSpeechTagging开启。python版如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyhanlp <span class="keyword">import</span>*</span><br><span class="line">segment=DoubleArrayTrieSegment()</span><br><span class="line"></span><br><span class="line">segment.enablePartOfSpeechTagging(<span class="literal">True</span>)</span><br><span class="line">HanLP.Config.ShowTermNature = <span class="literal">True</span></span><br><span class="line"><span class="built_in">print</span>(segment.seg(<span class="string">&#x27;上海市虹口区大连西路550号SISU&#x27;</span>))</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><p><img src="https://12345picture.oss-cn-hangzhou.aliyuncs.com/202404262145817.png" alt=""></p><p>这里的词性标注依然是基于词典的，永远返回词典指定的第一个词性。词典中的词语成千上万，有了词典级的默认词性，用户就不必逐条写词性了。词条所指定的词性优先级更高。</p><p>如果需要分别获取分词结果中的<wavy>词语与词性</wavy>，Python：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> term <span class="keyword">in</span> segment.seg(<span class="string">&quot;上海市虹口区大连西路550号SISU&quot;</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;单词：%s 词性：%s&quot;</span> % (term.word,term.nature))</span><br></pre></td></tr></table></figure><h3 id="2-8-2AhoCorasickDoubleArrayTrieSegment"><a href="#2-8-2AhoCorasickDoubleArrayTrieSegment" class="headerlink" title="2.8.2AhoCorasickDoubleArrayTrieSegment"></a>2.8.2AhoCorasickDoubleArrayTrieSegment</h3><p>如果用户的词语都很长，那么ACDAT的速度会更快。HanLP封装了基于ACDAT的实现AhoCorasickDoubleArrayTrieSegment，接口与DoubleAaaryTrieSegment类似。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">segment=JClass(<span class="string">&#x27;&#x27;</span>)()</span><br><span class="line"><span class="built_in">print</span>(segment.seg(<span class="string">&#x27;&#x27;</span>))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>好事近</title>
      <link href="/2024/04/20/%E5%A5%BD%E4%BA%8B%E8%BF%91%E2%8B%85cdot%E6%A2%A6%E4%B8%AD%E4%BD%9C/"/>
      <url>/2024/04/20/%E5%A5%BD%E4%BA%8B%E8%BF%91%E2%8B%85cdot%E6%A2%A6%E4%B8%AD%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class='poem'><div class='poem-title'>好事近$\cdot$梦中作，秦观</div><div class='poem-author'></div><p>春路雨添花，花动一山春色。行到小溪深处，有黄鹂千百。</p><p>飞云当面化龙蛇，夭矫转空碧。醉卧古藤阴下，了不知南北。</p></div>]]></content>
      
      
      <categories>
          
          <category> 诗情画意 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>双数组字典树</title>
      <link href="/2024/04/17/%E5%8F%8C%E6%95%B0%E7%BB%84%E5%AD%97%E5%85%B8%E6%A0%91/"/>
      <url>/2024/04/17/%E5%8F%8C%E6%95%B0%E7%BB%84%E5%AD%97%E5%85%B8%E6%A0%91/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="双数组字典树"><a href="#双数组字典树" class="headerlink" title="双数组字典树"></a>双数组字典树</h3><p>BinTrie的弱点：除了根节点的完美散列外，其余节点都在用二分查找。当存在c个子节点时，每次状态转移的复杂度为$O(logc)$。当c很大时，依然很慢。能否提高单次状态转移的速度呢？</p><p><strong>双数组字典树</strong>$(double Array Trie,DAT)$就是一种状态转移复杂度为常数的数据结构，它由base和check两个数组构成，又简称双数组。</p><h4 id="1-双数组定义"><a href="#1-双数组定义" class="headerlink" title="1.双数组定义"></a>1.双数组定义</h4><p>字典树与双数组字典树均是DFA，其状态由$base$与$check$中的元素和下标表示。具体来说，当状态b接受字符c转移到状态p时，双数组满足：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">p=base[b]+c</span><br><span class="line">check[p]=base[b]</span><br></pre></td></tr></table></figure><p>若不满足此条件，则状态转移失败。若当前状态为自然（状态由一个整数下标表示），如果想知道是否可以转移到自然人，首先先定义$自然人=base[自然]+人$，然后检查$check[自然人]==base[自然]$是否成立，据此判断是否转移成功。</p><p>定义一个双数组：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DoubleArrayTrie</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_</span>(<span class="params">self,dic:<span class="built_in">dict</span></span>)-&gt;<span class="literal">None</span>:</span><br><span class="line">        m=JClass(<span class="string">&#x27;java.util.TreeMap&#x27;</span>)()</span><br><span class="line">        <span class="keyword">for</span> k,v <span class="keyword">in</span> dic.items():</span><br><span class="line">            m[k]=v</span><br><span class="line">        dat=JClass(<span class="string">&#x27;com.hankcs.hanlp.collection.trie.DoubleArrayTrie&#x27;</span>)(m)</span><br><span class="line">        self.base=dat.base</span><br><span class="line">        self.check=dat.check</span><br><span class="line">        self.value=dat.v</span><br></pre></td></tr></table></figure><p>将双数组的构造委托给HanLP，直接拿到构造完毕的base、check以及用户赋予每个键的值value数组。</p><h3 id="2-状态转移"><a href="#2-状态转移" class="headerlink" title="2.状态转移"></a>2.状态转移</h3><p>并非所有节点都对应词语终点，只有字典树中的终点节点(蓝色节点)才对应一个词语，为了区分，实现上可以借鉴C语言中的设计，在每个字符串末尾添加一个散列值等于0的\0.也就是说，\0充当了蓝色节点的角色，这样普通节点就不需要再分配内存标记自己的颜色了。为了避免与用户输入的文本混淆，将文本字符的<strong>hashcode</strong>加1就行了。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">transition</span>(<span class="params">self,c,b</span>)-&gt;<span class="built_in">int</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    状态转移</span></span><br><span class="line"><span class="string">    :param c:字符</span></span><br><span class="line"><span class="string">    :param b:初始状态</span></span><br><span class="line"><span class="string">    :return: 转移后的状态，-1表示失败</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    p=self.base[b]+self.char_hash(c)+<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> self.base[b]==self.check(p):</span><br><span class="line">        <span class="keyword">return</span> p</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure><p>除了额外加1的技巧外，这段代码与双数组字典树的定义是吻合的。</p><h3 id="3-查询"><a href="#3-查询" class="headerlink" title="3.查询"></a>3.查询</h3><p>在实现上，value数组可以存储任何类型的对象，双数组只维护键的字典序。需要值的时候，先查询到键的字典序，然后将字典序做下标去value数组中取值。具体做法是将字典序作为自然数赋予作为单词结尾的那些节点。</p><p>为了节省内存，不必要另开一个数组，只需约定当状态p满足<strong>base[p]&lt;0</strong>时，该状态对应单词结尾，且单词的字典序为<strong>-base[p] - 1</strong>。之所以减一是因为负数从-1开始，取相反数减一后为0，是第一个自然数。</p><p>伪代码：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">def_getitem_(self,key:<span class="built_in">str</span>):</span><br><span class="line">    b=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="built_in">len</span>(key)):<span class="comment">#len(key)次状态转移</span></span><br><span class="line">        p=self.transition(key[i],b)</span><br><span class="line">        <span class="keyword">if</span> p <span class="keyword">is</span> <span class="keyword">not</span> -<span class="number">1</span>:</span><br><span class="line">            b=p</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    p=self.base[b]<span class="comment">#按字符&#x27;\0&#x27;进行状态转移</span></span><br><span class="line">    n=self.base[p]<span class="comment">#查询base</span></span><br><span class="line">    <span class="keyword">if</span> p==self.check[p]:</span><br><span class="line">        index=-n-<span class="number">1</span><span class="comment">#取得字典序</span></span><br><span class="line">        <span class="keyword">return</span> self.value[index]</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习day1</title>
      <link href="/2024/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0day1/"/>
      <url>/2024/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0day1/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><ol><li><p>深度学习</p><ul><li>机器学习的分支，人工神经网络为基础，对数据的特征进行学习的方法</li></ul></li><li><p>机器学习和深度学习的区别</p><ul><li>特征抽取<ul><li>人工的特征抽取的过程</li><li>深度学习：自动的进行特征抽取</li></ul></li></ul><ul><li>数据量<ul><li>机器学习：数据少，效果不是特别好</li><li>深度学习：数据多，效果更好</li></ul></li></ul></li><li><p>场景</p><ul><li>文本识别</li><li>机器翻译</li><li>聊天对话</li></ul></li><li><p>框架</p><ul><li>pytorch</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>字典树</title>
      <link href="/2024/04/14/%E5%AD%97%E5%85%B8%E6%A0%91%E7%9A%84%E8%8A%82%E7%82%B9%E5%AE%9E%E7%8E%B0/"/>
      <url>/2024/04/14/%E5%AD%97%E5%85%B8%E6%A0%91%E7%9A%84%E8%8A%82%E7%82%B9%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="1-字典树的节点实现"><a href="#1-字典树的节点实现" class="headerlink" title="1.字典树的节点实现"></a>1.字典树的节点实现</h3><p>由上篇文章可知，每个节点都应该知道自己的子节点与对应的边，以及自己是否对应一个词。如果要实现映射而不是集合的话，还需要知道自己对应的值。我们约定用值为None表示节点不对应词语，虽然这样就不能插入值为None的键了，但是实现起来更简洁。那么节点的实现用Python描述如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Node</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,value</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="comment">#初始化节点，创建一个空的子节点字典和一个值属性</span></span><br><span class="line">        self._children=&#123;&#125;</span><br><span class="line">        self._value=value</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_add_child</span>(<span class="params">self,char,value,overwrite=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="comment">#添加一个子节点，如果子节点已存在且overwrite为True,则更新子节点的值</span></span><br><span class="line">        child=self._children.get(char)</span><br><span class="line">        <span class="keyword">if</span> child <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment">#如果子节点不存在，则创建一个新的子节点并添加到子节点字典中</span></span><br><span class="line">            child=Node(value)</span><br><span class="line">            self._children[char]=child</span><br><span class="line">        <span class="keyword">elif</span> overwrite:</span><br><span class="line">            <span class="comment">#如果子节点存在且overwrite为True，更新子节点的值</span></span><br><span class="line">            child._value=value</span><br><span class="line">        <span class="keyword">return</span> child    </span><br></pre></td></tr></table></figure><p>这段代码定义了一个Node类，表示树结构中的一个节点。每个节点都有一个_children字典来存储子节点，以及一个_value属性来存储节点的值。</p><p>_init__方法用于初始化节点对象，接受一个value参数作为节点的值，并创建一个空的子节点字典。</p><p>_add_child方法用于向节点添加子节点。它接受三个参数：char表示子节点的键，value表示子节点的值，overwrite表示是否覆盖已存在的子节点的值。如果子节点不存在，则创建一个新的字节点并将其添加到子节点字典中；如果子节点存在且overwrite为True，则更新子节点的值。最后返回子节点对象。</p><h3 id="2-字典树的增删改查实现"><a href="#2-字典树的增删改查实现" class="headerlink" title="2.字典树的增删改查实现"></a>2.字典树的增删改查实现</h3><p>上边实现了节点以及连接方法，只要把它们连接到根节点上去，我们就得到了一颗字典树。根节点实际上也是一个普通节点，只不过多了一些面向用户的公开方法。在设计模式上，根节点应该继承普通节点。抓住根节点往上一提，我们就抓住了整棵树。接下来，实现树上的删改查操作</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Trie</span>(<span class="title class_ inherited__">Node</span>):</span><br><span class="line">    <span class="comment">#构造函数，初始化一个空的字典树节点</span></span><br><span class="line">    def_init_(self)-&gt;<span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>()._init_(<span class="literal">None</span>)</span><br><span class="line">    <span class="comment">#判断给定的键是否存在于字典树中。如果存在，返回True;否则返回False</span></span><br><span class="line">    def_contains_(self,key):</span><br><span class="line">        <span class="keyword">return</span> self[key] <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">    <span class="comment">#根据给定的键从字典树中获取对应的值。如果键不存在，返回None;否则返回对应的值。</span></span><br><span class="line">    def_getitem_(self,key):</span><br><span class="line">        state=self</span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> key:</span><br><span class="line">            state=state._children.get(char)</span><br><span class="line">            <span class="keyword">if</span> state <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">return</span> state._value</span><br><span class="line">   <span class="comment">#将给定的键值对插入到字典树中。如果键已经存在，则更新对应的值；否则创建新的节点并插入。 </span></span><br><span class="line">    def_setitem_(self,key,value):</span><br><span class="line">        state=self</span><br><span class="line">        <span class="keyword">for</span> i,char <span class="keyword">in</span> <span class="built_in">enumerate</span>(key):</span><br><span class="line">            <span class="keyword">if</span> i&lt;<span class="built_in">len</span>(key)-<span class="number">1</span>:<span class="comment">#判断当前字符是否是最后一个字符</span></span><br><span class="line">                state=state._add_child(char,<span class="literal">None</span>,<span class="literal">False</span>)<span class="comment">#将当前字符作为键添加到字典中，值为None，并且不设置为最后一个节点</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                state=state._add_child(char,value,<span class="literal">True</span>)<span class="comment">#最后一个字符，值为value，并且设置为最后一个节点。</span></span><br></pre></td></tr></table></figure><h3 id="3-首字散列其余二分的字典树"><a href="#3-首字散列其余二分的字典树" class="headerlink" title="3.首字散列其余二分的字典树"></a>3.首字散列其余二分的字典树</h3><p><strong>散列函数</strong>，它用来将对象转换为整数（称为<strong>散列值</strong>）。散列函数必须满足的基本要求是：对象相同，散列值必须相同。如果对象不同，散列值也不同，则称作完美散列。不完美的散列会导致多个对象被映射到同一个位置，不方便索引。即便是完美散列，如果散列值不连续，则无法以散列值为地址索引到整块内存，毕竟中间的空洞会造成浪费。一种解决办法是将不连续的散列值映射为连续散列值，但这势必带来额外的开销。可见，如果函数设计不当，则散列表的内存效率和查找效率都不高。</p><p>上一节的代码中，我们使用<strong>Python</strong>内置的dict作为散列表，键作为字符，值作为子节点。由于<strong>Python</strong>没有char类型，字符被视作长度为1的字符串，所以实际调用的就是str的散列函数。但两个字符由于Unicode字符集的差距会导致散列值相差很大。</p><p><strong>HanLP</strong>实现了泛型的字典树容器，其根节点的关键代码：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">public <span class="keyword">class</span> <span class="title class_">BinTree</span>&lt;V&gt; extends BaseNode&lt;V&gt; implements </span><br><span class="line"><span class="comment">#定义一个名为BinTree的公共类，该类继承自BaseNode并实现ITrie和Externalizable接口</span></span><br><span class="line">ITrie&lt;V&gt;,Externalizable</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">#定义一个私有整型变量size,用于存储树的大小</span></span><br><span class="line">    private <span class="built_in">int</span> size;</span><br><span class="line">    <span class="comment">#定义一个无参构造方法BinTrie</span></span><br><span class="line">    public BinTrie()</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">#初始化child数组，大小为65536（2^16）,用于存储子节点</span></span><br><span class="line">        child=new BaseNode[<span class="number">65535</span>+<span class="number">1</span>];</span><br><span class="line">        <span class="comment">#将size初始化为0，表示数的大小为0</span></span><br><span class="line">        size=<span class="number">0</span>;</span><br><span class="line">        <span class="comment">#将status设置为Status.NOT_WORD_1,表示当前节点不是一个完整的单词</span></span><br><span class="line">        status=Status.NOT_WORD_1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>其中,BaseNode是普通节点。普通节点与根节点没有多大差别，只不过child是按需增长的动态数组而已。相应地，普通节点的child数组不能直接通过散列得到下标，而是先维护数组的有序性，然后基于它进行二分查找。普通节点的算法与Python版的思路一致，单出来手写二分查找与动态数组外，还多了一些遍历、序列化、反序列化的代码。</p><h3 id="前缀树的妙用"><a href="#前缀树的妙用" class="headerlink" title="前缀树的妙用"></a>前缀树的妙用</h3><p>利用字典树的概念，还可以做的更快。字典树其实就是一棵前缀树。如何加速词典分词呢？在朴素实现中，会依次查询“自”，“自然”，“自然语”，“自然语言”等词语是否在字典中。但事实上，如果“自然”这条路径不存在于前缀树中，则可以断定一切“自然”开头的词语都不可能存在。也就是说，在状态转移失败时，就可以提前中断对以“自”开头的扫描。</p><p>考虑到“全切分”“最长匹配”是常见的操作，BinTrie提供了相应接口，分别对应parseText和parseLongestText。以parseText为例，它是这样利用前缀树做状态转移的。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">public void parseText(String text,AhoCorasickDoubleArrayTrie.IHit&lt;V&gt; processor)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">#获取文本的长度</span></span><br><span class="line">    <span class="built_in">int</span> length=text.length();</span><br><span class="line">    <span class="comment">#初始化开始位置为0</span></span><br><span class="line">    <span class="built_in">int</span> begin=<span class="number">0</span>;</span><br><span class="line">    <span class="comment">#初始化状态为当前节点</span></span><br><span class="line">    BaseNode&lt;V&gt; state=this;</span><br><span class="line">    <span class="comment">#遍历文本中的每个字符</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="built_in">int</span> i=begin;i&lt;length;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">#根据当前字符进行状态转移</span></span><br><span class="line">        state=state.transition(text.charAt(i));</span><br><span class="line">        <span class="comment">#如果状态不为空，表示成功转移到下一个状态</span></span><br><span class="line">        <span class="keyword">if</span>(state!=null)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">#获取当前状态的值</span></span><br><span class="line">            V value=state.getValue();</span><br><span class="line">            <span class="comment">#如果值不为空，表示找到了一个匹配的模式</span></span><br><span class="line">            <span class="keyword">if</span>(value!=null)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">#调用处理器的hit方法处理匹配结果</span></span><br><span class="line">                processor.hit(begin,i+<span class="number">1</span>,value);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">#如果状态为空，表示没有找到匹配的模式，需要回溯</span></span><br><span class="line">            i=begin;</span><br><span class="line">            ++begin;</span><br><span class="line">            state=this;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;   </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>state:当前状态</li><li>begin:当前扫描的起点</li><li>i:状态转移时接受字符的下标</li><li>processor:回调函数</li></ul><p>从根节点this开始，顺序选择起点，然后递增i进行状态转移（if分支）。一旦状态转移失败（else分支），对以begin开头的词语扫描立即终止，begin递增，最后重新开始新前缀的扫描。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>鹧鸪天.西都作</title>
      <link href="/2024/04/09/%E9%B9%A7%E9%B8%AA%E5%A4%A9.%E8%A5%BF%E9%83%BD%E4%BD%9C/"/>
      <url>/2024/04/09/%E9%B9%A7%E9%B8%AA%E5%A4%A9.%E8%A5%BF%E9%83%BD%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="鹧鸪天-西都作"><a href="#鹧鸪天-西都作" class="headerlink" title="鹧鸪天.西都作"></a>鹧鸪天.西都作</h2><div class='poem'><div class='poem-title'>鹧鸪天</div><div class='poem-author'>朱敦儒</div><p>我是清都山水郎，天教分付与疏狂。曾批给雨支风敕，累上留云借月章。</p><p>诗万首，酒千觞。几曾着眼看君王？玉楼金阙慵归去，切插梅花醉洛阳。</p></div>]]></content>
      
      
      <categories>
          
          <category> 诗情画意 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>《自然语言处理入门》学习day3</title>
      <link href="/2024/04/09/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E5%AD%A6%E4%B9%A0day3/"/>
      <url>/2024/04/09/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E5%AD%A6%E4%B9%A0day3/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p><a href="词典分词">2. 词典分词</a></p><ul><li><a href="什么是词">2.1 什么是词</a></li><li><a href="词典">2.2 词典</a></li><li><a href="切分算法">2.3 切分算法</a></li><li><a href="字典树">2.4 字典树</a></li><li><a href="基于字典树的其它算法">2.5 基于字典树的其它算法</a></li><li><a href="HanLP的词典分词实现">2.6 HanLP的词典分词实现</a></li></ul><h2 id="2-词典分词"><a href="#2-词典分词" class="headerlink" title="2.词典分词"></a>2.词典分词</h2><ul><li><strong>中文分词</strong>：指的是讲一段文本拆分成一系列单词的过程，这些单词按顺序拼接是原文本。</li><li>中文分词算法主要<strong>基于词典规则</strong>和<strong>基于机器学习</strong>这两大类</li></ul><h3 id="2-1什么是词"><a href="#2-1什么是词" class="headerlink" title="2.1什么是词"></a>2.1什么是词</h3><ul><li>在基于词典的中文分词中，词的定义现实得多：<strong>在词典中的字符串就是词</strong>。</li><li>词的性质：齐夫定律：一个单词的词频与它的词频排名成反比。</li></ul><h3 id="2-2词典"><a href="#2-2词典" class="headerlink" title="2.2词典"></a>2.2词典</h3><p>互联网词库(SogouW，15万个词条)、清华大学开放中文词库(THUOCL)、HanLP词库(千万级词条)</p><p>这里以HanLP附带的迷你核心词典为例:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">上升  V 98 vn 18</span><br><span class="line">上升期 n 1</span><br><span class="line">上升股 n 1</span><br><span class="line">上午   t 147</span><br><span class="line">上半叶 t 3</span><br><span class="line">上半场 n 2</span><br><span class="line">上半夜 t 1</span><br></pre></td></tr></table></figure><p>HanLP中的词典形式是一种以空格分隔的表格形式，第一列是单词本身，后几列依次是词性，词频，命名实体及相应频率。</p><h3 id="2-3切分算法"><a href="#2-3切分算法" class="headerlink" title="2.3切分算法"></a>2.3切分算法</h3><p>首先，加载词典：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_dictionary</span>():</span><br><span class="line">    dic=<span class="built_in">set</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#按行读取字典文件，每行第一个空格之前的字符串提取出来。</span></span><br><span class="line">    <span class="keyword">for</span> line inopen(<span class="string">&quot;文件地址&quot;</span>,<span class="string">&quot;r&quot;</span>):</span><br><span class="line">        dic.add(line[<span class="number">0</span>:line.find(<span class="string">&#x27; &#x27;</span>)])</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> dic</span><br></pre></td></tr></table></figure><p>1.<strong>完全切分</strong></p><p>指的是，找出一段文本中的所有单词。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fully_segment</span>(<span class="params">text,dic</span>):</span><br><span class="line">    word_list = []</span><br><span class="line">    <span class="comment">#i从0到text的最后一个字的下标遍历</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(text)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>,<span class="built_in">len</span>(text)+<span class="number">1</span>):</span><br><span class="line">            word=text[i:j]</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> dic:</span><br><span class="line">                word_list.append(word)</span><br><span class="line">    <span class="keyword">return</span> word_list</span><br><span class="line">dic=load_dictionary()</span><br><span class="line"><span class="built_in">print</span>(fully_segment(<span class="string">&#x27;就读北京大学&#x27;</span>,dic))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[&#x27;就&#x27;,&#x27;就读&#x27;,&#x27;读&#x27;,&#x27;北&#x27;,&#x27;北京大学&#x27;,&#x27;京&#x27;,&#x27;大&#x27;,&#x27;大学&#x27;,&#x27;学&#x27;]</span><br></pre></td></tr></table></figure><p>输出了所有可能的单词，因为词库中含有单字，所以也输出了一些单字。</p><p>2.<strong>正向最长匹配</strong></p><p>上面的输出并不是中文分词，我们更需要那种有意义的词语序列，而不是所有出现在词典中的单词所构成的链表。比如，我们希望“北京大学”成为一整个词，而不是“北京+大学”之类的碎片。具体来说，就是在某个下标为起点递增查词的过程中，优先输出更长的单词，这种规则被称为<strong>最长匹配算法</strong>。从前往后匹配则称为<strong>正向最长匹配</strong>，反之则称为<strong>逆向最长匹配</strong>。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_segment</span>(<span class="params">text,dic</span>):</span><br><span class="line">    word_list=[]</span><br><span class="line">    i=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(text)):</span><br><span class="line">        longest_word=text[i]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>,<span class="built_in">len</span>(text)+<span class="number">1</span>):</span><br><span class="line">            word=text[i:j]</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> dic:</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(word)&gt;<span class="built_in">len</span>(longest_word):</span><br><span class="line">                    longest_word=word</span><br><span class="line">        word_list.append(longest_word)</span><br><span class="line">        i+=<span class="built_in">len</span>(longest_word)</span><br><span class="line">   <span class="keyword">return</span> word_list</span><br><span class="line"></span><br><span class="line">dic=load_dictionary()</span><br><span class="line"><span class="built_in">print</span>(forward_segment(<span class="string">&#x27;就读北京大学&#x27;</span>,dic))</span><br><span class="line"><span class="built_in">print</span>(forward_segment(<span class="string">&#x27;研究生命起源&#x27;</span>,duc))</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[&#x27;就读&#x27;,&#x27;北京大学&#x27;]</span><br><span class="line">[&#x27;研究生&#x27;,&#x27;命&#x27;,&#x27;起源&#x27;]</span><br></pre></td></tr></table></figure><p>第二句话产生了误差，我们是要把“研究”提取出来，结果按照正向最长匹配算法就提取出了“研究生”，所以人们就想出了逆向最长匹配。</p><p>3.<strong>逆向最长匹配</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backward_segment</span>(<span class="params">text,dic</span>):</span><br><span class="line">    word_list=[]</span><br><span class="line">    i=<span class="built_in">len</span>(text)-<span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> i&gt;=<span class="number">0</span>:</span><br><span class="line">        longest_word=text[i]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,i):</span><br><span class="line">            word=text[j:i+<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> dic:</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(word)&gt;<span class="built_in">len</span>(longest_word):</span><br><span class="line">                    longest_word=word</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">         word_list.insert(<span class="number">0</span>,longest_word)</span><br><span class="line">         i-=<span class="built_in">len</span>(longest_word)</span><br><span class="line">   <span class="keyword">return</span> word_list</span><br><span class="line">dic=load_dictionary()</span><br><span class="line"><span class="built_in">print</span>(backward_segment(<span class="string">&#x27;研究生命起源&#x27;</span>,dic))</span><br><span class="line"><span class="built_in">print</span>(backward_segment(<span class="string">&#x27;项目的研究&#x27;</span>,dic))</span><br><span class="line">            </span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[&#x27;研究&#x27;,&#x27;生命&#x27;,&#x27;起源&#x27;]</span><br><span class="line">[&#x27;项&#x27;,&#x27;目的&#x27;,&#x27;研究&#x27;]</span><br></pre></td></tr></table></figure><p>第一句正确了，但下一句又出错了，可谓拆东墙补西墙。另一些人提出综合两种规则，期待它们取长补短，称为双向最长匹配。</p><p>4.<strong>双向最长匹配</strong></p><p>这是一种融合两种匹配算法的复杂规则集，流程如下：</p><ul><li>同时执行正向和逆向最长匹配，若两者的词数不同，则返回词数更少的那一个。</li><li>否则，返回两者中单字更少的那一个。当单字数也相同时，优先返回逆向匹配的结果。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">count_single_char</span>(<span class="params">word_list:<span class="built_in">list</span></span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(<span class="number">1</span> <span class="keyword">for</span> word <span class="keyword">in</span> word_list <span class="keyword">if</span> <span class="built_in">len</span>(word)==<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bidirectional_segment</span>(<span class="params">text,dic</span>):</span><br><span class="line">    f=forward_segment(text,dic)</span><br><span class="line">    b=backward_segment(text,dic)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(f)&lt;<span class="built_in">len</span>(b):</span><br><span class="line">        <span class="keyword">return</span> f</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">len</span>(f)&gt;<span class="built_in">len</span>(b):</span><br><span class="line">        <span class="keyword">return</span> b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> count_single_char(f)&lt;count_singlr_char(b):</span><br><span class="line">            <span class="keyword">return</span> f</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> b</span><br><span class="line"><span class="built_in">print</span>(bidirectional_segment(<span class="string">&#x27;研究生命起源&#x27;</span>,dic))</span><br><span class="line"><span class="built_in">print</span>(bidirectional_segment(<span class="string">&#x27;项目的研究&#x27;</span>,dic))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[&#x27;研究&#x27;,&#x27;生命&#x27;,&#x27;起源&#x27;]</span><br><span class="line">[&#x27;项&#x27;,&#x27;目的&#x27;,&#x27;研究&#x27;]</span><br></pre></td></tr></table></figure><p>通过以上几种切分算法，我们可以做一个对比：</p><p>上图所示，双向最长匹配的确在2,3,5这三种情况下选择出了最好的结果，但在4号句子上选择错误的结果，使得最终正确率3/6反而小于逆向最长匹配的4/6.由此，规则系统的脆弱可见一斑。规则集的维护有时是拆东墙补西墙，有时是帮倒忙。</p><h3 id="2-4字典树"><a href="#2-4字典树" class="headerlink" title="2.4字典树"></a>2.4字典树</h3><p>匹配算法的瓶颈之一在于如何判断集合(词典)中是否含有字符串。如果用有序集合(TreeMap)的话，复杂度是O(logn)(n是词典大小);如果用散列表(Java的HashMap.Python的dict)的话，账面上的时间复杂度虽然下降了，但内存复杂度却上去了。有没有速度又快、内存又省得数据结构呢？这就是<strong>字典树</strong>。</p><ol><li><p><strong>什么是字典树</strong></p><p>​    字符串集合常用字典树(trie树、前缀树)存储，这是一种字符串上的树形结构。字典树中每条边都对应一个字符，从根节点往下的路径构成一个个字符串。字典树并不直接在节点上存储字符串，而是将词语视作从跟节点到某节点的一条路径，并在终点节点(蓝色)上做个标记”该节点对应词语的结尾”。字符串就是一条路径，只需从根节点顺着这条路径往下走就能到达特殊标记的节点，则说明该字符串在集合中，否则说明不在。一个典型的字符串如下图所示</p><p><img src="https://github.com/NLP-LOVE/Introduction-NLP/raw/master/img/2020-2-4_14-46-52.png" alt=""></p></li></ol><p>​    其中，每个蓝色节点代表一个词语结尾，数字是人为的编号。按照路径我们可以得到如下表所示：</p><div class="table-container"><table><thead><tr><th>词语</th><th>路径</th></tr></thead><tbody><tr><td>入门</td><td>0-1-2</td></tr><tr><td>自然</td><td>0-3-4</td></tr><tr><td>自语</td><td>0-3-8</td></tr><tr><td>自然人</td><td>0-3-4-5</td></tr><tr><td>自然语言</td><td>0-3-4-6-7</td></tr></tbody></table></div><p>​    当词典大小为n时，算法时间复杂度依然为O(logn)(假设子节点用对数复杂度的数据结构存储，所有词语都是单字)，但它的实际速度比二分查找快，这是因为随着路径深入，前缀匹配是递进的过程，不必理会字符串的前缀。</p><ol><li><p><strong>字典树的实现</strong></p><p>由上图可知，每个节点都应该知道自己的子节点与对应的边，以及自己是否对应一个词。如果要实现映射而不是集合的话，还应该知道自己对应的值。我们约定用值为None表示节点不对应词语，虽然这样就不能插入值为None的键了，但实现起来更简洁。那么字典树的实现参照下图所示项目路径</p><p>通过<strong>debug运行 trie.py 代码</strong>，可以观察到 trie 类的字典树结构：</p><p><img src="https://github.com/NLP-LOVE/Introduction-NLP/raw/master/img/2020-2-4_16-49-15.png" alt=""></p></li></ol><h3 id="2-5基于字典树的其它算法"><a href="#2-5基于字典树的其它算法" class="headerlink" title="2.5基于字典树的其它算法"></a>2.5基于字典树的其它算法</h3><p>字典树的数据结构在以上的切分算法中已经很快了，但厉害的是作者通过自己的努力改进了基于字典树的算法，把分词速度推向了千万字每秒的级别，主要按照以下递进关系优化：</p><ul><li>首字散列其余二分的字典树</li><li>双数组字典树</li><li>AC自动机(多模式匹配)</li><li>基于双数组字典树的AC自动机</li></ul><h3 id="2-6-HanLP的词典分词实现"><a href="#2-6-HanLP的词典分词实现" class="headerlink" title="2.6 HanLP的词典分词实现"></a>2.6 HanLP的词典分词实现</h3><p>1.<strong>DoubleArrayTrieSegment</strong></p><p>​    DoubleArrayTrieSegment分词器是对DAT最长匹配的封装，默认加载hanlo.proeries中CoreDictionaryPath制定的词典。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyhanlp <span class="keyword">import</span> *</span><br><span class="line"><span class="comment">#不显示词性</span></span><br><span class="line">HanLP.Config.ShowTermNature=<span class="literal">False</span></span><br><span class="line"><span class="comment">#可传入自定义字典[dir1,dir2]</span></span><br><span class="line">segment=DoubleArrayTrieSegment()</span><br><span class="line"><span class="comment">#激活数字和英文识别</span></span><br><span class="line">segment.enablePartOfSpeechTagging(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(segment.seg(<span class="string">&quot;江西鄱阳湖干枯，中国最大淡水湖变成大草原&quot;</span>))</span><br><span class="line"><span class="built_in">print</span>(segment.seg(<span class="string">&quot;上海市虹口区大连西路550号SISU&quot;</span>))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[江西, 鄱阳湖, 干枯, ，,中国, 最大, 淡水湖, 变成, 大草原]</span><br></pre></td></tr></table></figure><p>2.<strong>去掉停用词</strong></p><p>停用词词典文件：<a href="https://github.com/NLP-LOVE/Introduction-NLP/blob/master/data/dictionnary/stopwords.txt">data/dictionnary/stopwords.txt</a></p><p>该词典收录了常见的中英文无意义词汇(不含敏感词)，每行一个词。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_from_file</span>(<span class="params">path</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    从词典文件加载DoubleArrayTrie</span></span><br><span class="line"><span class="string">    :param path: 词典路径</span></span><br><span class="line"><span class="string">    return：双数组trie树</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">map</span>=JClass(<span class="string">&#x27;Java.util.TreeMap&#x27;</span>)()<span class="comment">#创建TreeMap实例</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path) <span class="keyword">as</span> src:</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> src:</span><br><span class="line">            word=word.strip()</span><br><span class="line">            <span class="built_in">map</span>[word]=word()</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">JClass(<span class="string">&#x27;com.hankcs.hanlp.collection.trie.DoubleArrayTrie&#x27;</span>)(<span class="built_in">map</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 去掉停用词</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">remove_stopwords_termlist</span>(<span class="params">termlist,trie</span>):</span><br><span class="line">    <span class="keyword">return</span> [term.word <span class="keyword">for</span> term <span class="keyword">in</span> termlist <span class="keyword">if</span> <span class="keyword">not</span></span><br><span class="line">trie.containsKey(term.word)]</span><br><span class="line">    </span><br><span class="line">trie=load_from_file(<span class="string">&#x27;stopwords.txt&#x27;</span>)</span><br><span class="line">termlist=segment.seg(<span class="string">&quot;江西鄱阳湖干枯了，中国最大的淡水湖变成了大草原&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;去掉停用词前：&#x27;</span>,termlist)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;去掉停用词后：&#x27;</span>,remove_stopwords_termlist(termlist,trie))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">去掉停用词前：[江西, 鄱阳湖, 干枯, 了, ，,中国, 最大, 的, 淡水湖, 变成, 了, 大草原]</span><br><span class="line">去掉停用词后：[&#x27;江西&#x27;,&#x27;鄱阳湖&#x27;,&#x27;干枯&#x27;,&#x27;中国&#x27;,&#x27;最大&#x27;,&#x27;淡水湖&#x27;,&#x27;变成&#x27;,&#x27;大草原&#x27;]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>《自然语言处理入门》学习Day1</title>
      <link href="/2024/04/07/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E5%AD%A6%E4%B9%A0Day1/"/>
      <url>/2024/04/07/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E5%AD%A6%E4%B9%A0Day1/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p><a href="#1-新手上路">1.新手上路</a></p><ul><li><a href="">1.1 自然语言与编程语言的比较</a></li><li><a href="">1.2 自然语言处理的层次</a></li><li><a href="">1.3 自然语言处理的流派</a></li><li><a href="">1.4 机器学习</a></li><li><a href="">1.5 语料库</a></li><li><a href="">1.6 开源工具</a></li><li><a href="">1.7 总结</a></li></ul><h2 id="1-新手上路"><a href="#1-新手上路" class="headerlink" title="1.新手上路"></a>1.新手上路</h2><p>自然语言处理（Natural Language Processing, NLP）是一门融合了计算机科学、人工智能及语言学的交叉学科，它们的关系如下图所示。这门学科研究的是如何通过机器学习等技术，让计算机学会处理人类语言，乃至实现终极目标—理解人类语言或人工智能。</p><p><img src="https://github.com/NLP-LOVE/Introduction-NLP/raw/master/img/2020-2-3_10-50-30.png" alt=""></p><h3 id="1-1自然语言与编程语言的比较"><a href="#1-1自然语言与编程语言的比较" class="headerlink" title="1.1自然语言与编程语言的比较"></a>1.1自然语言与编程语言的比较</h3><div class="table-container"><table><thead><tr><th>比较</th><th>不同</th><th>例子</th></tr></thead><tbody><tr><td>词汇量</td><td>自然语言中的词汇比编程语言中的关键词丰富，我们还可以随时创造各种类型的新词</td><td>蓝瘦、香菇</td></tr><tr><td>机构化</td><td>自然语言是非结构化的，而编程语言是结构化的</td><td></td></tr><tr><td>歧义性</td><td>自然语言中含有大量歧义，而编程语言是结构化的</td><td>这人真有意思：没意思</td></tr><tr><td>容错性</td><td>自然语言错误随处可见，而编程语言错误会导致编译不通过</td><td>的、地的用法错误</td></tr><tr><td>易变性</td><td>自然语言变化相对迅速嘈杂一些，而编程语言的变化要缓慢的多</td><td>新时代词汇</td></tr><tr><td>简略性</td><td>自然语言往往简洁、干练，而编程语言就要明确定义</td><td>“老地方”不必明确指出</td></tr></tbody></table></div><h3 id="1-2自然语言处理的层次"><a href="#1-2自然语言处理的层次" class="headerlink" title="1.2自然语言处理的层次"></a>1.2自然语言处理的层次</h3><p><img src="https://github.com/NLP-LOVE/Introduction-NLP/raw/master/img/2020-2-3_11-17-38.png" alt=""></p><ol><li><p><strong>语音、图像和文本</strong></p><p>自然语言处理系统的输入源一共有三个，即语音、图像与文本。语音和图像这两种形式一般经过识别后转化为文字，转化后就可以进行后续的NLP任务了。</p></li><li><p><strong>中文分词、词性标注和命名实体识别</strong></p><p>这三个任务都是围绕词语进行的分析，所以统称<strong>词法分析</strong>。词法分析的主要任务是将文本分隔为有意义的词语(<strong>中文分词</strong>)，确定每个词语的类别和浅层的歧义消除(词性标注)，并且识别出一些较长的专有名词(<strong>命名实体识别</strong>)。对中文而言，词性分析常常是后续高级任务的基础。</p></li><li><p><strong>信息抽取</strong></p><p>词法分析之后，文本已经呈现出部分结构化的趋势，根据分析出来的每个单词和附有自己词性及其他标签的数据，抽取出一部分有用的信息、关键词、专业术语等，也可以根据统计学信息抽取出更大颗粒度的文本。</p></li><li><p><strong>文本分类与文本聚类</strong></p><p>将文本拆分为一系列词语之后，就可以对文本进行分类和聚类操作，找出相似的文本。</p></li><li><p><strong>句法分析</strong></p><p>词法分析只能得到零散的词汇信息，计算机不知道词语之间的关系，在一些问答系统中，需要得到句子的主谓宾结构，这就是句法分析得到的结果，如下图所示：</p><p><img src="https://github.com/NLP-LOVE/Introduction-NLP/raw/master/img/2020-2-3_12-8-55.png" alt=""></p><p>不仅是问答系统或搜索引擎，句法分析还经常应用有基于短语的机器翻译，给译文的词语重新排序。</p></li><li><p><strong>语义分析与篇章分析</strong></p><p>相较于语法分析，语义分析侧重语义而非语法。它包括<strong>词义消歧</strong>(确定一个词在语境中的含义，而不是简单的词性)、<strong>语义角色标注</strong>(标注句子中的谓语与其他成分的关系)乃至<strong>语义依存分析</strong>(分析句子中词之间的语义关系)。</p></li><li><p><strong>其他高级任务</strong></p><p>自动问答、自动摘要、机器翻译</p><p>注意，一般认为信息检索(Information Retrieve,IR)是区别于自然语言处理的独立学科，IR的目标是查询信息，而NLP的目标是理解语言。</p></li></ol><h3 id="1-3自然语言处理的流派"><a href="#1-3自然语言处理的流派" class="headerlink" title="1.3自然语言处理的流派"></a>1.3自然语言处理的流派</h3><ol><li><p><strong>基于规则的专家系统</strong></p><p>规则，指的是由专家手工制定的确定性流程。专家系统要求设计者对所处理的问题具备深入的理解，并且尽量以人力全面考虑所有所有可能的情况。它最大的弱点是难以拓展。当规则数量增加或者多个专家维护同一个系统时，就容易出现冲突。</p></li><li><p><strong>基于统计的学习方法</strong></p><p>人们使用统计方法让计算机自动学习语言。所谓”统计”，指的是在语料库上进行的统计。所谓”<strong>语料库</strong>“，指的是人工标注的结构化文本。</p></li><li><p><strong>历史</strong></p><p><img src="https://github.com/NLP-LOVE/Introduction-NLP/raw/master/img/2020-2-3_12-26-11.png" alt=""></p></li></ol><h3 id="1-4机器学习"><a href="#1-4机器学习" class="headerlink" title="1.4机器学习"></a>1.4机器学习</h3><ol><li><p><strong>什么是机器学习</strong></p><p><strong>机器学习</strong>指的是计算机通过某项任务的经验数据提高了在该项任务上的能力。</p></li><li><p><strong>模型</strong></p><p>模型是对现实问题的数学抽象，由一个假设函数以及一系列参数构成。以下就是最简单的模型公式：</p><p><img src="https://github.com/NLP-LOVE/Introduction-NLP/raw/master/img/2020-2-3_19-41-15.png" alt=""></p></li><li><p><strong>特征</strong></p><ul><li><strong>特征</strong>指的是事物的特点转化的数值。</li><li>如何挑选特征，如何设计特征模板，这称作<strong>特征工程</strong>。特征越多，参数就越多；参数越多，模型就越复杂。</li></ul></li><li><p><strong>数据集</strong></p><p>样本的集合在机器学习领域被称作<strong>数据集</strong>，在自然语言处理领域称作<strong>语料库</strong>。</p></li><li><p><strong>监督学习</strong></p><p>如果数据集附带标准答案y，则此时的学习算法称作<strong>监督学习</strong>。学习一遍误差还不够小，需要反复学习、反复调整。此时的算法是一种迭代式的算法，每一遍学习称作<strong>一次迭代</strong>。这种在有标签的数据集上迭代学习的过程称作<strong>训练</strong>。</p></li><li><p><strong>无监督学习</strong></p><p>如果我们只给机器做题，却不告诉它参考答案，机器学习仍然可以学到知识吗？可以！此时的学习称作<strong>无监督学习</strong>，而不含标准答案的数据集被称作<strong>无标注的数据集</strong>。无监督学习一般用于聚类和降维，<strong>降维</strong>指的是将样本点从高维空间变换成低维空间的过程。</p></li><li><p><strong>其他类型的机器学习算法</strong></p><ul><li><strong>半监督学习</strong>：如果我们训练多个模型，然后对同一个实例进行预测，会得到多个结果。如果这些结果多数一致，则可以将结果和实例放到一起作为一个训练样本，并扩充训练集。这样的算法称作半监督学习。</li><li><strong>强化学习</strong>：现实世界中的事物往往具有很强的因果链：我们要正确地执行一系列彼此关联的决策，才能得到最终的成果。这样的问题往往需要一边预测，一边根据环境的反馈规划下次的决策。这类算法称作强化学习。</li></ul></li></ol><h3 id="1-5语料库"><a href="#1-5语料库" class="headerlink" title="1.5语料库"></a>1.5语料库</h3><ol><li><p><strong>中文分词语料库</strong></p><p>中文分词语料库指的是，由人工正确切分的句子集合。以著名的1998年《人民日报》语料库为例：</p><blockquote><p>先 有 通货膨胀 干扰，后 有 通货 紧缩 叫板</p></blockquote></li><li><p><strong>词性标注语料库</strong></p><p>它指的是切分并为每个词语制定一个词性的语料。依然以《人民日报》语料库为例：</p><blockquote><p>迈向/v 充满/v 希望/n 的/u 新/a 世纪/n — 一九九八年/t 新年/t 讲话/n</p></blockquote></li><li><p><strong>命名实体识别语料库</strong></p><p>这种语料库人工标注了坐着关心的实体名词或者实体类别。比如《人民日报》中包含了人名、地名和机构名三种命名实体：</p><blockquote><p><strong>萨哈夫/n</strong> 说/v,/w <strong>伊拉克/ns</strong> 将/d 同/p <strong>[联合国/nt 销毁/v 伊拉克/ns  大规模/b 杀伤性/n 武器/n 特别/a 委员会/n]/nt</strong> 继续/v 保持/v 合作/v。/w</p></blockquote><p>这个句子加粗的部分分别是人名，地名，机构名，中括号里的是由机构名和地名构成的复合句，从这可看出机构名和地名会构成更长的机构名，这种构词法的嵌套现象增加了命名实体识别的难度</p></li><li><p><strong>句法分析语料库</strong></p><p><img src="https://github.com/NLP-LOVE/Introduction-NLP/raw/master/img/2020-2-3_13-40-56.png" alt=""></p></li><li><p><strong>文本分类语料库</strong></p><p>它指的是人工标注了所属分类的文章构成的语料库。</p></li><li><p><strong>语料库的建设</strong></p><p>语料库建设指的是构建一份语料库的过程，分为规范制定，人员培训与人工标注三个阶段。针对不同类型的任务，人们开发出许多标注的软件，其中比较成熟的一款是brat，它支持词性标注、命名实体识别和句法分析等任务。</p></li></ol><h3 id="1-6开源工具"><a href="#1-6开源工具" class="headerlink" title="1.6开源工具"></a>1.6开源工具</h3><ol><li><p><strong>主流NLP工具比较</strong></p><p><img src="https://github.com/NLP-LOVE/Introduction-NLP/raw/master/img/2020-2-3_13-52-12.png" alt=""></p><p>本书采用<strong>HanLP作为本书的实现</strong></p></li><li><p><strong>Python接口</strong></p><p>HanLP的Python接口由pyhanlp包提供，其安装只需一句命令：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$ pip install pyhanlp</span><br></pre></td></tr></table></figure></li></ol><h3 id="1-7总结"><a href="#1-7总结" class="headerlink" title="1.7总结"></a>1.7总结</h3><p>本章给出了人工智能、机器学习和自然语言处理的宏观缩略图与发展时间线。机器学习是人工智能的子集，而自然语言处理是人工智能、语言学和计算机科学的交集。这个交集虽然小，它的难度却很大。为了实现理解自然语言这个宏伟目标，人们尝试了规则系统，最终发展到基于大规模语料库的统计学习系统。</p><p>在接下来的章节中要学习第一个NLP问题—中文分词，从规则系统入手，介绍一些快而不准的算法，然后逐步进化到更加准确的统计模型。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>南乡子</title>
      <link href="/2024/04/06/%E5%8D%97%E4%B9%A1%E5%AD%90.%E5%92%8C%E6%9D%A8%E5%85%83%E7%B4%A0%E6%97%B6%E7%A7%BB%E7%A7%BB%E5%AE%88%E5%AF%86%E5%B7%9E/"/>
      <url>/2024/04/06/%E5%8D%97%E4%B9%A1%E5%AD%90.%E5%92%8C%E6%9D%A8%E5%85%83%E7%B4%A0%E6%97%B6%E7%A7%BB%E7%A7%BB%E5%AE%88%E5%AF%86%E5%B7%9E/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id=""><a href="#" class="headerlink" title=" "></a> </h2><div class='poem'><div class='poem-title'>南乡子</div><div class='poem-author'>苏轼</div><p>东武望余杭，云海天涯两杳茫。何日功成名遂了，还乡，醉笑陪公三万场。</p><p>不用诉离殇，痛饮从来别有肠。今夜送归灯火冷，河塘，堕泪羊公却姓杨。</p></div>]]></content>
      
      
      <categories>
          
          <category> 诗情画意 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>NLP学习day5：词向量汇总：word2vec、fastText、Glove,Bert</title>
      <link href="/2024/04/06/NLP%E5%AD%A6%E4%B9%A0day5/"/>
      <url>/2024/04/06/NLP%E5%AD%A6%E4%B9%A0day5/</url>
      
        <content type="html"><![CDATA[<p>-</p><h2 id="一、Onehot"><a href="#一、Onehot" class="headerlink" title="一、Onehot"></a>一、Onehot</h2><h3 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h3><ol><li>解决了分类器处理离散数据困难的问题</li><li>一定程度上起到了扩展特征的作用</li></ol><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol><li>one-hot是一个词袋模型，不考虑词与词之间的顺序问题，而在文本中次序是一个很重要的问题</li><li>one-hot是基于词与词之间相互独立的情况下的，然而在多数情况中，词与词之间应该是相互影响的</li><li>one-hot得到的特征是离散的、稀疏的</li></ol><h2 id="二、TF-IDF与TextRank"><a href="#二、TF-IDF与TextRank" class="headerlink" title="二、TF-IDF与TextRank"></a>二、TF-IDF与TextRank</h2><p>NLP中最为经典的关键词提取算法，虽然简单，但应用广泛</p><h3 id="1-TF-IDF-参见上文"><a href="#1-TF-IDF-参见上文" class="headerlink" title="1. TF-IDF:参见上文"></a>1. TF-IDF:参见上文</h3><p><strong>总结</strong>:</p><ul><li>当一个词在文档频率越高并且新鲜度高（即普遍度低），其TF-IDF值越高。</li><li>TF-IDF兼顾词频与新鲜度，过滤一些常见词，保留能提供更多信息的重要词。</li></ul><h3 id="2-TextRank简介"><a href="#2-TextRank简介" class="headerlink" title="2. TextRank简介"></a>2. TextRank简介</h3><p>通过词之间的相邻关系构建网络，然后用<strong>PageRank</strong>迭代计算每个节点的rank值，排序rank值即可得到关键词。</p><p>PageRank本来是用来解决网页排名的问题，网页之间的链接关系即为图的边，迭代计算公式如下：</p><script type="math/tex; mode=display">PR(Vi)=(1-d)+d*\sum_{j}</script>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Docker学习Day1</title>
      <link href="/2024/04/04/Docker%E5%AD%A6%E4%B9%A0day1/"/>
      <url>/2024/04/04/Docker%E5%AD%A6%E4%B9%A0day1/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>随着AI,云原生等技术的向前推进，容器技术逐渐成为每位算法同学的必备技能之一，开始关于容器技术的学习很有必要。我想从零基础实现将代码打包docker镜像-调试-提交仓库-提交云服务训练模型/天池大赛提交/学校服务器训练等流程。希望初次接触docker能够做到以上步骤。</p><h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><p>docker作为虚拟机领域成熟的轻量化容器产品，可以轻松的将代码和所依赖的整个环境（包括整个操作系统）都打包在一起，不依赖于软件环境，方便把自己的代码从windows电脑分享到mac电脑运行、或者服务器上运行等。</p><p>docker三要素：<strong>镜像(image)，容器(contarin)，registry(包含多个仓库)</strong></p><p>以下是对三个要素的分别解释：</p><h3 id="镜像"><a href="#镜像" class="headerlink" title="镜像"></a>镜像</h3><p>顾名思义就是将要把代码以及环境打包在一起的这个产物就叫做镜像</p><h3 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h3><p>运行起来的镜像称之为容器，可以理解为运行环境或者实例。其实质是进程，随着代码运行结束，进程结束容器也就消失了。</p><h3 id="registry"><a href="#registry" class="headerlink" title="registry"></a>registry</h3><p>镜像存储的地方在registry,这是各云厂商提供的镜像存取服务，类似于网盘，将镜像存储在云端仓库，方便我们随时随地在不同介质运行自己的代码或分享代码。如果你要把本地开发好的代码放在服务器上做耗时的训练动作，那么只需要在服务器上直接拉取自己云端的镜像运行即可。类似于git还有代码管理功能。</p>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>NLP学习day4：结合tfidf与RidgeClassifier(岭回归)计算f1_score得分</title>
      <link href="/2024/04/04/NLP%E5%AD%A6%E4%B9%A0day4/"/>
      <url>/2024/04/04/NLP%E5%AD%A6%E4%B9%A0day4/</url>
      
        <content type="html"><![CDATA[<h1 id="代码展示"><a href="#代码展示" class="headerlink" title="代码展示"></a>代码展示</h1><h2 id="导入相关库与工具"><a href="#导入相关库与工具" class="headerlink" title="导入相关库与工具"></a>导入相关库与工具</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br></pre></td></tr></table></figure><h2 id="提取数据表"><a href="#提取数据表" class="headerlink" title="提取数据表"></a>提取数据表</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df=pd.read_csv(<span class="string">&quot;C:\Users\hjg\OneDrive\桌面\train_set.csv&quot;</span>,sep=<span class="string">&#x27;\t&#x27;</span>,nrows=<span class="number">20000</span>)</span><br></pre></td></tr></table></figure><h2 id="提取tfidf值"><a href="#提取tfidf值" class="headerlink" title="提取tfidf值"></a>提取tfidf值</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tfidf=Vectorizer(ngram_range=(<span class="number">1</span>,<span class="number">3</span>),max_feature=<span class="number">3000</span>)</span><br><span class="line">train_test=tfidf.fit_transfrom(train_df[<span class="string">&#x27;text&#x27;</span>])</span><br></pre></td></tr></table></figure><h2 id="构建岭回归，拟合训练模型"><a href="#构建岭回归，拟合训练模型" class="headerlink" title="构建岭回归，拟合训练模型"></a>构建岭回归，拟合训练模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf=RidgeClassifier()</span><br><span class="line">clf.fit(train_test[:<span class="number">1000</span>],train_df[<span class="string">&#x27;label&#x27;</span>].values[:<span class="number">1000</span>])</span><br><span class="line">vlt_per=clf.predict(tran_test[<span class="number">1000</span>:])</span><br></pre></td></tr></table></figure><h2 id="计算f1-score分数"><a href="#计算f1-score分数" class="headerlink" title="计算f1_score分数"></a>计算f1_score分数</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(f1_score(train_df[<span class="string">&#x27;label&#x27;</span>].values[<span class="number">1000</span>:],vlt_pre,averages=<span class="string">&#x27;macro&#x27;</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>NLP学习Day3：基于TF—IDF机器学习模型完成新闻文本分类</title>
      <link href="/2024/04/02/NLP%E5%AD%A6%E4%B9%A0day3/"/>
      <url>/2024/04/02/NLP%E5%AD%A6%E4%B9%A0day3/</url>
      
        <content type="html"><![CDATA[<h2 id="Task3-基于机器学习的文本分类"><a href="#Task3-基于机器学习的文本分类" class="headerlink" title="Task3 基于机器学习的文本分类"></a>Task3 基于机器学习的文本分类</h2><h3 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>学会TF-IDF的原理</li><li>使用sklearn的机器学习模型完成文本分类</li></ul><h2 id="机器学习模型"><a href="#机器学习模型" class="headerlink" title="机器学习模型"></a>机器学习模型</h2><p>机器学习是对能通过经验自动改进的计算机算法的研究机器学习通过历史数据<strong>训练</strong>出<strong>模型</strong>对应于人类经验进行<strong>归纳</strong>的过程，机器学习利用<strong>模型</strong>对新数据进行<strong>预测</strong>对应于人类利用总结的<strong>规律</strong>对新问题进行预测的过程。</p><p>机器学习有很多分支，对于学习者来说应该优先掌握机器学习算法的分类，然后在其中一种机器学习算法进行学习。</p><p>机器学习初学者，应该知道如下的事情：</p><ol><li>机器学习能解决一定的问题，但不能奢求机器学习是万能的</li><li>机器学习算法有很多种，看具体问题进行选择算法</li><li>每种机器学习算法有一定的偏好，具体问题具体分析</li></ol><h2 id="文本表示方法-Part1"><a href="#文本表示方法-Part1" class="headerlink" title="文本表示方法 Part1"></a>文本表示方法 Part1</h2><p>首先要知道在机器学习算法的训练过程中，假设给定N个样本，每个样本有M个特征，这样组成了N<em>M的样本举证，然后完成算法的训练和预测。但在NLP中这样的方法是不行的：文本是不定长度的。文本表示成计算机能够运算的数字或向量的方法一般称为<em>*词嵌入方法</em></em>。词嵌入将不定长的文本转换到定长的空间内，是文本分类的第一步。</p><h2 id="One-hot"><a href="#One-hot" class="headerlink" title="One-hot"></a>One-hot</h2><p>这里的One-hot与数据挖掘任务中的操作是一致的，即将每一个单词使用一个离散的向量表示。具体讲灭各自/词编码一个索引，然后根据索引进行赋值。</p><p>One-hot表示方法的例子如下：</p><blockquote><p>句子1：我 爱 北 京 天 安 门</p><p>句子2：我 喜 欢 上 海</p></blockquote><p>首先对所有句子的字进行索引，即将每个字分配一个编号：</p><blockquote><p>{</p><p>‘我’：1，‘爱’：2…………….’海’：11</p><p>}</p></blockquote><p>在这里共包括11个字，每个字可以转换为一个11维度稀疏向量：</p><p>我：[1,0,0,0,0,0,0,0,0,0,0]</p><p>爱：[0,1,0,0,0,0,0,0,0,0,0]</p><p>…….</p><p>海：[0,0,0,0,0,0,0,0,0,0,1]</p><h2 id="Bag-of-Words"><a href="#Bag-of-Words" class="headerlink" title="Bag of Words"></a>Bag of Words</h2><p>词袋表示，每个文档的字/词可以使用其出现次数来进行表示。词袋模型之所以被称为“词袋”模型，是因为这种模型在实现文本数值化过程中将所有词汇扔进一个袋子中，而忽略了它们在原文中的语法结构和顺序，只关心每个词汇出现的次数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction</span><br><span class="line">corpus=[</span><br><span class="line">    <span class="string">&#x27;This is the frist document.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;This document is the second document.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;And this is the third one.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Is this the first document&#x27;</span>,</span><br><span class="line">]</span><br><span class="line">vectorizer=CountVectorizer()</span><br><span class="line">vectorizer.fit_transform(corpus).toarray()</span><br></pre></td></tr></table></figure><h3 id="文本数值化分析过程"><a href="#文本数值化分析过程" class="headerlink" title="文本数值化分析过程"></a>文本数值化分析过程</h3><ul><li>对文本分词</li><li>去除停用词</li><li>构建词汇表</li><li>文本向量化</li></ul><h2 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h2><p>TF-IDF分数由两部分组成：第一部分是<strong>词语频率</strong>（Term Frequency），第二部分<strong>逆文档频率</strong>（Lnverse Document）。其中计算语料库中文档总数除以含有该词语的文档数量，然后再取对数就是逆文档频率。</p><h3 id="TF-IDF作用："><a href="#TF-IDF作用：" class="headerlink" title="TF-IDF作用："></a>TF-IDF作用：</h3><ul><li>评估每个词在文档中的重要性，以实现对关键词的抽取</li><li>将输入文档表示为向量（关键词重要向量），可用于文档检索</li></ul><h3 id="1-算法公式"><a href="#1-算法公式" class="headerlink" title="1.算法公式"></a>1.算法公式</h3><p>TF-IDF=TF*IDF，评估一个词的重要性要综合考虑该词的TF值和IDF值。</p><ol><li>TF:表示一个词在文档中的出现次数</li><li>IDF:表示包含某个词的文档数量，如果包含该词的文档数量越少则IDF值越大</li></ol><p>例如，词A和词B是在某个文档中出现的词，出现数量一样多，当：</p><ol><li>词A在其他文档中大量出现</li><li>词B在其它文档中少量出现</li></ol><p>则认为：词A的重要程度不如词B</p><p><strong><em>TF-IDF认为词A在其它文档中大量出现，说明该词很普遍，会降低该词的重要性。反之，则认为该词更为重要</em></strong></p><h4 id="TF（词频）"><a href="#TF（词频）" class="headerlink" title="TF（词频）"></a>TF（词频）</h4><p>TF（词频）=某个词在文档中出现的次数</p><p>TF~month~=1+log(TF)</p><h4 id="IDF（逆文档词频）"><a href="#IDF（逆文档词频）" class="headerlink" title="IDF（逆文档词频）"></a>IDF（逆文档词频）</h4><p>IDF（逆文档词频）=<script type="math/tex">\log{\left(\frac{文档总数}{包含某个词的文档数量}\right)}</script></p><p>IDF~month~=<script type="math/tex">\log{\left(\frac{文档总数+1}{包含某个词的文档数量+1}\right)}</script></p><h4 id="公式反映出的信息是什么？"><a href="#公式反映出的信息是什么？" class="headerlink" title="公式反映出的信息是什么？"></a>公式反映出的信息是什么？</h4><p>文档集合中，包含某个词的文档数量越多，该词的IDF值就越小，反之则越大。</p><h4 id="为什么进行log运算？"><a href="#为什么进行log运算？" class="headerlink" title="为什么进行log运算？"></a>为什么进行<script type="math/tex">log</script>运算？</h4><p>对数函数能够缩小数据范围，将大范围的值映射到相对较小的范围。通过取对数，可以有效地缩小高频词的影响，使得低频词在计算中更具影响力</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>NLP学习Day2：数据读取与数据分析</title>
      <link href="/2024/04/01/NLP%E5%AD%A6%E4%B9%A0day2/"/>
      <url>/2024/04/01/NLP%E5%AD%A6%E4%B9%A0day2/</url>
      
        <content type="html"><![CDATA[<h1 id="NLP学习Day2"><a href="#NLP学习Day2" class="headerlink" title="NLP学习Day2"></a>NLP学习Day2</h1><h2 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h2><ul><li>学习使用pandas读取赛题数据</li><li>分析赛题数据的分布规律</li></ul><h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><p>首先文本数据是使用csv格式进行存储。因此可以直接使用pandas完成数据读取的操作</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">train_df=pd.read_csv(<span class="string">&#x27;文本地址&#x27;</span>,sep=<span class="string">&#x27;\t&#x27;</span>,nrows=<span class="number">100</span>)</span><br></pre></td></tr></table></figure><h3 id="read-csv解析"><a href="#read-csv解析" class="headerlink" title="read_csv解析"></a>read_csv解析</h3><ul><li>读取的文件路径要改成本地的路径（相对路径或绝对路径）</li><li>分隔符<strong>sep</strong>,为每列分割的字符，设置为<strong>\t</strong>即可；</li><li>读取行数nrows,为每次读取文件的函数，是数值类型（数据集比较大）</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df.head()</span><br></pre></td></tr></table></figure><h2 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h2><p>在数据读取操作完成之后，要对数据集进行数据分析的操作，我们希望在读取训练集数据之后得到以下结论：</p><ul><li>赛题数据中，新闻文本的长度是多少？</li><li>赛题数据的类别分布是怎么样的，哪些类别比较多？</li><li>赛题数据中，字符分布是怎么样的？</li></ul><h3 id="句子长度分析"><a href="#句子长度分析" class="headerlink" title="句子长度分析"></a>句子长度分析</h3><p>在赛题数据中每行句子的字符使用空格进行隔开，所以可以直接统计单词的个数来得到每个句子的长度</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%pylab inline</span><br><span class="line">train_df[<span class="string">&#x27;text_len&#x27;</span>]=train_df[<span class="string">&#x27;text&#x27;</span>].apply(<span class="keyword">lambda</span> x:<span class="built_in">len</span>(x,split(<span class="string">&#x27; &#x27;</span>)))</span><br><span class="line">train_df[<span class="string">&#x27;text_len&#x27;</span>].describe()</span><br></pre></td></tr></table></figure><p>下面将句子长度绘制直方图</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.hist(train_df[<span class="string">&#x27;text_len&#x27;</span>],bins=<span class="number">200</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Text char count&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Histogram of char count&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="新闻类别分布"><a href="#新闻类别分布" class="headerlink" title="新闻类别分布"></a>新闻类别分布</h2><p>对数据集的类别进行分布统计，具体统计没类新闻的样本个数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df[<span class="string">&#x27;label&#x27;</span>].value_counts().plot(kind=<span class="string">&#x27;bar&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;News class count&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;category&quot;</span>)</span><br></pre></td></tr></table></figure><p>在数据集中标签的对应关系如下：</p><blockquote><p>在数据集中标签的对应的关系如下：{‘科技’: 0, ‘股票’: 1, ‘体育’: 2, ‘娱乐’: 3, ‘时政’: 4, ‘社会’: 5, ‘教育’: 6, ‘财经’: 7, ‘家居’: 8, ‘游戏’: 9, ‘房产’: 10, ‘时尚’: 11, ‘彩票’: 12, ‘星座’: 13}</p></blockquote><p><strong>从统计结果看出，赛题的数据集类别分布存在较为不均匀的情况，科技类新闻最多</strong></p><h2 id="字符分布统计"><a href="#字符分布统计" class="headerlink" title="字符分布统计"></a>字符分布统计</h2><p>接下来可以统计每个字符出现的次数，首先将训练集中所有的句子进行拼接进而划分为字符，并统计每个字符的个数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">all_lines=<span class="string">&#x27; &#x27;</span>.join(<span class="built_in">list</span>(train_df[<span class="string">&#x27;text&#x27;</span>]))</span><br><span class="line">word_count=Counter(all_lines.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">word_count=<span class="built_in">sorted</span>(word_count.items(),key=<span class="keyword">lambda</span> d:d[<span class="number">1</span>],reserve=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(word_count))</span><br><span class="line"><span class="built_in">print</span>(word_count[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(word_count[-<span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>还可根据字在每个句子的出现情况，反推出标点符号。下面代码统计了不同字符在句子中出现的次数</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">train_df[<span class="string">&#x27;text_unique&#x27;</span>]=train_df[<span class="string">&#x27;text&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="string">&#x27; &#x27;</span>.join(<span class="built_in">list</span>(<span class="built_in">set</span>(x.split(<span class="string">&#x27; &#x27;</span>)))))</span><br><span class="line">all_lines=<span class="string">&#x27; &#x27;</span>.join(<span class="built_in">list</span>(train_df[<span class="string">&#x27;text_unique&#x27;</span>]))</span><br><span class="line">word_count=Counter(all_lines.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">word_count=<span class="built_in">sorted</span>(word_count.items(),key=<span class="keyword">lambda</span> d:<span class="built_in">int</span>(d[<span class="number">1</span>]),reserve=<span class="literal">True</span>)</span><br><span class="line">word_count[<span class="number">0</span>]</span><br><span class="line">word_count[<span class="number">1</span>]</span><br><span class="line">word_count[<span class="number">2</span>]</span><br></pre></td></tr></table></figure><h2 id="数据分析的结论"><a href="#数据分析的结论" class="headerlink" title="数据分析的结论"></a>数据分析的结论</h2><p>通过上述分析可得出以下结论：</p><ol><li>赛题中每个新闻包含的字符个数平均为1000个，还有一些新闻字符较长</li><li>赛题中新闻类别分布不均匀，科技类新闻样本量接近4w,星座类不到1k</li><li>赛题总共包括7000——8000个字符</li></ol><p>通过数据分析，我们还可以得出以下结论</p><ol><li>每个新闻平均字符个数较多，需要截断</li><li>由于类别不均衡，会严重影响模型的精度</li></ol><h2 id="本章小结"><a href="#本章小结" class="headerlink" title="本章小结"></a>本章小结</h2><p>对赛题数据进行读取，并新闻句子长度、类别和字符进行了可视化分析</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>NLP学习day1：NPL实战流程</title>
      <link href="/2024/03/31/NLP%E5%AD%A6%E4%B9%A0day1/"/>
      <url>/2024/03/31/NLP%E5%AD%A6%E4%B9%A0day1/</url>
      
        <content type="html"><![CDATA[<h2 id="赛事理解"><a href="#赛事理解" class="headerlink" title="赛事理解"></a>赛事理解</h2><ul><li>通过这道赛题应了解NLP的预处理、模型构建和模型训练等知识点</li><li>本赛题以自然语言处理为背景，对新闻文本进行分类，是一个典型的字符识别问题<h2 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h2></li><li>理解赛题背景与赛题数据</li><li>完成赛题报名和数据下载，理解赛题的解题思路<h2 id="赛题数据"><a href="#赛题数据" class="headerlink" title="赛题数据"></a>赛题数据</h2></li></ul><ul><li>赛题数据为新闻文本，并按照字符匿名处理。整合划分出14个类别<h2 id="数据标签"><a href="#数据标签" class="headerlink" title="数据标签"></a>数据标签</h2></li></ul><hr><p>在数据集中标签的对应的关系如下：{‘科技’: 0, ‘股票’: 1, ‘体育’: 2, ‘娱乐’: 3, ‘时政’: 4, ‘社会’: 5, ‘教育’: 6, ‘财经’: 7, ‘家居’: 8, ‘游戏’: 9, ‘房产’: 10, ‘时尚’: 11, ‘彩票’: 12, ‘星座’: 13}</p><hr><h2 id="评测指标"><a href="#评测指标" class="headerlink" title="评测指标"></a>评测指标</h2><p><strong><em>为类别f1_score的均值</em></strong>，提交结果与实际测试集的类别进行对比，结果越大越好。</p><h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><p>使用<strong>pandas</strong>库完成读取操作，并对赛题数据进行分析</p><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>赛题实质是一个文本分类问题，需要对每句的字符进行分类，但赛题给出的数据是匿名的，无法直接使用中文分词等操作，这个是赛题的难点。<br>赛题难点：需要对匿名字符进行建模，进而完成文本分类的过程，这涉及到<strong>特征提取</strong>和<strong>分类模型</strong>两部分。</p><ul><li><p>思路一：TF-IDF+机器学习分类器</p><p>直接使用TF-IDF对文本提取特征，并使用分类器进行分类。在分类器的选择上，可以使用SVM、LR、或者XGBoost。<del>看不懂qwq</del></p></li><li><p>思路二：FastText</p><p>这是一款入门款的词向量，利用Facebook提供的FastText工具，可以快速构建出分类器。<del>还是看不懂（逃）</del></p></li><li><p>思路三：WordVec + 深度学习分类器</p><p>这是进阶款的词向量，并通过构建深度学习分类完成分类。深度学习分类的网络结构可选择TextCNN、TextRNN或者BiLSTM。<del>这是啥啊</del></p></li><li><p>思路四：Bert词向量</p><p>Bert是高配款的词向量，具有强大的建模学习能力。<del>呜呜呜，不像给NLP0基础新人做的，不会只有我零基础吧</del></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>要学习的技术</title>
      <link href="/2024/03/31/%E4%B8%80%E6%AE%B5%E6%97%B6%E9%97%B4%E7%9A%84%E5%B0%8F%E7%9B%AE%E6%A0%87/"/>
      <url>/2024/03/31/%E4%B8%80%E6%AE%B5%E6%97%B6%E9%97%B4%E7%9A%84%E5%B0%8F%E7%9B%AE%E6%A0%87/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><ol><li>个人博客主题优化，维护，并设计小功能</li><li>docker容器环境搭建并运行</li><li>NLP新闻文本分类入门+何晗老师的《自然语言处理入门》</li><li>微信机器人小程序学习部署</li></ol>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
