<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>《自然语言处理入门》学习Day1</title>
      <link href="/2024/04/07/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E5%AD%A6%E4%B9%A0Day1/"/>
      <url>/2024/04/07/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E5%AD%A6%E4%B9%A0Day1/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>新手上路</li></ol><ul><li>1.1 自然语言与编程语言的比较</li><li>1.2 自然语言处理的层次</li><li>1.3 自然语言处理的流派</li><li>1.4 机器学习</li><li>1.5 语料库</li><li>1.6 开源工具</li><li>1.7 总结</li></ul><h2 id="1-新手上路"><a href="#1-新手上路" class="headerlink" title="1.新手上路"></a>1.新手上路</h2><p>自然语言处理（Natural Language Processing, NLP）是一门融合了计算机科学、人工智能及语言学的交叉学科，它们的关系如下图所示。这门学科研究的是如何通过机器学习等技术，让计算机学会处理人类语言，乃至实现终极目标—理解人类语言或人工智能。</p><h3 id="1-1自然语言与编程语言的比较"><a href="#1-1自然语言与编程语言的比较" class="headerlink" title="1.1自然语言与编程语言的比较"></a>1.1自然语言与编程语言的比较</h3><div class="table-container"><table><thead><tr><th>比较</th><th>不同</th><th>例子</th></tr></thead><tbody><tr><td>词汇量</td><td>自然语言中的词汇比编程语言中的关键词丰富，我们还可以随时创造各种类型的新词</td><td>蓝瘦、香菇</td></tr><tr><td>机构化</td><td>自然语言是非结构化的，而编程语言是结构化的</td><td></td></tr><tr><td>歧义性</td><td>自然语言中含有大量歧义，而编程语言是结构化的</td><td>这人真有意思：没意思</td></tr><tr><td>容错性</td><td>自然语言错误随处可见，而编程语言错误会导致编译不通过</td><td>的、地的用法错误</td></tr><tr><td>易变性</td><td>自然语言变化相对迅速嘈杂一些，而编程语言的变化要缓慢的多</td><td>新时代词汇</td></tr><tr><td>简略性</td><td>自然语言往往简洁、干练，而编程语言就要明确定义</td><td>“老地方”不必明确指出</td></tr></tbody></table></div><h3 id="1-2自然语言处理的层次"><a href="#1-2自然语言处理的层次" class="headerlink" title="1.2自然语言处理的层次"></a>1.2自然语言处理的层次</h3><ol><li><p><strong>语音、图像和文本</strong></p><p>自然语言处理系统的输入源一共有三个，即语音、图像与文本。语音和图像这两种形式一般经过识别后转化为文字，转化后就可以进行后续的NLP任务了。</p></li><li><p><strong>中文分词、词性标注和命名实体识别</strong></p><p>这三个任务都是围绕词语进行的分析，所以统称<strong>词法分析</strong>。词法分析的主要任务是将文本分隔为有意义的词语(<strong>中文分词</strong>)，确定每个词语的类别和浅层的歧义消除(词性标注)，并且识别出一些较长的专有名词(<strong>命名实体识别</strong>)。对中文而言，词性分析常常是后续高级任务的基础。</p></li><li><p><strong>信息抽取</strong></p><p>词法分析之后，文本已经呈现出部分结构化的趋势，根据分析出来的每个单词和附有自己词性及其他标签的数据，抽取出一部分有用的信息、关键词、专业术语等，也可以根据统计学信息抽取出更大颗粒度的文本。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>南乡子</title>
      <link href="/2024/04/06/%E5%8D%97%E4%B9%A1%E5%AD%90.%E5%92%8C%E6%9D%A8%E5%85%83%E7%B4%A0%E6%97%B6%E7%A7%BB%E7%A7%BB%E5%AE%88%E5%AF%86%E5%B7%9E/"/>
      <url>/2024/04/06/%E5%8D%97%E4%B9%A1%E5%AD%90.%E5%92%8C%E6%9D%A8%E5%85%83%E7%B4%A0%E6%97%B6%E7%A7%BB%E7%A7%BB%E5%AE%88%E5%AF%86%E5%B7%9E/</url>
      
        <content type="html"><![CDATA[<h2 id="南乡子-和杨元素时移移守密州"><a href="#南乡子-和杨元素时移移守密州" class="headerlink" title="南乡子.和杨元素时移移守密州"></a>南乡子.和杨元素时移移守密州</h2><p>东武望余杭，云海天涯两杳茫。何日功成名遂了，还乡，醉笑陪公三万场。</p><p>不用诉离殇，痛饮从来别有肠。今夜送归灯火冷，河塘，堕泪羊公却姓杨。</p>]]></content>
      
      
      <categories>
          
          <category> 诗情画意 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>NLP学习day5：词向量汇总：word2vec、fastText、Glove,Bert</title>
      <link href="/2024/04/06/NLP%E5%AD%A6%E4%B9%A0day5/"/>
      <url>/2024/04/06/NLP%E5%AD%A6%E4%B9%A0day5/</url>
      
        <content type="html"><![CDATA[<p>-</p><h2 id="一、Onehot"><a href="#一、Onehot" class="headerlink" title="一、Onehot"></a>一、Onehot</h2><h3 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h3><ol><li>解决了分类器处理离散数据困难的问题</li><li>一定程度上起到了扩展特征的作用</li></ol><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol><li>one-hot是一个词袋模型，不考虑词与词之间的顺序问题，而在文本中次序是一个很重要的问题</li><li>one-hot是基于词与词之间相互独立的情况下的，然而在多数情况中，词与词之间应该是相互影响的</li><li>one-hot得到的特征是离散的、稀疏的</li></ol><h2 id="二、TF-IDF与TextRank"><a href="#二、TF-IDF与TextRank" class="headerlink" title="二、TF-IDF与TextRank"></a>二、TF-IDF与TextRank</h2><p>NLP中最为经典的关键词提取算法，虽然简单，但应用广泛</p><h3 id="1-TF-IDF-参见上文"><a href="#1-TF-IDF-参见上文" class="headerlink" title="1. TF-IDF:参见上文"></a>1. TF-IDF:参见上文</h3><p><strong>总结</strong>:</p><ul><li>当一个词在文档频率越高并且新鲜度高（即普遍度低），其TF-IDF值越高。</li><li>TF-IDF兼顾词频与新鲜度，过滤一些常见词，保留能提供更多信息的重要词。</li></ul><h3 id="2-TextRank简介"><a href="#2-TextRank简介" class="headerlink" title="2. TextRank简介"></a>2. TextRank简介</h3><p>通过词之间的相邻关系构建网络，然后用<strong>PageRank</strong>迭代计算每个节点的rank值，排序rank值即可得到关键词。</p><p>PageRank本来是用来解决网页排名的问题，网页之间的链接关系即为图的边，迭代计算公式如下：</p><script type="math/tex; mode=display">PR(Vi)=(1-d)+d*\sum_{j}</script>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Docker学习Day1</title>
      <link href="/2024/04/04/Docker%E5%AD%A6%E4%B9%A0day1/"/>
      <url>/2024/04/04/Docker%E5%AD%A6%E4%B9%A0day1/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>随着AI,云原生等技术的向前推进，容器技术逐渐成为每位算法同学的必备技能之一，开始关于容器技术的学习很有必要。我想从零基础实现将代码打包docker镜像-调试-提交仓库-提交云服务训练模型/天池大赛提交/学校服务器训练等流程。希望初次接触docker能够做到以上步骤。</p><h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><p>docker作为虚拟机领域成熟的轻量化容器产品，可以轻松的将代码和所依赖的整个环境（包括整个操作系统）都打包在一起，不依赖于软件环境，方便把自己的代码从windows电脑分享到mac电脑运行、或者服务器上运行等。</p><p>docker三要素：<strong>镜像(image)，容器(contarin)，registry(包含多个仓库)</strong></p><p>以下是对三个要素的分别解释：</p><h3 id="镜像"><a href="#镜像" class="headerlink" title="镜像"></a>镜像</h3><p>顾名思义就是将要把代码以及环境打包在一起的这个产物就叫做镜像</p><h3 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h3><p>运行起来的镜像称之为容器，可以理解为运行环境或者实例。其实质是进程，随着代码运行结束，进程结束容器也就消失了。</p><h3 id="registry"><a href="#registry" class="headerlink" title="registry"></a>registry</h3><p>镜像存储的地方在registry,这是各云厂商提供的镜像存取服务，类似于网盘，将镜像存储在云端仓库，方便我们随时随地在不同介质运行自己的代码或分享代码。如果你要把本地开发好的代码放在服务器上做耗时的训练动作，那么只需要在服务器上直接拉取自己云端的镜像运行即可。类似于git还有代码管理功能。</p>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>NLP学习day4：结合tfidf与RidgeClassifier(岭回归)计算f1_score得分</title>
      <link href="/2024/04/04/NLP%E5%AD%A6%E4%B9%A0day4/"/>
      <url>/2024/04/04/NLP%E5%AD%A6%E4%B9%A0day4/</url>
      
        <content type="html"><![CDATA[<h1 id="代码展示"><a href="#代码展示" class="headerlink" title="代码展示"></a>代码展示</h1><h2 id="导入相关库与工具"><a href="#导入相关库与工具" class="headerlink" title="导入相关库与工具"></a>导入相关库与工具</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br></pre></td></tr></table></figure><h2 id="提取数据表"><a href="#提取数据表" class="headerlink" title="提取数据表"></a>提取数据表</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df=pd.read_csv(<span class="string">&quot;C:\Users\hjg\OneDrive\桌面\train_set.csv&quot;</span>,sep=<span class="string">&#x27;\t&#x27;</span>,nrows=<span class="number">20000</span>)</span><br></pre></td></tr></table></figure><h2 id="提取tfidf值"><a href="#提取tfidf值" class="headerlink" title="提取tfidf值"></a>提取tfidf值</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tfidf=Vectorizer(ngram_range=(<span class="number">1</span>,<span class="number">3</span>),max_feature=<span class="number">3000</span>)</span><br><span class="line">train_test=tfidf.fit_transfrom(train_df[<span class="string">&#x27;text&#x27;</span>])</span><br></pre></td></tr></table></figure><h2 id="构建岭回归，拟合训练模型"><a href="#构建岭回归，拟合训练模型" class="headerlink" title="构建岭回归，拟合训练模型"></a>构建岭回归，拟合训练模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf=RidgeClassifier()</span><br><span class="line">clf.fit(train_test[:<span class="number">1000</span>],train_df[<span class="string">&#x27;label&#x27;</span>].values[:<span class="number">1000</span>])</span><br><span class="line">vlt_per=clf.predict(tran_test[<span class="number">1000</span>:])</span><br></pre></td></tr></table></figure><h2 id="计算f1-score分数"><a href="#计算f1-score分数" class="headerlink" title="计算f1_score分数"></a>计算f1_score分数</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(f1_score(train_df[<span class="string">&#x27;label&#x27;</span>].values[<span class="number">1000</span>:],vlt_pre,averages=<span class="string">&#x27;macro&#x27;</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>NLP学习Day3：基于TF—IDF机器学习模型完成新闻文本分类</title>
      <link href="/2024/04/02/NLP%E5%AD%A6%E4%B9%A0day3/"/>
      <url>/2024/04/02/NLP%E5%AD%A6%E4%B9%A0day3/</url>
      
        <content type="html"><![CDATA[<h2 id="Task3-基于机器学习的文本分类"><a href="#Task3-基于机器学习的文本分类" class="headerlink" title="Task3 基于机器学习的文本分类"></a>Task3 基于机器学习的文本分类</h2><h3 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>学会TF-IDF的原理</li><li>使用sklearn的机器学习模型完成文本分类</li></ul><h2 id="机器学习模型"><a href="#机器学习模型" class="headerlink" title="机器学习模型"></a>机器学习模型</h2><p>机器学习是对能通过经验自动改进的计算机算法的研究机器学习通过历史数据<strong>训练</strong>出<strong>模型</strong>对应于人类经验进行<strong>归纳</strong>的过程，机器学习利用<strong>模型</strong>对新数据进行<strong>预测</strong>对应于人类利用总结的<strong>规律</strong>对新问题进行预测的过程。</p><p>机器学习有很多分支，对于学习者来说应该优先掌握机器学习算法的分类，然后在其中一种机器学习算法进行学习。</p><p>机器学习初学者，应该知道如下的事情：</p><ol><li>机器学习能解决一定的问题，但不能奢求机器学习是万能的</li><li>机器学习算法有很多种，看具体问题进行选择算法</li><li>每种机器学习算法有一定的偏好，具体问题具体分析</li></ol><h2 id="文本表示方法-Part1"><a href="#文本表示方法-Part1" class="headerlink" title="文本表示方法 Part1"></a>文本表示方法 Part1</h2><p>首先要知道在机器学习算法的训练过程中，假设给定N个样本，每个样本有M个特征，这样组成了N<em>M的样本举证，然后完成算法的训练和预测。但在NLP中这样的方法是不行的：文本是不定长度的。文本表示成计算机能够运算的数字或向量的方法一般称为<em>*词嵌入方法</em></em>。词嵌入将不定长的文本转换到定长的空间内，是文本分类的第一步。</p><h2 id="One-hot"><a href="#One-hot" class="headerlink" title="One-hot"></a>One-hot</h2><p>这里的One-hot与数据挖掘任务中的操作是一致的，即将每一个单词使用一个离散的向量表示。具体讲灭各自/词编码一个索引，然后根据索引进行赋值。</p><p>One-hot表示方法的例子如下：</p><blockquote><p>句子1：我 爱 北 京 天 安 门</p><p>句子2：我 喜 欢 上 海</p></blockquote><p>首先对所有句子的字进行索引，即将每个字分配一个编号：</p><blockquote><p>{</p><p>‘我’：1，‘爱’：2…………….’海’：11</p><p>}</p></blockquote><p>在这里共包括11个字，每个字可以转换为一个11维度稀疏向量：</p><p>我：[1,0,0,0,0,0,0,0,0,0,0]</p><p>爱：[0,1,0,0,0,0,0,0,0,0,0]</p><p>…….</p><p>海：[0,0,0,0,0,0,0,0,0,0,1]</p><h2 id="Bag-of-Words"><a href="#Bag-of-Words" class="headerlink" title="Bag of Words"></a>Bag of Words</h2><p>词袋表示，每个文档的字/词可以使用其出现次数来进行表示。词袋模型之所以被称为“词袋”模型，是因为这种模型在实现文本数值化过程中将所有词汇扔进一个袋子中，而忽略了它们在原文中的语法结构和顺序，只关心每个词汇出现的次数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction</span><br><span class="line">corpus=[</span><br><span class="line">    <span class="string">&#x27;This is the frist document.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;This document is the second document.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;And this is the third one.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Is this the first document&#x27;</span>,</span><br><span class="line">]</span><br><span class="line">vectorizer=CountVectorizer()</span><br><span class="line">vectorizer.fit_transform(corpus).toarray()</span><br></pre></td></tr></table></figure><h3 id="文本数值化分析过程"><a href="#文本数值化分析过程" class="headerlink" title="文本数值化分析过程"></a>文本数值化分析过程</h3><ul><li>对文本分词</li><li>去除停用词</li><li>构建词汇表</li><li>文本向量化</li></ul><h2 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h2><p>TF-IDF分数由两部分组成：第一部分是<strong>词语频率</strong>（Term Frequency），第二部分<strong>逆文档频率</strong>（Lnverse Document）。其中计算语料库中文档总数除以含有该词语的文档数量，然后再取对数就是逆文档频率。</p><h3 id="TF-IDF作用："><a href="#TF-IDF作用：" class="headerlink" title="TF-IDF作用："></a>TF-IDF作用：</h3><ul><li>评估每个词在文档中的重要性，以实现对关键词的抽取</li><li>将输入文档表示为向量（关键词重要向量），可用于文档检索</li></ul><h3 id="1-算法公式"><a href="#1-算法公式" class="headerlink" title="1.算法公式"></a>1.算法公式</h3><p>TF-IDF=TF*IDF，评估一个词的重要性要综合考虑该词的TF值和IDF值。</p><ol><li>TF:表示一个词在文档中的出现次数</li><li>IDF:表示包含某个词的文档数量，如果包含该词的文档数量越少则IDF值越大</li></ol><p>例如，词A和词B是在某个文档中出现的词，出现数量一样多，当：</p><ol><li>词A在其他文档中大量出现</li><li>词B在其它文档中少量出现</li></ol><p>则认为：词A的重要程度不如词B</p><p><strong><em>TF-IDF认为词A在其它文档中大量出现，说明该词很普遍，会降低该词的重要性。反之，则认为该词更为重要</em></strong></p><h4 id="TF（词频）"><a href="#TF（词频）" class="headerlink" title="TF（词频）"></a>TF（词频）</h4><p>TF（词频）=某个词在文档中出现的次数</p><p>TF~month~=1+log(TF)</p><h4 id="IDF（逆文档词频）"><a href="#IDF（逆文档词频）" class="headerlink" title="IDF（逆文档词频）"></a>IDF（逆文档词频）</h4><p>IDF（逆文档词频）=<script type="math/tex">\log{\left(\frac{文档总数}{包含某个词的文档数量}\right)}</script></p><p>IDF~month~=<script type="math/tex">\log{\left(\frac{文档总数+1}{包含某个词的文档数量+1}\right)}</script></p><h4 id="公式反映出的信息是什么？"><a href="#公式反映出的信息是什么？" class="headerlink" title="公式反映出的信息是什么？"></a>公式反映出的信息是什么？</h4><p>文档集合中，包含某个词的文档数量越多，该词的IDF值就越小，反之则越大。</p><h4 id="为什么进行log运算？"><a href="#为什么进行log运算？" class="headerlink" title="为什么进行log运算？"></a>为什么进行<script type="math/tex">log</script>运算？</h4><p>对数函数能够缩小数据范围，将大范围的值映射到相对较小的范围。通过取对数，可以有效地缩小高频词的影响，使得低频词在计算中更具影响力</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>NLP学习Day2：数据读取与数据分析</title>
      <link href="/2024/04/01/NLP%E5%AD%A6%E4%B9%A0day2/"/>
      <url>/2024/04/01/NLP%E5%AD%A6%E4%B9%A0day2/</url>
      
        <content type="html"><![CDATA[<h1 id="NLP学习Day2"><a href="#NLP学习Day2" class="headerlink" title="NLP学习Day2"></a>NLP学习Day2</h1><h2 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h2><ul><li>学习使用pandas读取赛题数据</li><li>分析赛题数据的分布规律</li></ul><h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><p>首先文本数据是使用csv格式进行存储。因此可以直接使用pandas完成数据读取的操作</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">train_df=pd.read_csv(<span class="string">&#x27;文本地址&#x27;</span>,sep=<span class="string">&#x27;\t&#x27;</span>,nrows=<span class="number">100</span>)</span><br></pre></td></tr></table></figure><h3 id="read-csv解析"><a href="#read-csv解析" class="headerlink" title="read_csv解析"></a>read_csv解析</h3><ul><li>读取的文件路径要改成本地的路径（相对路径或绝对路径）</li><li>分隔符<strong>sep</strong>,为每列分割的字符，设置为<strong>\t</strong>即可；</li><li>读取行数nrows,为每次读取文件的函数，是数值类型（数据集比较大）</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df.head()</span><br></pre></td></tr></table></figure><h2 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h2><p>在数据读取操作完成之后，要对数据集进行数据分析的操作，我们希望在读取训练集数据之后得到以下结论：</p><ul><li>赛题数据中，新闻文本的长度是多少？</li><li>赛题数据的类别分布是怎么样的，哪些类别比较多？</li><li>赛题数据中，字符分布是怎么样的？</li></ul><h3 id="句子长度分析"><a href="#句子长度分析" class="headerlink" title="句子长度分析"></a>句子长度分析</h3><p>在赛题数据中每行句子的字符使用空格进行隔开，所以可以直接统计单词的个数来得到每个句子的长度</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%pylab inline</span><br><span class="line">train_df[<span class="string">&#x27;text_len&#x27;</span>]=train_df[<span class="string">&#x27;text&#x27;</span>].apply(<span class="keyword">lambda</span> x:<span class="built_in">len</span>(x,split(<span class="string">&#x27; &#x27;</span>)))</span><br><span class="line">train_df[<span class="string">&#x27;text_len&#x27;</span>].describe()</span><br></pre></td></tr></table></figure><p>下面将句子长度绘制直方图</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.hist(train_df[<span class="string">&#x27;text_len&#x27;</span>],bins=<span class="number">200</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Text char count&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Histogram of char count&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="新闻类别分布"><a href="#新闻类别分布" class="headerlink" title="新闻类别分布"></a>新闻类别分布</h2><p>对数据集的类别进行分布统计，具体统计没类新闻的样本个数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df[<span class="string">&#x27;label&#x27;</span>].value_counts().plot(kind=<span class="string">&#x27;bar&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;News class count&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;category&quot;</span>)</span><br></pre></td></tr></table></figure><p>在数据集中标签的对应关系如下：</p><blockquote><p>在数据集中标签的对应的关系如下：{‘科技’: 0, ‘股票’: 1, ‘体育’: 2, ‘娱乐’: 3, ‘时政’: 4, ‘社会’: 5, ‘教育’: 6, ‘财经’: 7, ‘家居’: 8, ‘游戏’: 9, ‘房产’: 10, ‘时尚’: 11, ‘彩票’: 12, ‘星座’: 13}</p></blockquote><p><strong>从统计结果看出，赛题的数据集类别分布存在较为不均匀的情况，科技类新闻最多</strong></p><h2 id="字符分布统计"><a href="#字符分布统计" class="headerlink" title="字符分布统计"></a>字符分布统计</h2><p>接下来可以统计每个字符出现的次数，首先将训练集中所有的句子进行拼接进而划分为字符，并统计每个字符的个数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">all_lines=<span class="string">&#x27; &#x27;</span>.join(<span class="built_in">list</span>(train_df[<span class="string">&#x27;text&#x27;</span>]))</span><br><span class="line">word_count=Counter(all_lines.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">word_count=<span class="built_in">sorted</span>(word_count.items(),key=<span class="keyword">lambda</span> d:d[<span class="number">1</span>],reserve=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(word_count))</span><br><span class="line"><span class="built_in">print</span>(word_count[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(word_count[-<span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>还可根据字在每个句子的出现情况，反推出标点符号。下面代码统计了不同字符在句子中出现的次数</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">train_df[<span class="string">&#x27;text_unique&#x27;</span>]=train_df[<span class="string">&#x27;text&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="string">&#x27; &#x27;</span>.join(<span class="built_in">list</span>(<span class="built_in">set</span>(x.split(<span class="string">&#x27; &#x27;</span>)))))</span><br><span class="line">all_lines=<span class="string">&#x27; &#x27;</span>.join(<span class="built_in">list</span>(train_df[<span class="string">&#x27;text_unique&#x27;</span>]))</span><br><span class="line">word_count=Counter(all_lines.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">word_count=<span class="built_in">sorted</span>(word_count.items(),key=<span class="keyword">lambda</span> d:<span class="built_in">int</span>(d[<span class="number">1</span>]),reserve=<span class="literal">True</span>)</span><br><span class="line">word_count[<span class="number">0</span>]</span><br><span class="line">word_count[<span class="number">1</span>]</span><br><span class="line">word_count[<span class="number">2</span>]</span><br></pre></td></tr></table></figure><h2 id="数据分析的结论"><a href="#数据分析的结论" class="headerlink" title="数据分析的结论"></a>数据分析的结论</h2><p>通过上述分析可得出以下结论：</p><ol><li>赛题中每个新闻包含的字符个数平均为1000个，还有一些新闻字符较长</li><li>赛题中新闻类别分布不均匀，科技类新闻样本量接近4w,星座类不到1k</li><li>赛题总共包括7000——8000个字符</li></ol><p>通过数据分析，我们还可以得出以下结论</p><ol><li>每个新闻平均字符个数较多，需要截断</li><li>由于类别不均衡，会严重影响模型的精度</li></ol><h2 id="本章小结"><a href="#本章小结" class="headerlink" title="本章小结"></a>本章小结</h2><p>对赛题数据进行读取，并新闻句子长度、类别和字符进行了可视化分析</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>NLP学习day1：NPL实战流程</title>
      <link href="/2024/03/31/NLP%E5%AD%A6%E4%B9%A0day1/"/>
      <url>/2024/03/31/NLP%E5%AD%A6%E4%B9%A0day1/</url>
      
        <content type="html"><![CDATA[<h2 id="赛事理解"><a href="#赛事理解" class="headerlink" title="赛事理解"></a>赛事理解</h2><ul><li>通过这道赛题应了解NLP的预处理、模型构建和模型训练等知识点</li><li>本赛题以自然语言处理为背景，对新闻文本进行分类，是一个典型的字符识别问题<h2 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h2></li><li>理解赛题背景与赛题数据</li><li>完成赛题报名和数据下载，理解赛题的解题思路<h2 id="赛题数据"><a href="#赛题数据" class="headerlink" title="赛题数据"></a>赛题数据</h2></li></ul><ul><li>赛题数据为新闻文本，并按照字符匿名处理。整合划分出14个类别<h2 id="数据标签"><a href="#数据标签" class="headerlink" title="数据标签"></a>数据标签</h2></li></ul><hr><p>在数据集中标签的对应的关系如下：{‘科技’: 0, ‘股票’: 1, ‘体育’: 2, ‘娱乐’: 3, ‘时政’: 4, ‘社会’: 5, ‘教育’: 6, ‘财经’: 7, ‘家居’: 8, ‘游戏’: 9, ‘房产’: 10, ‘时尚’: 11, ‘彩票’: 12, ‘星座’: 13}</p><hr><h2 id="评测指标"><a href="#评测指标" class="headerlink" title="评测指标"></a>评测指标</h2><p><strong><em>为类别f1_score的均值</em></strong>，提交结果与实际测试集的类别进行对比，结果越大越好。</p><h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><p>使用<strong>pandas</strong>库完成读取操作，并对赛题数据进行分析</p><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>赛题实质是一个文本分类问题，需要对每句的字符进行分类，但赛题给出的数据是匿名的，无法直接使用中文分词等操作，这个是赛题的难点。<br>赛题难点：需要对匿名字符进行建模，进而完成文本分类的过程，这涉及到<strong>特征提取</strong>和<strong>分类模型</strong>两部分。</p><ul><li><p>思路一：TF-IDF+机器学习分类器</p><p>直接使用TF-IDF对文本提取特征，并使用分类器进行分类。在分类器的选择上，可以使用SVM、LR、或者XGBoost。<del>看不懂qwq</del></p></li><li><p>思路二：FastText</p><p>这是一款入门款的词向量，利用Facebook提供的FastText工具，可以快速构建出分类器。<del>还是看不懂（逃）</del></p></li><li><p>思路三：WordVec + 深度学习分类器</p><p>这是进阶款的词向量，并通过构建深度学习分类完成分类。深度学习分类的网络结构可选择TextCNN、TextRNN或者BiLSTM。<del>这是啥啊</del></p></li><li><p>思路四：Bert词向量</p><p>Bert是高配款的词向量，具有强大的建模学习能力。<del>呜呜呜，不像给NLP0基础新人做的，不会只有我零基础吧</del></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>要学习的技术</title>
      <link href="/2024/03/31/%E4%B8%80%E6%AE%B5%E6%97%B6%E9%97%B4%E7%9A%84%E5%B0%8F%E7%9B%AE%E6%A0%87/"/>
      <url>/2024/03/31/%E4%B8%80%E6%AE%B5%E6%97%B6%E9%97%B4%E7%9A%84%E5%B0%8F%E7%9B%AE%E6%A0%87/</url>
      
        <content type="html"><![CDATA[<ol><li>个人博客主题优化，维护，并设计小功能</li><li>docker容器环境搭建并运行</li><li>NLP新闻文本分类入门+西瓜书机器学习入门</li><li>微信机器人小程序学习部署</li><li>算竞训练（一天一道1800，写题解）<br>此外<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">务须坚持学习，写博客</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
